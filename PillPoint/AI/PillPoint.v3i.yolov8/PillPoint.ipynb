{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train path: True\n",
      "Validation path: True\n",
      "Test path: True\n",
      " The number of images in training set: 1418\n",
      " The number of images in validation set: 195\n",
      " The number of images in test set: 98\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "with open('data.yaml', 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "print(\"Train path:\", os.path.exists(data['train']))\n",
    "print(\"Validation path:\", os.path.exists(data['val']))\n",
    "print(\"Test path:\", os.path.exists(data['test']))\n",
    "\n",
    "def count_images(directory):\n",
    "    supported_formats = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif')\n",
    "    return len([name for name in os.listdir(directory) if name.lower().endswith(supported_formats)])\n",
    "\n",
    "print(f\" The number of images in training set: {count_images(data['train'])}\")\n",
    "print(f\" The number of images in validation set: {count_images(data['val'])}\")\n",
    "print(f\" The number of images in test set: {count_images(data['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd39bce228f9468ab248c7aab26f91d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/6.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.227 ðŸš€ Python-3.9.18 torch-1.13.1 CPU (Apple M1)\n",
      "Ultralytics YOLOv8.0.227 ðŸš€ Python-3.9.18 torch-1.13.1 CPU (Apple M1)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/data.yaml, epochs=10, patience=50, batch=16, imgsz=480, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/data.yaml, epochs=10, patience=50, batch=16, imgsz=480, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary: 225 layers, 3011628 parameters, 3011612 gradients\n",
      "Model summary: 225 layers, 3011628 parameters, 3011612 gradients\n",
      "\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/train/labels... 1418 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1418/1418 [00:00<00:00, 2563.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/train/labels.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/train/labels.cache\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 540, len(boxes) = 4618. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 540, len(boxes) = 4618. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/valid/labels... 195 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195/195 [00:00<00:00, 2116.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/valid/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/valid/labels.cache\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 82, len(boxes) = 581. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 82, len(boxes) = 581. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 480 train, 480 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Image sizes 480 train, 480 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.674      2.511      1.574         25        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [06:30<00:00,  4.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.858      0.451      0.592      0.314\n",
      "                   all        195        581      0.858      0.451      0.592      0.314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.574      1.745      1.479         37        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:40<00:00,  3.83s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581        0.7      0.505      0.586        0.3\n",
      "                   all        195        581        0.7      0.505      0.586        0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.589      1.618      1.461         25        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:22<00:00,  3.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.649       0.64      0.643      0.333\n",
      "                   all        195        581      0.649       0.64      0.643      0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.554      1.475      1.458         33        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:29<00:00,  3.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.663      0.647      0.689      0.382\n",
      "                   all        195        581      0.663      0.647      0.689      0.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.516      1.385      1.413         25        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:23<00:00,  3.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.643      0.704      0.722      0.423\n",
      "                   all        195        581      0.643      0.704      0.722      0.423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.471      1.274      1.369         16        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:16<00:00,  3.56s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.699      0.777      0.781      0.452\n",
      "                   all        195        581      0.699      0.777      0.781      0.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.418      1.164      1.353         32        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:07<00:00,  3.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.734      0.728      0.799      0.457\n",
      "                   all        195        581      0.734      0.728      0.799      0.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.375      1.078      1.315         37        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:16<00:00,  3.55s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.765      0.788      0.825      0.487\n",
      "                   all        195        581      0.765      0.788      0.825      0.487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.351      1.009      1.292         57        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:11<00:00,  3.50s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581        0.8      0.745      0.829      0.507\n",
      "                   all        195        581        0.8      0.745      0.829      0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.309     0.9443      1.269         23        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:19<00:00,  3.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.753      0.821      0.851      0.517\n",
      "                   all        195        581      0.753      0.821      0.851      0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.961 hours.\n",
      "\n",
      "10 epochs completed in 0.961 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.227 ðŸš€ Python-3.9.18 torch-1.13.1 CPU (Apple M1)\n",
      "Ultralytics YOLOv8.0.227 ðŸš€ Python-3.9.18 torch-1.13.1 CPU (Apple M1)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.751       0.82       0.85      0.516\n",
      "                   all        195        581      0.751       0.82       0.85      0.516\n",
      "                 Hands        195        261      0.822      0.867      0.908      0.504\n",
      "                 Hands        195        261      0.822      0.867      0.908      0.504\n",
      "                  Open        195         82      0.635      0.622      0.655      0.315\n",
      "                  Open        195         82      0.635      0.622      0.655      0.315\n",
      "                  Pill        195        157      0.677      0.879      0.906      0.634\n",
      "                  Pill        195        157      0.677      0.879      0.906      0.634\n",
      "               PillBox        195         81      0.871      0.914      0.931      0.612\n",
      "               PillBox        195         81      0.871      0.914      0.931      0.612\n",
      "Speed: 0.8ms preprocess, 87.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Speed: 0.8ms preprocess, 87.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "\n",
    "absolute_from_relative_path = Path('data.yaml').resolve()\n",
    "\n",
    "model = YOLO('yolov8n.pt')  # Load a pretrained model (recommended for training)\n",
    "results = model.train(data=absolute_from_relative_path, epochs=10, imgsz=480)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0: 480x480 17 Pills, 1: 480x480 1 Pill, 2: 480x480 5 Opens, 3: 480x480 2 Pills, 4: 480x480 1 PillBox, 5: 480x480 2 Handss, 6: 480x480 2 Pills, 7: 480x480 4 Handss, 8: 480x480 1 Hands, 9: 480x480 1 Hands, 10: 480x480 6 Handss, 11: 480x480 1 Hands, 12: 480x480 2 Pills, 13: 480x480 1 Hands, 14: 480x480 20 Pills, 15: 480x480 1 PillBox, 16: 480x480 1 Hands, 17: 480x480 2 Handss, 18: 480x480 1 Hands, 19: 480x480 11 Handss, 20: 480x480 2 Pills, 21: 480x480 18 Pills, 22: 480x480 17 Handss, 23: 480x480 4 Handss, 24: 480x480 2 Pills, 25: 480x480 1 PillBox, 26: 480x480 2 Pills, 27: 480x480 1 Hands, 28: 480x480 2 Pills, 29: 480x480 1 Hands, 30: 480x480 6 Opens, 31: 480x480 2 Pills, 32: 480x480 2 Pills, 33: 480x480 2 Handss, 34: 480x480 1 Hands, 35: 480x480 1 PillBox, 36: 480x480 2 Handss, 37: 480x480 8 PillBoxs, 38: 480x480 2 Handss, 39: 480x480 7 Opens, 40: 480x480 10 Handss, 41: 480x480 1 Pill, 42: 480x480 1 PillBox, 43: 480x480 2 Pills, 44: 480x480 5 Handss, 45: 480x480 6 Handss, 46: 480x480 2 Opens, 47: 480x480 2 PillBoxs, 48: 480x480 2 Pills, 49: 480x480 5 Handss, 50: 480x480 1 Open, 51: 480x480 2 Pills, 52: 480x480 3 PillBoxs, 53: 480x480 1 Hands, 54: 480x480 1 Pill, 55: 480x480 2 Pills, 56: 480x480 1 PillBox, 57: 480x480 2 Handss, 58: 480x480 7 Handss, 59: 480x480 2 Pills, 60: 480x480 3 PillBoxs, 61: 480x480 3 Opens, 62: 480x480 1 Open, 63: 480x480 2 Pills, 64: 480x480 2 Handss, 65: 480x480 4 Handss, 66: 480x480 9 Handss, 67: 480x480 1 PillBox, 68: 480x480 6 PillBoxs, 69: 480x480 2 Pills, 70: 480x480 (no detections), 71: 480x480 5 Handss, 72: 480x480 4 Handss, 73: 480x480 2 Pills, 74: 480x480 1 PillBox, 75: 480x480 2 Handss, 76: 480x480 1 Pill, 77: 480x480 5 Handss, 78: 480x480 18 Pills, 79: 480x480 1 Pill, 80: 480x480 1 Open, 1 PillBox, 81: 480x480 4 Opens, 82: 480x480 8 Handss, 83: 480x480 2 Handss, 84: 480x480 1 PillBox, 85: 480x480 2 Handss, 86: 480x480 1 PillBox, 87: 480x480 4 Handss, 88: 480x480 7 Handss, 89: 480x480 3 Opens, 90: 480x480 2 Pills, 91: 480x480 1 Hands, 92: 480x480 2 Handss, 93: 480x480 7 Handss, 94: 480x480 7 PillBoxs, 95: 480x480 2 Pills, 96: 480x480 1 Pill, 97: 480x480 3 Pills, 10197.2ms\n",
      "0: 480x480 17 Pills, 1: 480x480 1 Pill, 2: 480x480 5 Opens, 3: 480x480 2 Pills, 4: 480x480 1 PillBox, 5: 480x480 2 Handss, 6: 480x480 2 Pills, 7: 480x480 4 Handss, 8: 480x480 1 Hands, 9: 480x480 1 Hands, 10: 480x480 6 Handss, 11: 480x480 1 Hands, 12: 480x480 2 Pills, 13: 480x480 1 Hands, 14: 480x480 20 Pills, 15: 480x480 1 PillBox, 16: 480x480 1 Hands, 17: 480x480 2 Handss, 18: 480x480 1 Hands, 19: 480x480 11 Handss, 20: 480x480 2 Pills, 21: 480x480 18 Pills, 22: 480x480 17 Handss, 23: 480x480 4 Handss, 24: 480x480 2 Pills, 25: 480x480 1 PillBox, 26: 480x480 2 Pills, 27: 480x480 1 Hands, 28: 480x480 2 Pills, 29: 480x480 1 Hands, 30: 480x480 6 Opens, 31: 480x480 2 Pills, 32: 480x480 2 Pills, 33: 480x480 2 Handss, 34: 480x480 1 Hands, 35: 480x480 1 PillBox, 36: 480x480 2 Handss, 37: 480x480 8 PillBoxs, 38: 480x480 2 Handss, 39: 480x480 7 Opens, 40: 480x480 10 Handss, 41: 480x480 1 Pill, 42: 480x480 1 PillBox, 43: 480x480 2 Pills, 44: 480x480 5 Handss, 45: 480x480 6 Handss, 46: 480x480 2 Opens, 47: 480x480 2 PillBoxs, 48: 480x480 2 Pills, 49: 480x480 5 Handss, 50: 480x480 1 Open, 51: 480x480 2 Pills, 52: 480x480 3 PillBoxs, 53: 480x480 1 Hands, 54: 480x480 1 Pill, 55: 480x480 2 Pills, 56: 480x480 1 PillBox, 57: 480x480 2 Handss, 58: 480x480 7 Handss, 59: 480x480 2 Pills, 60: 480x480 3 PillBoxs, 61: 480x480 3 Opens, 62: 480x480 1 Open, 63: 480x480 2 Pills, 64: 480x480 2 Handss, 65: 480x480 4 Handss, 66: 480x480 9 Handss, 67: 480x480 1 PillBox, 68: 480x480 6 PillBoxs, 69: 480x480 2 Pills, 70: 480x480 (no detections), 71: 480x480 5 Handss, 72: 480x480 4 Handss, 73: 480x480 2 Pills, 74: 480x480 1 PillBox, 75: 480x480 2 Handss, 76: 480x480 1 Pill, 77: 480x480 5 Handss, 78: 480x480 18 Pills, 79: 480x480 1 Pill, 80: 480x480 1 Open, 1 PillBox, 81: 480x480 4 Opens, 82: 480x480 8 Handss, 83: 480x480 2 Handss, 84: 480x480 1 PillBox, 85: 480x480 2 Handss, 86: 480x480 1 PillBox, 87: 480x480 4 Handss, 88: 480x480 7 Handss, 89: 480x480 3 Opens, 90: 480x480 2 Pills, 91: 480x480 1 Hands, 92: 480x480 2 Handss, 93: 480x480 7 Handss, 94: 480x480 7 PillBoxs, 95: 480x480 2 Pills, 96: 480x480 1 Pill, 97: 480x480 3 Pills, 10197.2ms\n",
      "Speed: 2.2ms preprocess, 104.1ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 480)\n",
      "Speed: 2.2ms preprocess, 104.1ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 480)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "97 labels saved to runs/detect/predict/labels\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "97 labels saved to runs/detect/predict/labels\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240531_092058_Chrome_jpg.rf.781e6428f262e4970964114b64111520.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "conf: tensor([0.6302, 0.6277, 0.5582, 0.5184, 0.5138, 0.5049, 0.4849, 0.4467, 0.4446, 0.4438, 0.4221, 0.3647, 0.3369, 0.3099, 0.2799, 0.2586, 0.2524])\n",
      "data: tensor([[2.6060e+02, 3.3960e+02, 2.9618e+02, 3.6528e+02, 6.3019e-01, 2.0000e+00],\n",
      "        [1.2764e+02, 1.1353e+02, 1.7052e+02, 1.4109e+02, 6.2769e-01, 2.0000e+00],\n",
      "        [2.7904e+02, 8.5625e+01, 3.1165e+02, 1.1102e+02, 5.5822e-01, 2.0000e+00],\n",
      "        [4.0619e+02, 3.3576e+02, 4.4602e+02, 3.6836e+02, 5.1838e-01, 2.0000e+00],\n",
      "        [2.8169e+02, 1.3860e+02, 3.1879e+02, 1.6292e+02, 5.1378e-01, 2.0000e+00],\n",
      "        [3.6119e+02, 1.4461e+02, 4.1263e+02, 1.8008e+02, 5.0491e-01, 2.0000e+00],\n",
      "        [3.2884e+02, 3.4031e+02, 3.7554e+02, 3.6629e+02, 4.8494e-01, 2.0000e+00],\n",
      "        [9.7057e+01, 1.4679e+02, 1.4056e+02, 1.7674e+02, 4.4665e-01, 2.0000e+00],\n",
      "        [2.4984e+02, 1.1776e+02, 2.9706e+02, 1.4131e+02, 4.4463e-01, 2.0000e+00],\n",
      "        [2.2778e+02, 1.4864e+02, 2.7151e+02, 1.7620e+02, 4.4381e-01, 2.0000e+00],\n",
      "        [1.7714e+02, 1.6444e+02, 2.1727e+02, 1.9264e+02, 4.2209e-01, 2.0000e+00],\n",
      "        [2.5276e+02, 1.7449e+02, 3.0792e+02, 2.0010e+02, 3.6471e-01, 2.0000e+00],\n",
      "        [1.5536e+02, 1.9554e+02, 1.9835e+02, 2.2295e+02, 3.3690e-01, 2.0000e+00],\n",
      "        [2.0369e+02, 1.2610e+02, 2.3911e+02, 1.5294e+02, 3.0994e-01, 2.0000e+00],\n",
      "        [2.1202e+02, 1.9416e+02, 2.3989e+02, 2.1110e+02, 2.7987e-01, 2.0000e+00],\n",
      "        [3.0881e+02, 1.1138e+02, 3.3527e+02, 1.2667e+02, 2.5864e-01, 2.0000e+00],\n",
      "        [2.2144e+02, 9.8516e+01, 2.7358e+02, 1.1852e+02, 2.5240e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([17, 6])\n",
      "xywh: tensor([[278.3918, 352.4437,  35.5779,  25.6797],\n",
      "        [149.0781, 127.3103,  42.8792,  27.5672],\n",
      "        [295.3450,  98.3243,  32.6187,  25.3979],\n",
      "        [426.1048, 352.0621,  39.8229,  32.5950],\n",
      "        [300.2397, 150.7633,  37.0969,  24.3221],\n",
      "        [386.9097, 162.3457,  51.4353,  35.4680],\n",
      "        [352.1856, 353.2976,  46.6989,  25.9830],\n",
      "        [118.8097, 161.7684,  43.5061,  29.9478],\n",
      "        [273.4463, 129.5350,  47.2225,  23.5407],\n",
      "        [249.6445, 162.4179,  43.7318,  27.5551],\n",
      "        [197.2025, 178.5385,  40.1283,  28.1941],\n",
      "        [280.3385, 187.2934,  55.1598,  25.6035],\n",
      "        [176.8574, 209.2412,  42.9924,  27.4097],\n",
      "        [221.3979, 139.5183,  35.4224,  26.8340],\n",
      "        [225.9562, 202.6299,  27.8667,  16.9494],\n",
      "        [322.0382, 119.0260,  26.4659,  15.2929],\n",
      "        [247.5097, 108.5193,  52.1403,  20.0059]])\n",
      "xywhn: tensor([[0.5800, 0.7343, 0.0741, 0.0535],\n",
      "        [0.3106, 0.2652, 0.0893, 0.0574],\n",
      "        [0.6153, 0.2048, 0.0680, 0.0529],\n",
      "        [0.8877, 0.7335, 0.0830, 0.0679],\n",
      "        [0.6255, 0.3141, 0.0773, 0.0507],\n",
      "        [0.8061, 0.3382, 0.1072, 0.0739],\n",
      "        [0.7337, 0.7360, 0.0973, 0.0541],\n",
      "        [0.2475, 0.3370, 0.0906, 0.0624],\n",
      "        [0.5697, 0.2699, 0.0984, 0.0490],\n",
      "        [0.5201, 0.3384, 0.0911, 0.0574],\n",
      "        [0.4108, 0.3720, 0.0836, 0.0587],\n",
      "        [0.5840, 0.3902, 0.1149, 0.0533],\n",
      "        [0.3685, 0.4359, 0.0896, 0.0571],\n",
      "        [0.4612, 0.2907, 0.0738, 0.0559],\n",
      "        [0.4707, 0.4221, 0.0581, 0.0353],\n",
      "        [0.6709, 0.2480, 0.0551, 0.0319],\n",
      "        [0.5156, 0.2261, 0.1086, 0.0417]])\n",
      "xyxy: tensor([[260.6029, 339.6039, 296.1808, 365.2836],\n",
      "        [127.6385, 113.5267, 170.5177, 141.0939],\n",
      "        [279.0356,  85.6254, 311.6544, 111.0233],\n",
      "        [406.1934, 335.7646, 446.0162, 368.3596],\n",
      "        [281.6913, 138.6022, 318.7882, 162.9243],\n",
      "        [361.1921, 144.6117, 412.6274, 180.0797],\n",
      "        [328.8361, 340.3061, 375.5350, 366.2891],\n",
      "        [ 97.0567, 146.7946, 140.5628, 176.7423],\n",
      "        [249.8350, 117.7647, 297.0576, 141.3054],\n",
      "        [227.7786, 148.6404, 271.5104, 176.1954],\n",
      "        [177.1384, 164.4415, 217.2667, 192.6355],\n",
      "        [252.7586, 174.4917, 307.9184, 200.0952],\n",
      "        [155.3612, 195.5363, 198.3536, 222.9460],\n",
      "        [203.6868, 126.1013, 239.1091, 152.9353],\n",
      "        [212.0228, 194.1552, 239.8895, 211.1046],\n",
      "        [308.8052, 111.3795, 335.2712, 126.6725],\n",
      "        [221.4396,  98.5163, 273.5798, 118.5223]])\n",
      "xyxyn: tensor([[0.5429, 0.7075, 0.6170, 0.7610],\n",
      "        [0.2659, 0.2365, 0.3552, 0.2939],\n",
      "        [0.5813, 0.1784, 0.6493, 0.2313],\n",
      "        [0.8462, 0.6995, 0.9292, 0.7674],\n",
      "        [0.5869, 0.2888, 0.6641, 0.3394],\n",
      "        [0.7525, 0.3013, 0.8596, 0.3752],\n",
      "        [0.6851, 0.7090, 0.7824, 0.7631],\n",
      "        [0.2022, 0.3058, 0.2928, 0.3682],\n",
      "        [0.5205, 0.2453, 0.6189, 0.2944],\n",
      "        [0.4745, 0.3097, 0.5656, 0.3671],\n",
      "        [0.3690, 0.3426, 0.4526, 0.4013],\n",
      "        [0.5266, 0.3635, 0.6415, 0.4169],\n",
      "        [0.3237, 0.4074, 0.4132, 0.4645],\n",
      "        [0.4243, 0.2627, 0.4981, 0.3186],\n",
      "        [0.4417, 0.4045, 0.4998, 0.4398],\n",
      "        [0.6433, 0.2320, 0.6985, 0.2639],\n",
      "        [0.4613, 0.2052, 0.5700, 0.2469]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/aprepitant-80-MG-13-_jpg.rf.22dd3b2a60a04bd33d9c2465b5705ccb.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2.])\n",
      "conf: tensor([0.8824])\n",
      "data: tensor([[118.2156,   1.2510, 383.1422, 480.0000,   0.8824,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[250.6789, 240.6255, 264.9265, 478.7490]])\n",
      "xywhn: tensor([[0.5222, 0.5013, 0.5519, 0.9974]])\n",
      "xyxy: tensor([[118.2156,   1.2510, 383.1422, 480.0000]])\n",
      "xyxyn: tensor([[0.2463, 0.0026, 0.7982, 1.0000]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130555_jpg.rf.af69b3438569e435289ac3ccf52b2022.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.7771, 0.6661, 0.5701, 0.5270, 0.4547])\n",
      "data: tensor([[333.5677, 178.0308, 366.5393, 249.6532,   0.7771,   1.0000],\n",
      "        [187.9591, 296.6383, 228.7766, 343.3214,   0.6661,   1.0000],\n",
      "        [335.4225, 266.8456, 378.4758, 340.1577,   0.5701,   1.0000],\n",
      "        [145.1057, 285.4172, 187.4994, 341.6649,   0.5270,   1.0000],\n",
      "        [264.0707, 307.5892, 304.9916, 342.6150,   0.4547,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[350.0535, 213.8420,  32.9716,  71.6224],\n",
      "        [208.3679, 319.9799,  40.8175,  46.6831],\n",
      "        [356.9492, 303.5016,  43.0533,  73.3121],\n",
      "        [166.3026, 313.5411,  42.3937,  56.2477],\n",
      "        [284.5312, 325.1021,  40.9208,  35.0258]])\n",
      "xywhn: tensor([[0.7293, 0.4455, 0.0687, 0.1492],\n",
      "        [0.4341, 0.6666, 0.0850, 0.0973],\n",
      "        [0.7436, 0.6323, 0.0897, 0.1527],\n",
      "        [0.3465, 0.6532, 0.0883, 0.1172],\n",
      "        [0.5928, 0.6773, 0.0853, 0.0730]])\n",
      "xyxy: tensor([[333.5677, 178.0308, 366.5393, 249.6532],\n",
      "        [187.9591, 296.6383, 228.7766, 343.3214],\n",
      "        [335.4225, 266.8456, 378.4758, 340.1577],\n",
      "        [145.1057, 285.4172, 187.4994, 341.6649],\n",
      "        [264.0707, 307.5892, 304.9916, 342.6150]])\n",
      "xyxyn: tensor([[0.6949, 0.3709, 0.7636, 0.5201],\n",
      "        [0.3916, 0.6180, 0.4766, 0.7153],\n",
      "        [0.6988, 0.5559, 0.7885, 0.7087],\n",
      "        [0.3023, 0.5946, 0.3906, 0.7118],\n",
      "        [0.5501, 0.6408, 0.6354, 0.7138]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/carvedilol-3-125-MG-5-Copy_jpg.rf.50227a2a80eace14d269a6924ebfe514.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9065, 0.8629])\n",
      "data: tensor([[115.1123,   6.1018, 390.4848, 226.8095,   0.9065,   2.0000],\n",
      "        [115.0305, 227.3158, 390.0182, 460.7313,   0.8629,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[252.7986, 116.4557, 275.3725, 220.7078],\n",
      "        [252.5244, 344.0235, 274.9878, 233.4155]])\n",
      "xywhn: tensor([[0.5267, 0.2426, 0.5737, 0.4598],\n",
      "        [0.5261, 0.7167, 0.5729, 0.4863]])\n",
      "xyxy: tensor([[115.1123,   6.1018, 390.4848, 226.8095],\n",
      "        [115.0305, 227.3158, 390.0182, 460.7313]])\n",
      "xyxyn: tensor([[0.2398, 0.0127, 0.8135, 0.4725],\n",
      "        [0.2396, 0.4736, 0.8125, 0.9599]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_152007_Pinterest_jpg.rf.3eb8f0440cf6fe6d34dece18755607a5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8276])\n",
      "data: tensor([[ 54.2711, 104.0464, 478.3741, 281.4518,   0.8276,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[266.3226, 192.7491, 424.1030, 177.4054]])\n",
      "xywhn: tensor([[0.5548, 0.4016, 0.8835, 0.3696]])\n",
      "xyxy: tensor([[ 54.2711, 104.0464, 478.3741, 281.4518]])\n",
      "xyxyn: tensor([[0.1131, 0.2168, 0.9966, 0.5864]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100005_Pinterest_jpg.rf.678060ab21c6e3bf0ec5f687ddc8cded.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8156, 0.3512])\n",
      "data: tensor([[1.7061e+01, 3.2282e+01, 4.5245e+02, 3.1420e+02, 8.1560e-01, 0.0000e+00],\n",
      "        [1.1999e+02, 2.9115e+01, 4.3815e+02, 3.0123e+02, 3.5116e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[234.7551, 173.2400, 435.3889, 281.9158],\n",
      "        [279.0700, 165.1734, 318.1659, 272.1165]])\n",
      "xywhn: tensor([[0.4891, 0.3609, 0.9071, 0.5873],\n",
      "        [0.5814, 0.3441, 0.6628, 0.5669]])\n",
      "xyxy: tensor([[ 17.0606,  32.2820, 452.4495, 314.1979],\n",
      "        [119.9871,  29.1151, 438.1530, 301.2316]])\n",
      "xyxyn: tensor([[0.0355, 0.0673, 0.9426, 0.6546],\n",
      "        [0.2500, 0.0607, 0.9128, 0.6276]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Oseltamivir-45-MG-6-_jpg.rf.755a2091fd9eeabd66a03d888e510d9e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9473, 0.9459])\n",
      "data: tensor([[ 30.9275,   2.8210, 470.2544, 189.6617,   0.9473,   2.0000],\n",
      "        [ 30.4700, 191.8623, 477.5099, 390.6424,   0.9459,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[250.5909,  96.2413, 439.3268, 186.8407],\n",
      "        [253.9900, 291.2523, 447.0399, 198.7801]])\n",
      "xywhn: tensor([[0.5221, 0.2005, 0.9153, 0.3893],\n",
      "        [0.5291, 0.6068, 0.9313, 0.4141]])\n",
      "xyxy: tensor([[ 30.9275,   2.8210, 470.2544, 189.6617],\n",
      "        [ 30.4700, 191.8623, 477.5099, 390.6424]])\n",
      "xyxyn: tensor([[0.0644, 0.0059, 0.9797, 0.3951],\n",
      "        [0.0635, 0.3997, 0.9948, 0.8138]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150803_Pinterest_jpg.rf.0889529e31ab9be5218171909eee43d1.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0.])\n",
      "conf: tensor([0.7857, 0.7598, 0.6945, 0.6148])\n",
      "data: tensor([[263.5645, 230.1087, 459.4758, 382.0066,   0.7857,   0.0000],\n",
      "        [ 51.6480, 180.4789, 204.5535, 321.3681,   0.7598,   0.0000],\n",
      "        [243.1803, 114.0805, 345.4829, 190.5915,   0.6945,   0.0000],\n",
      "        [337.0647, 100.7941, 436.8384, 174.0910,   0.6148,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[361.5201, 306.0576, 195.9113, 151.8979],\n",
      "        [128.1008, 250.9235, 152.9055, 140.8892],\n",
      "        [294.3316, 152.3360, 102.3026,  76.5109],\n",
      "        [386.9515, 137.4425,  99.7737,  73.2969]])\n",
      "xywhn: tensor([[0.7532, 0.6376, 0.4081, 0.3165],\n",
      "        [0.2669, 0.5228, 0.3186, 0.2935],\n",
      "        [0.6132, 0.3174, 0.2131, 0.1594],\n",
      "        [0.8061, 0.2863, 0.2079, 0.1527]])\n",
      "xyxy: tensor([[263.5645, 230.1087, 459.4758, 382.0066],\n",
      "        [ 51.6480, 180.4789, 204.5535, 321.3681],\n",
      "        [243.1803, 114.0805, 345.4829, 190.5915],\n",
      "        [337.0647, 100.7941, 436.8384, 174.0910]])\n",
      "xyxyn: tensor([[0.5491, 0.4794, 0.9572, 0.7958],\n",
      "        [0.1076, 0.3760, 0.4262, 0.6695],\n",
      "        [0.5066, 0.2377, 0.7198, 0.3971],\n",
      "        [0.7022, 0.2100, 0.9101, 0.3627]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094931_Pinterest_jpg.rf.6360e2b2e54ee14bd88b38b44226b8bc.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8140])\n",
      "data: tensor([[ 37.8148,  65.3657, 400.1452, 331.0920,   0.8140,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[218.9800, 198.2289, 362.3304, 265.7264]])\n",
      "xywhn: tensor([[0.4562, 0.4130, 0.7549, 0.5536]])\n",
      "xyxy: tensor([[ 37.8148,  65.3657, 400.1452, 331.0920]])\n",
      "xyxyn: tensor([[0.0788, 0.1362, 0.8336, 0.6898]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100054_Pinterest_jpg.rf.8a92ff4e2a379268128ca46352988fb6.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7406])\n",
      "data: tensor([[ 46.3267,  26.5062, 456.2601, 351.8203,   0.7406,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[251.2934, 189.1632, 409.9333, 325.3141]])\n",
      "xywhn: tensor([[0.5235, 0.3941, 0.8540, 0.6777]])\n",
      "xyxy: tensor([[ 46.3267,  26.5062, 456.2601, 351.8203]])\n",
      "xyxyn: tensor([[0.0965, 0.0552, 0.9505, 0.7330]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150739_Pinterest_jpg.rf.98db28037e042799dc6000369bf51a4a.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.9078, 0.8617, 0.7564, 0.7092, 0.6837, 0.6624])\n",
      "data: tensor([[255.9944,  19.3433, 473.9764, 128.0687,   0.9078,   0.0000],\n",
      "        [ 11.7684,  14.5199, 234.6443, 126.5098,   0.8617,   0.0000],\n",
      "        [  6.6229, 242.3093, 219.7566, 350.6924,   0.7564,   0.0000],\n",
      "        [ 13.2574, 128.6154, 218.6432, 237.6005,   0.7092,   0.0000],\n",
      "        [250.0845, 132.6729, 446.3253, 236.3712,   0.6837,   0.0000],\n",
      "        [252.7537, 238.2616, 452.5609, 355.0830,   0.6624,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[364.9854,  73.7060, 217.9820, 108.7254],\n",
      "        [123.2063,  70.5148, 222.8758, 111.9899],\n",
      "        [113.1897, 296.5009, 213.1337, 108.3831],\n",
      "        [115.9503, 183.1079, 205.3858, 108.9851],\n",
      "        [348.2049, 184.5220, 196.2408, 103.6983],\n",
      "        [352.6573, 296.6723, 199.8072, 116.8214]])\n",
      "xywhn: tensor([[0.7604, 0.1536, 0.4541, 0.2265],\n",
      "        [0.2567, 0.1469, 0.4643, 0.2333],\n",
      "        [0.2358, 0.6177, 0.4440, 0.2258],\n",
      "        [0.2416, 0.3815, 0.4279, 0.2271],\n",
      "        [0.7254, 0.3844, 0.4088, 0.2160],\n",
      "        [0.7347, 0.6181, 0.4163, 0.2434]])\n",
      "xyxy: tensor([[255.9944,  19.3433, 473.9764, 128.0687],\n",
      "        [ 11.7684,  14.5199, 234.6443, 126.5098],\n",
      "        [  6.6229, 242.3093, 219.7566, 350.6924],\n",
      "        [ 13.2574, 128.6154, 218.6432, 237.6005],\n",
      "        [250.0845, 132.6729, 446.3253, 236.3712],\n",
      "        [252.7537, 238.2616, 452.5609, 355.0830]])\n",
      "xyxyn: tensor([[0.5333, 0.0403, 0.9875, 0.2668],\n",
      "        [0.0245, 0.0302, 0.4888, 0.2636],\n",
      "        [0.0138, 0.5048, 0.4578, 0.7306],\n",
      "        [0.0276, 0.2679, 0.4555, 0.4950],\n",
      "        [0.5210, 0.2764, 0.9298, 0.4924],\n",
      "        [0.5266, 0.4964, 0.9428, 0.7398]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100013_Pinterest_jpg.rf.991941b3808fa9249bd2f078893337ca.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6164])\n",
      "data: tensor([[ 35.6532,  64.8707, 455.6686, 300.3665,   0.6164,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[245.6609, 182.6186, 420.0154, 235.4958]])\n",
      "xywhn: tensor([[0.5118, 0.3805, 0.8750, 0.4906]])\n",
      "xyxy: tensor([[ 35.6532,  64.8707, 455.6686, 300.3665]])\n",
      "xyxyn: tensor([[0.0743, 0.1351, 0.9493, 0.6258]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/pitavastatin-1-MG-11-Copy_jpg.rf.804c18270ec8d3a154ad02cdd4db59ff.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9362, 0.9249])\n",
      "data: tensor([[247.3212,  49.1170, 477.9076, 340.1346,   0.9362,   2.0000],\n",
      "        [ 15.6027,  50.1093, 248.0847, 341.0617,   0.9249,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[362.6144, 194.6258, 230.5864, 291.0176],\n",
      "        [131.8437, 195.5855, 232.4820, 290.9524]])\n",
      "xywhn: tensor([[0.7554, 0.4055, 0.4804, 0.6063],\n",
      "        [0.2747, 0.4075, 0.4843, 0.6062]])\n",
      "xyxy: tensor([[247.3212,  49.1170, 477.9076, 340.1346],\n",
      "        [ 15.6027,  50.1093, 248.0847, 341.0617]])\n",
      "xyxyn: tensor([[0.5153, 0.1023, 0.9956, 0.7086],\n",
      "        [0.0325, 0.1044, 0.5168, 0.7105]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100642_Pinterest_jpg.rf.692bd27b877db6ec312b2d38e61c8563.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.5921])\n",
      "data: tensor([[165.2537,  31.7504, 475.0327, 149.5792,   0.5921,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[320.1432,  90.6648, 309.7790, 117.8288]])\n",
      "xywhn: tensor([[0.6670, 0.1889, 0.6454, 0.2455]])\n",
      "xyxy: tensor([[165.2537,  31.7504, 475.0327, 149.5792]])\n",
      "xyxyn: tensor([[0.3443, 0.0661, 0.9897, 0.3116]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240531_092741_Chrome_jpg.rf.e1bd8ccabc60c6c116526de7a789f207.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "conf: tensor([0.7557, 0.7399, 0.7366, 0.6869, 0.6677, 0.6666, 0.6410, 0.6388, 0.6123, 0.6035, 0.5865, 0.5454, 0.5011, 0.4380, 0.4158, 0.3596, 0.3337, 0.3327, 0.3305, 0.2802])\n",
      "data: tensor([[3.3130e+02, 1.3419e+02, 3.7891e+02, 1.6117e+02, 7.5571e-01, 2.0000e+00],\n",
      "        [1.3909e+02, 1.9509e+02, 2.0961e+02, 2.2654e+02, 7.3993e-01, 2.0000e+00],\n",
      "        [2.5335e+02, 1.4477e+02, 3.1992e+02, 1.7493e+02, 7.3657e-01, 2.0000e+00],\n",
      "        [3.1046e+02, 1.6725e+02, 3.7061e+02, 1.9398e+02, 6.8690e-01, 2.0000e+00],\n",
      "        [2.2231e+02, 1.8063e+02, 2.9744e+02, 2.1195e+02, 6.6775e-01, 2.0000e+00],\n",
      "        [1.2073e+02, 9.9660e+01, 1.9371e+02, 1.6903e+02, 6.6655e-01, 2.0000e+00],\n",
      "        [1.7907e+02, 1.5888e+02, 2.4565e+02, 1.8658e+02, 6.4105e-01, 2.0000e+00],\n",
      "        [9.2031e+01, 1.7393e+02, 1.6201e+02, 2.0245e+02, 6.3881e-01, 2.0000e+00],\n",
      "        [1.9334e+02, 1.1913e+02, 2.8957e+02, 1.5031e+02, 6.1234e-01, 2.0000e+00],\n",
      "        [3.9802e+01, 1.1154e+02, 1.2257e+02, 1.7868e+02, 6.0347e-01, 2.0000e+00],\n",
      "        [1.8147e+02, 3.5502e+02, 2.2069e+02, 3.8662e+02, 5.8649e-01, 2.0000e+00],\n",
      "        [3.6457e+01, 3.5718e+02, 7.2393e+01, 3.8455e+02, 5.4542e-01, 2.0000e+00],\n",
      "        [2.9538e+02, 1.1917e+02, 3.2729e+02, 1.3845e+02, 5.0106e-01, 2.0000e+00],\n",
      "        [3.6929e+02, 3.5310e+02, 3.9832e+02, 3.7171e+02, 4.3801e-01, 2.0000e+00],\n",
      "        [1.0706e+02, 3.5853e+02, 1.5089e+02, 3.8398e+02, 4.1585e-01, 2.0000e+00],\n",
      "        [2.4373e+02, 4.1528e+02, 2.8692e+02, 4.3575e+02, 3.5957e-01, 2.0000e+00],\n",
      "        [3.3676e+02, 3.5098e+02, 3.6794e+02, 3.7206e+02, 3.3373e-01, 2.0000e+00],\n",
      "        [2.7316e+02, 3.5098e+02, 3.0580e+02, 3.7164e+02, 3.3275e-01, 2.0000e+00],\n",
      "        [2.4386e+02, 4.3297e+02, 2.8678e+02, 4.5351e+02, 3.3049e-01, 2.0000e+00],\n",
      "        [2.9365e+02, 1.1419e+02, 3.2970e+02, 1.3690e+02, 2.8020e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([20, 6])\n",
      "xywh: tensor([[355.1058, 147.6761,  47.6064,  26.9816],\n",
      "        [174.3492, 210.8151,  70.5176,  31.4515],\n",
      "        [286.6363, 159.8499,  66.5741,  30.1559],\n",
      "        [340.5335, 180.6176,  60.1482,  26.7253],\n",
      "        [259.8735, 196.2903,  75.1273,  31.3221],\n",
      "        [157.2201, 134.3445,  72.9735,  69.3690],\n",
      "        [212.3575, 172.7340,  66.5809,  27.6980],\n",
      "        [127.0226, 188.1905,  69.9830,  28.5260],\n",
      "        [241.4530, 134.7224,  96.2350,  31.1751],\n",
      "        [ 81.1859, 145.1099,  82.7684,  67.1460],\n",
      "        [201.0803, 370.8185,  39.2292,  31.6017],\n",
      "        [ 54.4253, 370.8624,  35.9361,  27.3687],\n",
      "        [311.3373, 128.8091,  31.9113,  19.2760],\n",
      "        [383.8058, 362.4052,  29.0362,  18.6144],\n",
      "        [128.9764, 371.2548,  43.8292,  25.4543],\n",
      "        [265.3279, 425.5135,  43.1888,  20.4670],\n",
      "        [352.3497, 361.5222,  31.1886,  21.0776],\n",
      "        [289.4813, 361.3113,  32.6445,  20.6631],\n",
      "        [265.3203, 443.2415,  42.9142,  20.5352],\n",
      "        [311.6769, 125.5483,  36.0497,  22.7113]])\n",
      "xywhn: tensor([[0.7398, 0.3077, 0.0992, 0.0562],\n",
      "        [0.3632, 0.4392, 0.1469, 0.0655],\n",
      "        [0.5972, 0.3330, 0.1387, 0.0628],\n",
      "        [0.7094, 0.3763, 0.1253, 0.0557],\n",
      "        [0.5414, 0.4089, 0.1565, 0.0653],\n",
      "        [0.3275, 0.2799, 0.1520, 0.1445],\n",
      "        [0.4424, 0.3599, 0.1387, 0.0577],\n",
      "        [0.2646, 0.3921, 0.1458, 0.0594],\n",
      "        [0.5030, 0.2807, 0.2005, 0.0649],\n",
      "        [0.1691, 0.3023, 0.1724, 0.1399],\n",
      "        [0.4189, 0.7725, 0.0817, 0.0658],\n",
      "        [0.1134, 0.7726, 0.0749, 0.0570],\n",
      "        [0.6486, 0.2684, 0.0665, 0.0402],\n",
      "        [0.7996, 0.7550, 0.0605, 0.0388],\n",
      "        [0.2687, 0.7734, 0.0913, 0.0530],\n",
      "        [0.5528, 0.8865, 0.0900, 0.0426],\n",
      "        [0.7341, 0.7532, 0.0650, 0.0439],\n",
      "        [0.6031, 0.7527, 0.0680, 0.0430],\n",
      "        [0.5528, 0.9234, 0.0894, 0.0428],\n",
      "        [0.6493, 0.2616, 0.0751, 0.0473]])\n",
      "xyxy: tensor([[331.3026, 134.1853, 378.9090, 161.1669],\n",
      "        [139.0904, 195.0894, 209.6080, 226.5409],\n",
      "        [253.3493, 144.7719, 319.9233, 174.9278],\n",
      "        [310.4594, 167.2550, 370.6076, 193.9802],\n",
      "        [222.3098, 180.6292, 297.4371, 211.9513],\n",
      "        [120.7334,  99.6600, 193.7068, 169.0289],\n",
      "        [179.0670, 158.8850, 245.6479, 186.5830],\n",
      "        [ 92.0311, 173.9275, 162.0141, 202.4535],\n",
      "        [193.3355, 119.1349, 289.5705, 150.3100],\n",
      "        [ 39.8017, 111.5369, 122.5701, 178.6829],\n",
      "        [181.4657, 355.0176, 220.6949, 386.6193],\n",
      "        [ 36.4572, 357.1780,  72.3933, 384.5467],\n",
      "        [295.3817, 119.1711, 327.2930, 138.4471],\n",
      "        [369.2878, 353.0980, 398.3239, 371.7124],\n",
      "        [107.0618, 358.5276, 150.8910, 383.9819],\n",
      "        [243.7334, 415.2800, 286.9223, 435.7471],\n",
      "        [336.7554, 350.9833, 367.9440, 372.0610],\n",
      "        [273.1591, 350.9798, 305.8036, 371.6429],\n",
      "        [243.8632, 432.9739, 286.7774, 453.5091],\n",
      "        [293.6520, 114.1927, 329.7017, 136.9040]])\n",
      "xyxyn: tensor([[0.6902, 0.2796, 0.7894, 0.3358],\n",
      "        [0.2898, 0.4064, 0.4367, 0.4720],\n",
      "        [0.5278, 0.3016, 0.6665, 0.3644],\n",
      "        [0.6468, 0.3484, 0.7721, 0.4041],\n",
      "        [0.4631, 0.3763, 0.6197, 0.4416],\n",
      "        [0.2515, 0.2076, 0.4036, 0.3521],\n",
      "        [0.3731, 0.3310, 0.5118, 0.3887],\n",
      "        [0.1917, 0.3623, 0.3375, 0.4218],\n",
      "        [0.4028, 0.2482, 0.6033, 0.3131],\n",
      "        [0.0829, 0.2324, 0.2554, 0.3723],\n",
      "        [0.3781, 0.7396, 0.4598, 0.8055],\n",
      "        [0.0760, 0.7441, 0.1508, 0.8011],\n",
      "        [0.6154, 0.2483, 0.6819, 0.2884],\n",
      "        [0.7693, 0.7356, 0.8298, 0.7744],\n",
      "        [0.2230, 0.7469, 0.3144, 0.8000],\n",
      "        [0.5078, 0.8652, 0.5978, 0.9078],\n",
      "        [0.7016, 0.7312, 0.7665, 0.7751],\n",
      "        [0.5691, 0.7312, 0.6371, 0.7743],\n",
      "        [0.5080, 0.9020, 0.5975, 0.9448],\n",
      "        [0.6118, 0.2379, 0.6869, 0.2852]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240523_083222_jpg.rf.9141a62bb5b647846b42f9450d5d24d3.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8101])\n",
      "data: tensor([[ 86.4012, 154.8590, 262.9149, 304.4553,   0.8101,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[174.6581, 229.6571, 176.5138, 149.5963]])\n",
      "xywhn: tensor([[0.3639, 0.4785, 0.3677, 0.3117]])\n",
      "xyxy: tensor([[ 86.4012, 154.8590, 262.9149, 304.4553]])\n",
      "xyxyn: tensor([[0.1800, 0.3226, 0.5477, 0.6343]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100541_Pinterest_jpg.rf.f795c5c218b86692db9deb559a52e184.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8285])\n",
      "data: tensor([[153.4947,  44.0735, 440.6668, 207.8405,   0.8285,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[297.0807, 125.9570, 287.1721, 163.7669]])\n",
      "xywhn: tensor([[0.6189, 0.2624, 0.5983, 0.3412]])\n",
      "xyxy: tensor([[153.4947,  44.0735, 440.6668, 207.8405]])\n",
      "xyxyn: tensor([[0.3198, 0.0918, 0.9181, 0.4330]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094840_Pinterest_jpg.rf.6b3b6c05405ef5828f547c07bfdd1342.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.7042, 0.5748])\n",
      "data: tensor([[ 42.5000,  43.8425, 269.2661, 218.1665,   0.7042,   0.0000],\n",
      "        [233.6965,  73.6427, 429.3978, 216.6425,   0.5748,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[155.8831, 131.0045, 226.7661, 174.3240],\n",
      "        [331.5472, 145.1426, 195.7013, 142.9998]])\n",
      "xywhn: tensor([[0.3248, 0.2729, 0.4724, 0.3632],\n",
      "        [0.6907, 0.3024, 0.4077, 0.2979]])\n",
      "xyxy: tensor([[ 42.5000,  43.8425, 269.2661, 218.1665],\n",
      "        [233.6965,  73.6427, 429.3978, 216.6425]])\n",
      "xyxyn: tensor([[0.0885, 0.0913, 0.5610, 0.4545],\n",
      "        [0.4869, 0.1534, 0.8946, 0.4513]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_095258_Pinterest_jpg.rf.8a5b9ceff5e2adacd867f3cdad37948e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8060])\n",
      "data: tensor([[ 67.0198,  10.7643, 470.4732, 268.4144,   0.8060,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[268.7465, 139.5893, 403.4534, 257.6502]])\n",
      "xywhn: tensor([[0.5599, 0.2908, 0.8405, 0.5368]])\n",
      "xyxy: tensor([[ 67.0198,  10.7643, 470.4732, 268.4144]])\n",
      "xyxyn: tensor([[0.1396, 0.0224, 0.9802, 0.5592]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150947_Pinterest_jpg.rf.c2ee92c5ffb3ea3782f6541d7aa8f511.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.7809, 0.5587, 0.5161, 0.4869, 0.4715, 0.4695, 0.4032, 0.2913, 0.2818, 0.2689, 0.2507])\n",
      "data: tensor([[2.7299e+02, 2.0930e+02, 4.4800e+02, 3.3102e+02, 7.8085e-01, 0.0000e+00],\n",
      "        [9.0632e+01, 1.0813e+02, 1.7447e+02, 1.9820e+02, 5.5872e-01, 0.0000e+00],\n",
      "        [4.1730e+01, 1.4810e+02, 1.2831e+02, 2.2672e+02, 5.1609e-01, 0.0000e+00],\n",
      "        [2.5014e+02, 9.0134e+01, 3.7199e+02, 1.2718e+02, 4.8695e-01, 0.0000e+00],\n",
      "        [2.9205e+02, 1.1875e+02, 4.3701e+02, 1.6727e+02, 4.7151e-01, 0.0000e+00],\n",
      "        [1.2495e+02, 3.5687e+02, 2.5691e+02, 3.9918e+02, 4.6953e-01, 0.0000e+00],\n",
      "        [7.0380e+01, 3.2974e+02, 1.4633e+02, 3.8491e+02, 4.0316e-01, 0.0000e+00],\n",
      "        [8.2389e+01, 2.6549e+02, 1.7339e+02, 2.9600e+02, 2.9131e-01, 0.0000e+00],\n",
      "        [1.0122e+02, 2.9453e+02, 1.9123e+02, 3.3588e+02, 2.8183e-01, 0.0000e+00],\n",
      "        [1.0341e+02, 2.9277e+02, 2.1305e+02, 3.4076e+02, 2.6889e-01, 0.0000e+00],\n",
      "        [1.2988e+02, 3.5733e+02, 2.1255e+02, 3.9881e+02, 2.5072e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([11, 6])\n",
      "xywh: tensor([[360.4940, 270.1621, 175.0076, 121.7197],\n",
      "        [132.5494, 153.1675,  83.8338,  90.0719],\n",
      "        [ 85.0223, 187.4117,  86.5845,  78.6254],\n",
      "        [311.0616, 108.6572, 121.8494,  37.0462],\n",
      "        [364.5273, 143.0134, 144.9561,  48.5198],\n",
      "        [190.9281, 378.0272, 131.9576,  42.3112],\n",
      "        [108.3565, 357.3221,  75.9522,  55.1702],\n",
      "        [127.8920, 280.7463,  91.0052,  30.5106],\n",
      "        [146.2224, 315.2063,  90.0141,  41.3528],\n",
      "        [158.2299, 316.7633, 109.6360,  47.9868],\n",
      "        [171.2170, 378.0685,  82.6723,  41.4819]])\n",
      "xywhn: tensor([[0.7510, 0.5628, 0.3646, 0.2536],\n",
      "        [0.2761, 0.3191, 0.1747, 0.1876],\n",
      "        [0.1771, 0.3904, 0.1804, 0.1638],\n",
      "        [0.6480, 0.2264, 0.2539, 0.0772],\n",
      "        [0.7594, 0.2979, 0.3020, 0.1011],\n",
      "        [0.3978, 0.7876, 0.2749, 0.0881],\n",
      "        [0.2257, 0.7444, 0.1582, 0.1149],\n",
      "        [0.2664, 0.5849, 0.1896, 0.0636],\n",
      "        [0.3046, 0.6567, 0.1875, 0.0862],\n",
      "        [0.3296, 0.6599, 0.2284, 0.1000],\n",
      "        [0.3567, 0.7876, 0.1722, 0.0864]])\n",
      "xyxy: tensor([[272.9902, 209.3022, 447.9977, 331.0219],\n",
      "        [ 90.6325, 108.1316, 174.4663, 198.2035],\n",
      "        [ 41.7300, 148.0990, 128.3145, 226.7244],\n",
      "        [250.1369,  90.1340, 371.9863, 127.1803],\n",
      "        [292.0493, 118.7535, 437.0054, 167.2733],\n",
      "        [124.9493, 356.8716, 256.9069, 399.1828],\n",
      "        [ 70.3804, 329.7371, 146.3326, 384.9072],\n",
      "        [ 82.3894, 265.4910, 173.3946, 296.0016],\n",
      "        [101.2154, 294.5299, 191.2295, 335.8827],\n",
      "        [103.4120, 292.7699, 213.0479, 340.7567],\n",
      "        [129.8809, 357.3276, 212.5532, 398.8095]])\n",
      "xyxyn: tensor([[0.5687, 0.4360, 0.9333, 0.6896],\n",
      "        [0.1888, 0.2253, 0.3635, 0.4129],\n",
      "        [0.0869, 0.3085, 0.2673, 0.4723],\n",
      "        [0.5211, 0.1878, 0.7750, 0.2650],\n",
      "        [0.6084, 0.2474, 0.9104, 0.3485],\n",
      "        [0.2603, 0.7435, 0.5352, 0.8316],\n",
      "        [0.1466, 0.6870, 0.3049, 0.8019],\n",
      "        [0.1716, 0.5531, 0.3612, 0.6167],\n",
      "        [0.2109, 0.6136, 0.3984, 0.6998],\n",
      "        [0.2154, 0.6099, 0.4438, 0.7099],\n",
      "        [0.2706, 0.7444, 0.4428, 0.8309]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/5b9bd9351f00002c002100a3_jpeg.rf.828c3ec3faf48a09dd4bbd6ded4c937f.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.6462, 0.6273])\n",
      "data: tensor([[236.1646, 310.0568, 273.5262, 373.3218,   0.6462,   2.0000],\n",
      "        [222.1966, 259.6575, 268.5539, 316.2559,   0.6273,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[254.8454, 341.6893,  37.3616,  63.2650],\n",
      "        [245.3752, 287.9567,  46.3573,  56.5984]])\n",
      "xywhn: tensor([[0.5309, 0.7119, 0.0778, 0.1318],\n",
      "        [0.5112, 0.5999, 0.0966, 0.1179]])\n",
      "xyxy: tensor([[236.1646, 310.0568, 273.5262, 373.3218],\n",
      "        [222.1966, 259.6575, 268.5539, 316.2559]])\n",
      "xyxyn: tensor([[0.4920, 0.6460, 0.5698, 0.7778],\n",
      "        [0.4629, 0.5410, 0.5595, 0.6589]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240531_092725_Chrome_jpg.rf.b20d5c6254aecfad2709f702c15e0e24.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "conf: tensor([0.9269, 0.8268, 0.8260, 0.8013, 0.7711, 0.7313, 0.6360, 0.6066, 0.6065, 0.5734, 0.5346, 0.5213, 0.4932, 0.4802, 0.4693, 0.4656, 0.4017, 0.3699])\n",
      "data: tensor([[3.3434e+02, 1.8276e+02, 4.7939e+02, 2.4660e+02, 9.2687e-01, 2.0000e+00],\n",
      "        [3.9294e+02, 8.1954e+01, 4.7514e+02, 1.4980e+02, 8.2679e-01, 2.0000e+00],\n",
      "        [1.9854e+02, 1.5386e+02, 3.3687e+02, 2.2110e+02, 8.2602e-01, 2.0000e+00],\n",
      "        [3.2255e+00, 8.2343e+01, 9.4484e+01, 1.2033e+02, 8.0132e-01, 2.0000e+00],\n",
      "        [3.0191e+00, 1.1972e+02, 9.3413e+01, 1.5945e+02, 7.7109e-01, 2.0000e+00],\n",
      "        [1.2725e+02, 2.3648e+02, 1.9006e+02, 2.9928e+02, 7.3128e-01, 2.0000e+00],\n",
      "        [1.1867e+02, 1.5134e+02, 2.0921e+02, 1.9117e+02, 6.3602e-01, 2.0000e+00],\n",
      "        [1.8450e+02, 2.3083e+02, 2.8755e+02, 3.0061e+02, 6.0664e-01, 2.0000e+00],\n",
      "        [2.3998e+02, 8.1564e+01, 3.1509e+02, 1.5490e+02, 6.0651e-01, 2.0000e+00],\n",
      "        [9.4122e+01, 8.1683e+01, 1.7147e+02, 1.5411e+02, 5.7344e-01, 2.0000e+00],\n",
      "        [2.8511e+00, 1.5984e+02, 1.1724e+02, 2.0642e+02, 5.3457e-01, 2.0000e+00],\n",
      "        [3.1625e+02, 1.1928e+02, 3.9921e+02, 1.8998e+02, 5.2128e-01, 2.0000e+00],\n",
      "        [3.4176e+02, 2.4214e+02, 4.7299e+02, 3.0248e+02, 4.9319e-01, 2.0000e+00],\n",
      "        [8.1427e-01, 2.0518e+02, 1.4998e+02, 2.4266e+02, 4.8015e-01, 2.0000e+00],\n",
      "        [2.8116e+02, 2.2681e+02, 3.5076e+02, 3.0075e+02, 4.6931e-01, 2.0000e+00],\n",
      "        [6.8957e+01, 2.3970e+02, 1.3311e+02, 3.0268e+02, 4.6559e-01, 2.0000e+00],\n",
      "        [3.0651e+02, 8.1842e+01, 3.9784e+02, 1.1920e+02, 4.0169e-01, 2.0000e+00],\n",
      "        [1.6778e+02, 8.1287e+01, 2.4360e+02, 1.5587e+02, 3.6988e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([18, 6])\n",
      "xywh: tensor([[406.8642, 214.6782, 145.0540,  63.8389],\n",
      "        [434.0393, 115.8794,  82.2020,  67.8504],\n",
      "        [267.7070, 187.4808, 138.3319,  67.2384],\n",
      "        [ 48.8549, 101.3377,  91.2588,  37.9893],\n",
      "        [ 48.2158, 139.5873,  90.3934,  39.7303],\n",
      "        [158.6543, 267.8779,  62.8113,  62.7988],\n",
      "        [163.9403, 171.2551,  90.5430,  39.8248],\n",
      "        [236.0259, 265.7205, 103.0455,  69.7737],\n",
      "        [277.5342, 118.2319,  75.1033,  73.3351],\n",
      "        [132.7939, 117.8964,  77.3433,  72.4271],\n",
      "        [ 60.0434, 183.1330, 114.3846,  46.5801],\n",
      "        [357.7271, 154.6340,  82.9573,  70.6994],\n",
      "        [407.3769, 272.3064, 131.2291,  60.3420],\n",
      "        [ 75.3977, 223.9157, 149.1668,  37.4794],\n",
      "        [315.9608, 263.7766,  69.6073,  73.9404],\n",
      "        [101.0350, 271.1895,  64.1552,  62.9756],\n",
      "        [352.1726, 100.5209,  91.3324,  37.3578],\n",
      "        [205.6878, 118.5782,  75.8242,  74.5819]])\n",
      "xywhn: tensor([[0.8476, 0.4472, 0.3022, 0.1330],\n",
      "        [0.9042, 0.2414, 0.1713, 0.1414],\n",
      "        [0.5577, 0.3906, 0.2882, 0.1401],\n",
      "        [0.1018, 0.2111, 0.1901, 0.0791],\n",
      "        [0.1004, 0.2908, 0.1883, 0.0828],\n",
      "        [0.3305, 0.5581, 0.1309, 0.1308],\n",
      "        [0.3415, 0.3568, 0.1886, 0.0830],\n",
      "        [0.4917, 0.5536, 0.2147, 0.1454],\n",
      "        [0.5782, 0.2463, 0.1565, 0.1528],\n",
      "        [0.2767, 0.2456, 0.1611, 0.1509],\n",
      "        [0.1251, 0.3815, 0.2383, 0.0970],\n",
      "        [0.7453, 0.3222, 0.1728, 0.1473],\n",
      "        [0.8487, 0.5673, 0.2734, 0.1257],\n",
      "        [0.1571, 0.4665, 0.3108, 0.0781],\n",
      "        [0.6583, 0.5495, 0.1450, 0.1540],\n",
      "        [0.2105, 0.5650, 0.1337, 0.1312],\n",
      "        [0.7337, 0.2094, 0.1903, 0.0778],\n",
      "        [0.4285, 0.2470, 0.1580, 0.1554]])\n",
      "xyxy: tensor([[334.3372, 182.7587, 479.3912, 246.5977],\n",
      "        [392.9383,  81.9542, 475.1403, 149.8046],\n",
      "        [198.5411, 153.8616, 336.8730, 221.1000],\n",
      "        [  3.2255,  82.3431,  94.4843, 120.3324],\n",
      "        [  3.0191, 119.7221,  93.4125, 159.4525],\n",
      "        [127.2486, 236.4785, 190.0599, 299.2773],\n",
      "        [118.6688, 151.3427, 209.2118, 191.1674],\n",
      "        [184.5032, 230.8336, 287.5487, 300.6073],\n",
      "        [239.9826,  81.5643, 315.0859, 154.8994],\n",
      "        [ 94.1223,  81.6828, 171.4656, 154.1099],\n",
      "        [  2.8511, 159.8430, 117.2357, 206.4230],\n",
      "        [316.2484, 119.2843, 399.2058, 189.9837],\n",
      "        [341.7623, 242.1354, 472.9915, 302.4774],\n",
      "        [  0.8143, 205.1760, 149.9811, 242.6554],\n",
      "        [281.1571, 226.8064, 350.7644, 300.7468],\n",
      "        [ 68.9574, 239.7017, 133.1126, 302.6772],\n",
      "        [306.5064,  81.8420, 397.8388, 119.1998],\n",
      "        [167.7757,  81.2873, 243.5999, 155.8692]])\n",
      "xyxyn: tensor([[0.6965, 0.3807, 0.9987, 0.5137],\n",
      "        [0.8186, 0.1707, 0.9899, 0.3121],\n",
      "        [0.4136, 0.3205, 0.7018, 0.4606],\n",
      "        [0.0067, 0.1715, 0.1968, 0.2507],\n",
      "        [0.0063, 0.2494, 0.1946, 0.3322],\n",
      "        [0.2651, 0.4927, 0.3960, 0.6235],\n",
      "        [0.2472, 0.3153, 0.4359, 0.3983],\n",
      "        [0.3844, 0.4809, 0.5991, 0.6263],\n",
      "        [0.5000, 0.1699, 0.6564, 0.3227],\n",
      "        [0.1961, 0.1702, 0.3572, 0.3211],\n",
      "        [0.0059, 0.3330, 0.2442, 0.4300],\n",
      "        [0.6589, 0.2485, 0.8317, 0.3958],\n",
      "        [0.7120, 0.5044, 0.9854, 0.6302],\n",
      "        [0.0017, 0.4275, 0.3125, 0.5055],\n",
      "        [0.5857, 0.4725, 0.7308, 0.6266],\n",
      "        [0.1437, 0.4994, 0.2773, 0.6306],\n",
      "        [0.6386, 0.1705, 0.8288, 0.2483],\n",
      "        [0.3495, 0.1693, 0.5075, 0.3247]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150324_Pinterest_jpg.rf.afc10224901d4c8f0a634bc99a768535.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8052, 0.6562, 0.6054, 0.5470, 0.5364, 0.4782, 0.4687, 0.4519, 0.4338, 0.3888, 0.3677, 0.3597, 0.3523, 0.3495, 0.3318, 0.3226, 0.2922])\n",
      "data: tensor([[6.6680e+01, 8.6459e+01, 2.3290e+02, 1.8936e+02, 8.0521e-01, 0.0000e+00],\n",
      "        [1.1173e+01, 2.6405e+02, 1.8965e+02, 3.0812e+02, 6.5624e-01, 0.0000e+00],\n",
      "        [6.3591e+01, 2.2676e+02, 2.0634e+02, 2.7277e+02, 6.0544e-01, 0.0000e+00],\n",
      "        [2.4209e+02, 2.4274e+02, 2.9929e+02, 2.7099e+02, 5.4698e-01, 0.0000e+00],\n",
      "        [3.2002e+02, 1.4723e+02, 3.9440e+02, 1.9250e+02, 5.3641e-01, 0.0000e+00],\n",
      "        [4.1080e+02, 1.1648e+02, 4.7803e+02, 1.5916e+02, 4.7824e-01, 0.0000e+00],\n",
      "        [4.0704e+02, 1.9814e+02, 4.7439e+02, 2.3991e+02, 4.6870e-01, 0.0000e+00],\n",
      "        [3.2415e+02, 1.9189e+02, 3.9664e+02, 2.2873e+02, 4.5194e-01, 0.0000e+00],\n",
      "        [4.0544e+02, 3.5867e+02, 4.5489e+02, 3.8446e+02, 4.3381e-01, 0.0000e+00],\n",
      "        [2.5617e+02, 1.8432e+02, 3.1845e+02, 2.2552e+02, 3.8879e-01, 0.0000e+00],\n",
      "        [3.4403e+02, 3.9914e+02, 3.9795e+02, 4.2848e+02, 3.6769e-01, 0.0000e+00],\n",
      "        [2.4327e+02, 2.7017e+02, 2.9775e+02, 2.9592e+02, 3.5969e-01, 0.0000e+00],\n",
      "        [4.1569e+02, 1.6063e+02, 4.7588e+02, 1.9917e+02, 3.5228e-01, 0.0000e+00],\n",
      "        [3.4283e+02, 3.7596e+02, 3.9565e+02, 4.0124e+02, 3.4949e-01, 0.0000e+00],\n",
      "        [4.0332e+02, 3.8575e+02, 4.5751e+02, 4.0640e+02, 3.3184e-01, 0.0000e+00],\n",
      "        [2.9196e+02, 3.6968e+02, 3.4094e+02, 3.9670e+02, 3.2259e-01, 0.0000e+00],\n",
      "        [4.0475e+02, 2.6017e+02, 4.6184e+02, 2.8775e+02, 2.9223e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([17, 6])\n",
      "xywh: tensor([[149.7913, 137.9087, 166.2235, 102.8987],\n",
      "        [100.4139, 286.0848, 178.4811,  44.0657],\n",
      "        [134.9668, 249.7625, 142.7520,  46.0106],\n",
      "        [270.6924, 256.8653,  57.1955,  28.2475],\n",
      "        [357.2091, 169.8647,  74.3761,  45.2749],\n",
      "        [444.4112, 137.8198,  67.2283,  42.6794],\n",
      "        [440.7150, 219.0219,  67.3575,  41.7709],\n",
      "        [360.3922, 210.3095,  72.4895,  36.8467],\n",
      "        [430.1642, 371.5654,  49.4474,  25.7919],\n",
      "        [287.3102, 204.9193,  62.2805,  41.2060],\n",
      "        [370.9903, 413.8105,  53.9120,  29.3369],\n",
      "        [270.5124, 283.0445,  54.4760,  25.7452],\n",
      "        [445.7848, 179.8980,  60.1902,  38.5401],\n",
      "        [369.2376, 388.5978,  52.8201,  25.2853],\n",
      "        [430.4168, 396.0739,  54.1942,  20.6572],\n",
      "        [316.4498, 383.1876,  48.9784,  27.0195],\n",
      "        [433.2947, 273.9648,  57.0910,  27.5797]])\n",
      "xywhn: tensor([[0.3121, 0.2873, 0.3463, 0.2144],\n",
      "        [0.2092, 0.5960, 0.3718, 0.0918],\n",
      "        [0.2812, 0.5203, 0.2974, 0.0959],\n",
      "        [0.5639, 0.5351, 0.1192, 0.0588],\n",
      "        [0.7442, 0.3539, 0.1550, 0.0943],\n",
      "        [0.9259, 0.2871, 0.1401, 0.0889],\n",
      "        [0.9182, 0.4563, 0.1403, 0.0870],\n",
      "        [0.7508, 0.4381, 0.1510, 0.0768],\n",
      "        [0.8962, 0.7741, 0.1030, 0.0537],\n",
      "        [0.5986, 0.4269, 0.1298, 0.0858],\n",
      "        [0.7729, 0.8621, 0.1123, 0.0611],\n",
      "        [0.5636, 0.5897, 0.1135, 0.0536],\n",
      "        [0.9287, 0.3748, 0.1254, 0.0803],\n",
      "        [0.7692, 0.8096, 0.1100, 0.0527],\n",
      "        [0.8967, 0.8252, 0.1129, 0.0430],\n",
      "        [0.6593, 0.7983, 0.1020, 0.0563],\n",
      "        [0.9027, 0.5708, 0.1189, 0.0575]])\n",
      "xyxy: tensor([[ 66.6796,  86.4593, 232.9031, 189.3580],\n",
      "        [ 11.1734, 264.0519, 189.6545, 308.1176],\n",
      "        [ 63.5908, 226.7572, 206.3428, 272.7677],\n",
      "        [242.0947, 242.7416, 299.2902, 270.9891],\n",
      "        [320.0211, 147.2272, 394.3972, 192.5021],\n",
      "        [410.7971, 116.4801, 478.0253, 159.1595],\n",
      "        [407.0363, 198.1364, 474.3938, 239.9074],\n",
      "        [324.1475, 191.8861, 396.6370, 228.7328],\n",
      "        [405.4405, 358.6694, 454.8879, 384.4613],\n",
      "        [256.1700, 184.3163, 318.4505, 225.5222],\n",
      "        [344.0343, 399.1421, 397.9463, 428.4789],\n",
      "        [243.2744, 270.1719, 297.7504, 295.9171],\n",
      "        [415.6897, 160.6279, 475.8799, 199.1680],\n",
      "        [342.8276, 375.9551, 395.6476, 401.2404],\n",
      "        [403.3197, 385.7453, 457.5139, 406.4026],\n",
      "        [291.9606, 369.6778, 340.9390, 396.6973],\n",
      "        [404.7492, 260.1750, 461.8402, 287.7547]])\n",
      "xyxyn: tensor([[0.1389, 0.1801, 0.4852, 0.3945],\n",
      "        [0.0233, 0.5501, 0.3951, 0.6419],\n",
      "        [0.1325, 0.4724, 0.4299, 0.5683],\n",
      "        [0.5044, 0.5057, 0.6235, 0.5646],\n",
      "        [0.6667, 0.3067, 0.8217, 0.4010],\n",
      "        [0.8558, 0.2427, 0.9959, 0.3316],\n",
      "        [0.8480, 0.4128, 0.9883, 0.4998],\n",
      "        [0.6753, 0.3998, 0.8263, 0.4765],\n",
      "        [0.8447, 0.7472, 0.9477, 0.8010],\n",
      "        [0.5337, 0.3840, 0.6634, 0.4698],\n",
      "        [0.7167, 0.8315, 0.8291, 0.8927],\n",
      "        [0.5068, 0.5629, 0.6203, 0.6165],\n",
      "        [0.8660, 0.3346, 0.9914, 0.4149],\n",
      "        [0.7142, 0.7832, 0.8243, 0.8359],\n",
      "        [0.8402, 0.8036, 0.9532, 0.8467],\n",
      "        [0.6083, 0.7702, 0.7103, 0.8265],\n",
      "        [0.8432, 0.5420, 0.9622, 0.5995]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104512_Pinterest_jpg.rf.6b27be1d9b728a30ae0807aba72ce260.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0.])\n",
      "conf: tensor([0.7443, 0.6829, 0.4990, 0.4485])\n",
      "data: tensor([[229.5880,  56.5204, 409.4347, 180.8593,   0.7443,   0.0000],\n",
      "        [252.9160, 225.0404, 448.2578, 364.1769,   0.6829,   0.0000],\n",
      "        [ 72.9009, 258.8887, 189.1045, 358.9826,   0.4990,   0.0000],\n",
      "        [ 17.4247, 101.2185, 178.5805, 213.0865,   0.4485,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[319.5113, 118.6898, 179.8467, 124.3389],\n",
      "        [350.5869, 294.6086, 195.3417, 139.1366],\n",
      "        [131.0027, 308.9357, 116.2036, 100.0939],\n",
      "        [ 98.0026, 157.1525, 161.1557, 111.8680]])\n",
      "xywhn: tensor([[0.6656, 0.2473, 0.3747, 0.2590],\n",
      "        [0.7304, 0.6138, 0.4070, 0.2899],\n",
      "        [0.2729, 0.6436, 0.2421, 0.2085],\n",
      "        [0.2042, 0.3274, 0.3357, 0.2331]])\n",
      "xyxy: tensor([[229.5880,  56.5204, 409.4347, 180.8593],\n",
      "        [252.9160, 225.0404, 448.2578, 364.1769],\n",
      "        [ 72.9009, 258.8887, 189.1045, 358.9826],\n",
      "        [ 17.4247, 101.2185, 178.5805, 213.0865]])\n",
      "xyxyn: tensor([[0.4783, 0.1178, 0.8530, 0.3768],\n",
      "        [0.5269, 0.4688, 0.9339, 0.7587],\n",
      "        [0.1519, 0.5394, 0.3940, 0.7479],\n",
      "        [0.0363, 0.2109, 0.3720, 0.4439]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/celecoxib-200-MG-4-Copy-Copy_jpg.rf.ee6e7cfae1dfeceb1167b8a95d144414.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9587, 0.9537])\n",
      "data: tensor([[ 18.1274,  18.1711, 255.7358, 479.1954,   0.9587,   2.0000],\n",
      "        [255.0787,  20.8016, 480.0000, 478.3268,   0.9537,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[136.9316, 248.6833, 237.6084, 461.0244],\n",
      "        [367.5394, 249.5642, 224.9213, 457.5252]])\n",
      "xywhn: tensor([[0.2853, 0.5181, 0.4950, 0.9605],\n",
      "        [0.7657, 0.5199, 0.4686, 0.9532]])\n",
      "xyxy: tensor([[ 18.1274,  18.1711, 255.7358, 479.1954],\n",
      "        [255.0787,  20.8016, 480.0000, 478.3268]])\n",
      "xyxyn: tensor([[0.0378, 0.0379, 0.5328, 0.9983],\n",
      "        [0.5314, 0.0433, 1.0000, 0.9965]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151711_Pinterest_jpg.rf.fab56c6cfef6b66389193f7c344d5fee.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8378])\n",
      "data: tensor([[  8.5944,  44.0697, 478.3009, 131.2611,   0.8378,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[243.4476,  87.6654, 469.7065,  87.1914]])\n",
      "xywhn: tensor([[0.5072, 0.1826, 0.9786, 0.1816]])\n",
      "xyxy: tensor([[  8.5944,  44.0697, 478.3009, 131.2611]])\n",
      "xyxyn: tensor([[0.0179, 0.0918, 0.9965, 0.2735]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Oseltamivir-45-MG-7-Copy-Copy_jpg.rf.c617886cb00bc34ff0110ce0d93e819c.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9542, 0.9537])\n",
      "data: tensor([[223.1393,   1.9557, 457.9472, 466.9087,   0.9542,   2.0000],\n",
      "        [  0.0000,   1.2620, 229.1649, 465.8438,   0.9537,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[340.5433, 234.4322, 234.8079, 464.9530],\n",
      "        [114.5824, 233.5529, 229.1649, 464.5817]])\n",
      "xywhn: tensor([[0.7095, 0.4884, 0.4892, 0.9687],\n",
      "        [0.2387, 0.4866, 0.4774, 0.9679]])\n",
      "xyxy: tensor([[223.1393,   1.9557, 457.9472, 466.9087],\n",
      "        [  0.0000,   1.2620, 229.1649, 465.8438]])\n",
      "xyxyn: tensor([[0.4649, 0.0041, 0.9541, 0.9727],\n",
      "        [0.0000, 0.0026, 0.4774, 0.9705]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094539_Pinterest_jpg.rf.52d626bf1e69cc81b4da92d7bda22379.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8666])\n",
      "data: tensor([[ 64.3598,  69.7819, 381.1455, 221.3473,   0.8666,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[222.7527, 145.5646, 316.7857, 151.5654]])\n",
      "xywhn: tensor([[0.4641, 0.3033, 0.6600, 0.3158]])\n",
      "xyxy: tensor([[ 64.3598,  69.7819, 381.1455, 221.3473]])\n",
      "xyxyn: tensor([[0.1341, 0.1454, 0.7941, 0.4611]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/sitagliptin-50-MG-12-Copy_jpg.rf.f48ebe3489a2c3a344b108e6c1766640.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9338, 0.9133])\n",
      "data: tensor([[ 40.4337,   1.1712, 339.0277, 227.6616,   0.9338,   2.0000],\n",
      "        [ 41.6128, 229.9025, 337.6135, 464.7648,   0.9133,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[189.7307, 114.4164, 298.5940, 226.4904],\n",
      "        [189.6132, 347.3336, 296.0007, 234.8624]])\n",
      "xywhn: tensor([[0.3953, 0.2384, 0.6221, 0.4719],\n",
      "        [0.3950, 0.7236, 0.6167, 0.4893]])\n",
      "xyxy: tensor([[ 40.4337,   1.1712, 339.0277, 227.6616],\n",
      "        [ 41.6128, 229.9025, 337.6135, 464.7648]])\n",
      "xyxyn: tensor([[0.0842, 0.0024, 0.7063, 0.4743],\n",
      "        [0.0867, 0.4790, 0.7034, 0.9683]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100041_Pinterest_jpg.rf.fc1a2dae50c3dfdfb529ba9da28b723e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.6781])\n",
      "data: tensor([[ 20.1424,  13.3379, 330.3277, 329.7928,   0.6781,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[175.2350, 171.5654, 310.1853, 316.4549]])\n",
      "xywhn: tensor([[0.3651, 0.3574, 0.6462, 0.6593]])\n",
      "xyxy: tensor([[ 20.1424,  13.3379, 330.3277, 329.7928]])\n",
      "xyxyn: tensor([[0.0420, 0.0278, 0.6882, 0.6871]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130312_jpg.rf.af07e08c4c97a6db0f4b847ecc738273.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.7335, 0.6911, 0.6536, 0.5247, 0.3102, 0.2928])\n",
      "data: tensor([[1.7195e+02, 2.5394e+02, 2.1138e+02, 3.4071e+02, 7.3353e-01, 1.0000e+00],\n",
      "        [3.1162e+02, 2.9768e+02, 3.6505e+02, 3.6713e+02, 6.9112e-01, 1.0000e+00],\n",
      "        [8.4396e+01, 2.4705e+02, 1.3736e+02, 3.2754e+02, 6.5364e-01, 1.0000e+00],\n",
      "        [1.3420e+02, 2.4521e+02, 1.7448e+02, 3.3656e+02, 5.2469e-01, 1.0000e+00],\n",
      "        [8.3449e+01, 2.4572e+02, 1.7453e+02, 3.3113e+02, 3.1025e-01, 1.0000e+00],\n",
      "        [2.7106e+02, 2.0882e+02, 2.8474e+02, 2.8081e+02, 2.9283e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[191.6656, 297.3254,  39.4371,  86.7728],\n",
      "        [338.3330, 332.4083,  53.4244,  69.4488],\n",
      "        [110.8789, 287.2955,  52.9668,  80.4892],\n",
      "        [154.3413, 290.8860,  40.2806,  91.3436],\n",
      "        [128.9907, 288.4250,  91.0833,  85.4004],\n",
      "        [277.8993, 244.8147,  13.6721,  71.9959]])\n",
      "xywhn: tensor([[0.3993, 0.6194, 0.0822, 0.1808],\n",
      "        [0.7049, 0.6925, 0.1113, 0.1447],\n",
      "        [0.2310, 0.5985, 0.1103, 0.1677],\n",
      "        [0.3215, 0.6060, 0.0839, 0.1903],\n",
      "        [0.2687, 0.6009, 0.1898, 0.1779],\n",
      "        [0.5790, 0.5100, 0.0285, 0.1500]])\n",
      "xyxy: tensor([[171.9471, 253.9390, 211.3841, 340.7118],\n",
      "        [311.6208, 297.6839, 365.0452, 367.1327],\n",
      "        [ 84.3955, 247.0509, 137.3623, 327.5401],\n",
      "        [134.2010, 245.2142, 174.4816, 336.5578],\n",
      "        [ 83.4490, 245.7248, 174.5323, 331.1252],\n",
      "        [271.0632, 208.8168, 284.7354, 280.8127]])\n",
      "xyxyn: tensor([[0.3582, 0.5290, 0.4404, 0.7098],\n",
      "        [0.6492, 0.6202, 0.7605, 0.7649],\n",
      "        [0.1758, 0.5147, 0.2862, 0.6824],\n",
      "        [0.2796, 0.5109, 0.3635, 0.7012],\n",
      "        [0.1739, 0.5119, 0.3636, 0.6898],\n",
      "        [0.5647, 0.4350, 0.5932, 0.5850]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/pitavastatin-1-MG-3-Copy-Copy_jpg.rf.d1fb1d9c1b7cbdcd9f87d4f5f4b710d5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9351, 0.9331])\n",
      "data: tensor([[148.2935,  16.6267, 425.3279, 247.3078,   0.9351,   2.0000],\n",
      "        [149.3537, 248.6130, 426.1539, 480.0000,   0.9331,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[286.8107, 131.9672, 277.0344, 230.6812],\n",
      "        [287.7538, 364.3065, 276.8002, 231.3870]])\n",
      "xywhn: tensor([[0.5975, 0.2749, 0.5772, 0.4806],\n",
      "        [0.5995, 0.7590, 0.5767, 0.4821]])\n",
      "xyxy: tensor([[148.2935,  16.6267, 425.3279, 247.3078],\n",
      "        [149.3537, 248.6130, 426.1539, 480.0000]])\n",
      "xyxyn: tensor([[0.3089, 0.0346, 0.8861, 0.5152],\n",
      "        [0.3112, 0.5179, 0.8878, 1.0000]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/pitavastatin-1-MG-20-Copy_jpg.rf.961c2f83b586cca55e47c4606837b961.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9366, 0.9258])\n",
      "data: tensor([[247.5247,  49.2448, 477.8575, 339.0406,   0.9366,   2.0000],\n",
      "        [ 15.1505,  50.1503, 248.5942, 339.3411,   0.9258,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[362.6911, 194.1427, 230.3327, 289.7958],\n",
      "        [131.8723, 194.7457, 233.4437, 289.1908]])\n",
      "xywhn: tensor([[0.7556, 0.4045, 0.4799, 0.6037],\n",
      "        [0.2747, 0.4057, 0.4863, 0.6025]])\n",
      "xyxy: tensor([[247.5247,  49.2448, 477.8575, 339.0406],\n",
      "        [ 15.1505,  50.1503, 248.5942, 339.3411]])\n",
      "xyxyn: tensor([[0.5157, 0.1026, 0.9955, 0.7063],\n",
      "        [0.0316, 0.1045, 0.5179, 0.7070]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094210_Pinterest_jpg.rf.3c06b2c7cecdd1d29fefcff2963beb37.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.9019, 0.7088])\n",
      "data: tensor([[158.2867,  16.4277, 478.7272, 198.8947,   0.9019,   0.0000],\n",
      "        [  0.0000,  15.8823, 265.4254, 222.4619,   0.7088,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[318.5070, 107.6612, 320.4406, 182.4669],\n",
      "        [132.7127, 119.1721, 265.4254, 206.5796]])\n",
      "xywhn: tensor([[0.6636, 0.2243, 0.6676, 0.3801],\n",
      "        [0.2765, 0.2483, 0.5530, 0.4304]])\n",
      "xyxy: tensor([[158.2867,  16.4277, 478.7272, 198.8947],\n",
      "        [  0.0000,  15.8823, 265.4254, 222.4619]])\n",
      "xyxyn: tensor([[0.3298, 0.0342, 0.9973, 0.4144],\n",
      "        [0.0000, 0.0331, 0.5530, 0.4635]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094425_Pinterest_jpg.rf.e79fddc0f9b03a02f993167bc80ce1b8.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3667])\n",
      "data: tensor([[0.0000e+00, 3.6984e+01, 4.5014e+02, 4.0690e+02, 3.6671e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[225.0716, 221.9442, 450.1432, 369.9199]])\n",
      "xywhn: tensor([[0.4689, 0.4624, 0.9378, 0.7707]])\n",
      "xyxy: tensor([[  0.0000,  36.9843, 450.1432, 406.9042]])\n",
      "xyxyn: tensor([[0.0000, 0.0771, 0.9378, 0.8477]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240523_104248_jpg.rf.6b08e7fde9eafb84f00bc6d2f05ca068.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.7767])\n",
      "data: tensor([[ 92.5710, 172.4239, 372.6324, 406.0923,   0.7767,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[232.6017, 289.2581, 280.0614, 233.6685]])\n",
      "xywhn: tensor([[0.4846, 0.6026, 0.5835, 0.4868]])\n",
      "xyxy: tensor([[ 92.5710, 172.4239, 372.6324, 406.0923]])\n",
      "xyxyn: tensor([[0.1929, 0.3592, 0.7763, 0.8460]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100750_Pinterest_jpg.rf.1d162cf18fa96c717c71a52462c51c8d.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8314, 0.8203])\n",
      "data: tensor([[230.6866,  21.9510, 435.6693, 153.1623,   0.8314,   0.0000],\n",
      "        [ 19.2948,  24.9296, 238.0919, 154.9865,   0.8203,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[333.1779,  87.5566, 204.9827, 131.2113],\n",
      "        [128.6934,  89.9580, 218.7971, 130.0569]])\n",
      "xywhn: tensor([[0.6941, 0.1824, 0.4270, 0.2734],\n",
      "        [0.2681, 0.1874, 0.4558, 0.2710]])\n",
      "xyxy: tensor([[230.6866,  21.9510, 435.6693, 153.1623],\n",
      "        [ 19.2948,  24.9296, 238.0919, 154.9865]])\n",
      "xyxyn: tensor([[0.4806, 0.0457, 0.9076, 0.3191],\n",
      "        [0.0402, 0.0519, 0.4960, 0.3229]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240531_093054_Chrome_jpg.rf.948bcdfe2d2525ea25885f35eec2e791.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "conf: tensor([0.8942, 0.8781, 0.8379, 0.7966, 0.7666, 0.6420, 0.3082, 0.2799])\n",
      "data: tensor([[2.4249e+02, 1.0574e+02, 4.6733e+02, 1.8291e+02, 8.9423e-01, 3.0000e+00],\n",
      "        [1.2531e+01, 2.2165e+02, 1.5890e+02, 2.7296e+02, 8.7812e-01, 3.0000e+00],\n",
      "        [2.4053e+02, 2.4485e+02, 4.6549e+02, 3.1916e+02, 8.3787e-01, 3.0000e+00],\n",
      "        [3.5797e+01, 7.9827e+01, 2.2756e+02, 1.3484e+02, 7.9660e-01, 3.0000e+00],\n",
      "        [2.4549e+02, 3.8050e+02, 4.5855e+02, 4.4570e+02, 7.6663e-01, 3.0000e+00],\n",
      "        [1.3322e+01, 3.5656e+02, 1.6383e+02, 4.0754e+02, 6.4203e-01, 3.0000e+00],\n",
      "        [4.7770e+01, 3.8659e+02, 2.1850e+02, 4.3985e+02, 3.0819e-01, 3.0000e+00],\n",
      "        [5.0046e+01, 2.5597e+02, 2.1632e+02, 3.0319e+02, 2.7994e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([8, 6])\n",
      "xywh: tensor([[354.9062, 144.3248, 224.8411,  77.1794],\n",
      "        [ 85.7177, 247.3059, 146.3735,  51.3071],\n",
      "        [353.0074, 282.0039, 224.9608,  74.3172],\n",
      "        [131.6773, 107.3329, 191.7599,  55.0123],\n",
      "        [352.0201, 413.0983, 213.0655,  65.1945],\n",
      "        [ 88.5770, 382.0477, 150.5103,  50.9846],\n",
      "        [133.1341, 413.2191, 170.7279,  53.2554],\n",
      "        [133.1814, 279.5769, 166.2708,  47.2177]])\n",
      "xywhn: tensor([[0.7394, 0.3007, 0.4684, 0.1608],\n",
      "        [0.1786, 0.5152, 0.3049, 0.1069],\n",
      "        [0.7354, 0.5875, 0.4687, 0.1548],\n",
      "        [0.2743, 0.2236, 0.3995, 0.1146],\n",
      "        [0.7334, 0.8606, 0.4439, 0.1358],\n",
      "        [0.1845, 0.7959, 0.3136, 0.1062],\n",
      "        [0.2774, 0.8609, 0.3557, 0.1109],\n",
      "        [0.2775, 0.5825, 0.3464, 0.0984]])\n",
      "xyxy: tensor([[242.4857, 105.7351, 467.3268, 182.9145],\n",
      "        [ 12.5309, 221.6524, 158.9044, 272.9595],\n",
      "        [240.5270, 244.8453, 465.4878, 319.1625],\n",
      "        [ 35.7974,  79.8267, 227.5573, 134.8390],\n",
      "        [245.4874, 380.5010, 458.5529, 445.6955],\n",
      "        [ 13.3219, 356.5554, 163.8322, 407.5400],\n",
      "        [ 47.7702, 386.5914, 218.4981, 439.8468],\n",
      "        [ 50.0460, 255.9681, 216.3168, 303.1858]])\n",
      "xyxyn: tensor([[0.5052, 0.2203, 0.9736, 0.3811],\n",
      "        [0.0261, 0.4618, 0.3311, 0.5687],\n",
      "        [0.5011, 0.5101, 0.9698, 0.6649],\n",
      "        [0.0746, 0.1663, 0.4741, 0.2809],\n",
      "        [0.5114, 0.7927, 0.9553, 0.9285],\n",
      "        [0.0278, 0.7428, 0.3413, 0.8490],\n",
      "        [0.0995, 0.8054, 0.4552, 0.9163],\n",
      "        [0.1043, 0.5333, 0.4507, 0.6316]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094155_Pinterest_jpg.rf.51e80075426b80a6ea3fbdf399698b21.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8514, 0.8428])\n",
      "data: tensor([[ 35.7091, 154.9772, 312.2622, 336.6583,   0.8514,   0.0000],\n",
      "        [217.8471,   8.4945, 450.8232, 190.0138,   0.8428,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[173.9857, 245.8177, 276.5531, 181.6810],\n",
      "        [334.3351,  99.2542, 232.9761, 181.5193]])\n",
      "xywhn: tensor([[0.3625, 0.5121, 0.5762, 0.3785],\n",
      "        [0.6965, 0.2068, 0.4854, 0.3782]])\n",
      "xyxy: tensor([[ 35.7091, 154.9772, 312.2622, 336.6583],\n",
      "        [217.8471,   8.4945, 450.8232, 190.0138]])\n",
      "xyxyn: tensor([[0.0744, 0.3229, 0.6505, 0.7014],\n",
      "        [0.4538, 0.0177, 0.9392, 0.3959]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130509_jpg.rf.f97975cd300274d5dcb57134a5018730.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.7687, 0.7278, 0.7094, 0.6603, 0.6112, 0.5356, 0.4540])\n",
      "data: tensor([[174.8532, 283.7897, 314.4359, 384.0375,   0.7687,   1.0000],\n",
      "        [236.2345, 221.3887, 356.3960, 284.3936,   0.7278,   1.0000],\n",
      "        [240.0525,  99.8626, 313.5301, 129.6220,   0.7094,   1.0000],\n",
      "        [ 90.1220, 231.6131, 187.8825, 284.8595,   0.6603,   1.0000],\n",
      "        [286.9518, 165.8593, 387.4353, 208.6434,   0.6112,   1.0000],\n",
      "        [162.6566, 166.1372, 253.7386, 214.3956,   0.5356,   1.0000],\n",
      "        [239.8819,  81.4071, 313.9922, 131.5727,   0.4540,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[244.6446, 333.9136, 139.5828, 100.2478],\n",
      "        [296.3152, 252.8911, 120.1614,  63.0049],\n",
      "        [276.7913, 114.7423,  73.4776,  29.7593],\n",
      "        [139.0023, 258.2363,  97.7604,  53.2464],\n",
      "        [337.1936, 187.2514, 100.4835,  42.7841],\n",
      "        [208.1976, 190.2664,  91.0820,  48.2584],\n",
      "        [276.9371, 106.4899,  74.1104,  50.1656]])\n",
      "xywhn: tensor([[0.5097, 0.6957, 0.2908, 0.2088],\n",
      "        [0.6173, 0.5269, 0.2503, 0.1313],\n",
      "        [0.5766, 0.2390, 0.1531, 0.0620],\n",
      "        [0.2896, 0.5380, 0.2037, 0.1109],\n",
      "        [0.7025, 0.3901, 0.2093, 0.0891],\n",
      "        [0.4337, 0.3964, 0.1898, 0.1005],\n",
      "        [0.5770, 0.2219, 0.1544, 0.1045]])\n",
      "xyxy: tensor([[174.8532, 283.7897, 314.4359, 384.0375],\n",
      "        [236.2345, 221.3887, 356.3960, 284.3936],\n",
      "        [240.0525,  99.8626, 313.5301, 129.6220],\n",
      "        [ 90.1220, 231.6131, 187.8825, 284.8595],\n",
      "        [286.9518, 165.8593, 387.4353, 208.6434],\n",
      "        [162.6566, 166.1372, 253.7386, 214.3956],\n",
      "        [239.8819,  81.4071, 313.9922, 131.5727]])\n",
      "xyxyn: tensor([[0.3643, 0.5912, 0.6551, 0.8001],\n",
      "        [0.4922, 0.4612, 0.7425, 0.5925],\n",
      "        [0.5001, 0.2080, 0.6532, 0.2700],\n",
      "        [0.1878, 0.4825, 0.3914, 0.5935],\n",
      "        [0.5978, 0.3455, 0.8072, 0.4347],\n",
      "        [0.3389, 0.3461, 0.5286, 0.4467],\n",
      "        [0.4998, 0.1696, 0.6542, 0.2741]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151222_Pinterest_jpg.rf.f14e85be13c0b1ea64c4a4d7039e6074.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.7501, 0.6711, 0.6704, 0.5951, 0.5715, 0.5610, 0.4721, 0.4378, 0.4229, 0.3149])\n",
      "data: tensor([[1.0807e+02, 9.3241e+01, 2.0271e+02, 1.5855e+02, 7.5013e-01, 0.0000e+00],\n",
      "        [2.8253e+02, 2.4595e+01, 3.7621e+02, 8.2368e+01, 6.7110e-01, 0.0000e+00],\n",
      "        [1.1177e+01, 9.4382e+01, 1.0111e+02, 1.5815e+02, 6.7045e-01, 0.0000e+00],\n",
      "        [1.1248e+01, 2.6067e+01, 9.6015e+01, 8.9273e+01, 5.9510e-01, 0.0000e+00],\n",
      "        [2.9513e+02, 9.5572e+01, 3.8110e+02, 1.5422e+02, 5.7149e-01, 0.0000e+00],\n",
      "        [2.1510e+02, 9.4339e+01, 2.8297e+02, 1.5732e+02, 5.6100e-01, 0.0000e+00],\n",
      "        [3.8923e+02, 2.7537e+01, 4.6564e+02, 9.0039e+01, 4.7208e-01, 0.0000e+00],\n",
      "        [1.1205e+02, 2.3654e+01, 1.9371e+02, 8.7705e+01, 4.3776e-01, 0.0000e+00],\n",
      "        [3.9251e+02, 9.2641e+01, 4.6656e+02, 1.5480e+02, 4.2288e-01, 0.0000e+00],\n",
      "        [2.0909e+02, 2.5246e+01, 2.7336e+02, 8.6243e+01, 3.1491e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([10, 6])\n",
      "xywh: tensor([[155.3944, 125.8975,  94.6405,  65.3123],\n",
      "        [329.3670,  53.4814,  93.6774,  57.7738],\n",
      "        [ 56.1440, 126.2646,  89.9331,  63.7652],\n",
      "        [ 53.6316,  57.6700,  84.7670,  63.2051],\n",
      "        [338.1157, 124.8975,  85.9686,  58.6510],\n",
      "        [249.0307, 125.8289,  67.8694,  62.9803],\n",
      "        [427.4363,  58.7879,  76.4152,  62.5011],\n",
      "        [152.8799,  55.6794,  81.6622,  64.0515],\n",
      "        [429.5377, 123.7185,  74.0498,  62.1543],\n",
      "        [241.2243,  55.7445,  64.2651,  60.9965]])\n",
      "xywhn: tensor([[0.3237, 0.2623, 0.1972, 0.1361],\n",
      "        [0.6862, 0.1114, 0.1952, 0.1204],\n",
      "        [0.1170, 0.2631, 0.1874, 0.1328],\n",
      "        [0.1117, 0.1201, 0.1766, 0.1317],\n",
      "        [0.7044, 0.2602, 0.1791, 0.1222],\n",
      "        [0.5188, 0.2621, 0.1414, 0.1312],\n",
      "        [0.8905, 0.1225, 0.1592, 0.1302],\n",
      "        [0.3185, 0.1160, 0.1701, 0.1334],\n",
      "        [0.8949, 0.2577, 0.1543, 0.1295],\n",
      "        [0.5026, 0.1161, 0.1339, 0.1271]])\n",
      "xyxy: tensor([[108.0742,  93.2413, 202.7147, 158.5536],\n",
      "        [282.5283,  24.5945, 376.2057,  82.3684],\n",
      "        [ 11.1775,  94.3820, 101.1106, 158.1472],\n",
      "        [ 11.2481,  26.0674,  96.0152,  89.2725],\n",
      "        [295.1314,  95.5720, 381.1000, 154.2229],\n",
      "        [215.0960,  94.3388, 282.9654, 157.3191],\n",
      "        [389.2287,  27.5374, 465.6439,  90.0385],\n",
      "        [112.0488,  23.6536, 193.7110,  87.7051],\n",
      "        [392.5128,  92.6413, 466.5626, 154.7956],\n",
      "        [209.0918,  25.2463, 273.3569,  86.2428]])\n",
      "xyxyn: tensor([[0.2252, 0.1943, 0.4223, 0.3303],\n",
      "        [0.5886, 0.0512, 0.7838, 0.1716],\n",
      "        [0.0233, 0.1966, 0.2106, 0.3295],\n",
      "        [0.0234, 0.0543, 0.2000, 0.1860],\n",
      "        [0.6149, 0.1991, 0.7940, 0.3213],\n",
      "        [0.4481, 0.1965, 0.5895, 0.3277],\n",
      "        [0.8109, 0.0574, 0.9701, 0.1876],\n",
      "        [0.2334, 0.0493, 0.4036, 0.1827],\n",
      "        [0.8177, 0.1930, 0.9720, 0.3225],\n",
      "        [0.4356, 0.0526, 0.5695, 0.1797]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/sitagliptin-50-MG-13-Copy-Copy_jpg.rf.441a7679b25fb28d5cdbba319ed3d606.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2.])\n",
      "conf: tensor([0.9457])\n",
      "data: tensor([[  0.0000,   0.0000, 479.1314, 480.0000,   0.9457,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[239.5657, 240.0000, 479.1314, 480.0000]])\n",
      "xywhn: tensor([[0.4991, 0.5000, 0.9982, 1.0000]])\n",
      "xyxy: tensor([[  0.0000,   0.0000, 479.1314, 480.0000]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.9982, 1.0000]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_101834_Pinterest_jpg.rf.00558def6da53398dcd995b2706d3405.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8456])\n",
      "data: tensor([[ 47.0833,  63.1380, 430.3753, 193.0206,   0.8456,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[238.7293, 128.0793, 383.2921, 129.8826]])\n",
      "xywhn: tensor([[0.4974, 0.2668, 0.7985, 0.2706]])\n",
      "xyxy: tensor([[ 47.0833,  63.1380, 430.3753, 193.0206]])\n",
      "xyxyn: tensor([[0.0981, 0.1315, 0.8966, 0.4021]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/aprepitant-80-MG-28-Copy_jpg.rf.58fe869bdb13b1c7767c0c2d53f3c0de.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9579, 0.9559])\n",
      "data: tensor([[224.0656,   0.0000, 456.7734, 477.8055,   0.9579,   2.0000],\n",
      "        [  0.0000,   0.0000, 227.0823, 469.6475,   0.9559,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[340.4195, 238.9027, 232.7077, 477.8055],\n",
      "        [113.5411, 234.8237, 227.0823, 469.6475]])\n",
      "xywhn: tensor([[0.7092, 0.4977, 0.4848, 0.9954],\n",
      "        [0.2365, 0.4892, 0.4731, 0.9784]])\n",
      "xyxy: tensor([[224.0656,   0.0000, 456.7734, 477.8055],\n",
      "        [  0.0000,   0.0000, 227.0823, 469.6475]])\n",
      "xyxyn: tensor([[0.4668, 0.0000, 0.9516, 0.9954],\n",
      "        [0.0000, 0.0000, 0.4731, 0.9784]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100355_Pinterest_jpg.rf.26c5c6adbfc3463f2baa905d550dd32d.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.7479, 0.6548, 0.6418, 0.6068, 0.5822])\n",
      "data: tensor([[262.6445,  86.3905, 439.0561, 206.0797,   0.7479,   0.0000],\n",
      "        [ 25.1623, 336.7557, 143.7468, 422.8383,   0.6548,   0.0000],\n",
      "        [122.0529, 312.6302, 219.8838, 415.8996,   0.6418,   0.0000],\n",
      "        [283.9277, 255.9143, 416.8654, 348.4434,   0.6068,   0.0000],\n",
      "        [ 58.6569, 169.8827, 213.6991, 272.3744,   0.5822,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[350.8503, 146.2351, 176.4116, 119.6893],\n",
      "        [ 84.4546, 379.7970, 118.5845,  86.0826],\n",
      "        [170.9684, 364.2649,  97.8309, 103.2694],\n",
      "        [350.3965, 302.1788, 132.9376,  92.5291],\n",
      "        [136.1780, 221.1285, 155.0422, 102.4917]])\n",
      "xywhn: tensor([[0.7309, 0.3047, 0.3675, 0.2494],\n",
      "        [0.1759, 0.7912, 0.2471, 0.1793],\n",
      "        [0.3562, 0.7589, 0.2038, 0.2151],\n",
      "        [0.7300, 0.6295, 0.2770, 0.1928],\n",
      "        [0.2837, 0.4607, 0.3230, 0.2135]])\n",
      "xyxy: tensor([[262.6445,  86.3905, 439.0561, 206.0797],\n",
      "        [ 25.1623, 336.7557, 143.7468, 422.8383],\n",
      "        [122.0529, 312.6302, 219.8838, 415.8996],\n",
      "        [283.9277, 255.9143, 416.8654, 348.4434],\n",
      "        [ 58.6569, 169.8827, 213.6991, 272.3744]])\n",
      "xyxyn: tensor([[0.5472, 0.1800, 0.9147, 0.4293],\n",
      "        [0.0524, 0.7016, 0.2995, 0.8809],\n",
      "        [0.2543, 0.6513, 0.4581, 0.8665],\n",
      "        [0.5915, 0.5332, 0.8685, 0.7259],\n",
      "        [0.1222, 0.3539, 0.4452, 0.5674]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151008_Pinterest_jpg.rf.81487bb0a24823071245217e7e0ec0bf.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.7238, 0.6440, 0.4477, 0.4070, 0.3478, 0.2631])\n",
      "data: tensor([[4.0517e+01, 1.1643e+02, 2.0695e+02, 2.5006e+02, 7.2377e-01, 0.0000e+00],\n",
      "        [3.5337e+01, 2.7659e+02, 1.7603e+02, 3.6908e+02, 6.4403e-01, 0.0000e+00],\n",
      "        [3.3205e+02, 3.3135e+01, 4.2203e+02, 8.2354e+01, 4.4774e-01, 0.0000e+00],\n",
      "        [3.4534e+02, 1.6546e+02, 4.0181e+02, 2.0929e+02, 4.0696e-01, 0.0000e+00],\n",
      "        [3.4582e+02, 1.6321e+02, 4.2017e+02, 2.1351e+02, 3.4780e-01, 0.0000e+00],\n",
      "        [3.2268e+02, 3.6442e+02, 3.8429e+02, 4.1477e+02, 2.6314e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[123.7346, 183.2452, 166.4353, 133.6362],\n",
      "        [105.6817, 322.8381, 140.6902,  92.4933],\n",
      "        [377.0365,  57.7449,  89.9771,  49.2192],\n",
      "        [373.5737, 187.3712,  56.4698,  43.8318],\n",
      "        [382.9967, 188.3582,  74.3530,  50.3022],\n",
      "        [353.4852, 389.5953,  61.6144,  50.3569]])\n",
      "xywhn: tensor([[0.2578, 0.3818, 0.3467, 0.2784],\n",
      "        [0.2202, 0.6726, 0.2931, 0.1927],\n",
      "        [0.7855, 0.1203, 0.1875, 0.1025],\n",
      "        [0.7783, 0.3904, 0.1176, 0.0913],\n",
      "        [0.7979, 0.3924, 0.1549, 0.1048],\n",
      "        [0.7364, 0.8117, 0.1284, 0.1049]])\n",
      "xyxy: tensor([[ 40.5169, 116.4271, 206.9522, 250.0633],\n",
      "        [ 35.3366, 276.5914, 176.0269, 369.0847],\n",
      "        [332.0480,  33.1353, 422.0250,  82.3545],\n",
      "        [345.3388, 165.4553, 401.8087, 209.2872],\n",
      "        [345.8202, 163.2071, 420.1732, 213.5092],\n",
      "        [322.6780, 364.4169, 384.2924, 414.7738]])\n",
      "xyxyn: tensor([[0.0844, 0.2426, 0.4312, 0.5210],\n",
      "        [0.0736, 0.5762, 0.3667, 0.7689],\n",
      "        [0.6918, 0.0690, 0.8792, 0.1716],\n",
      "        [0.7195, 0.3447, 0.8371, 0.4360],\n",
      "        [0.7205, 0.3400, 0.8754, 0.4448],\n",
      "        [0.6722, 0.7592, 0.8006, 0.8641]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130044_jpg.rf.d73b549ffebb81a1ed908f8780fd3663.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.5152, 0.4589])\n",
      "data: tensor([[274.7180,  92.7482, 348.2759, 182.6637,   0.5152,   1.0000],\n",
      "        [370.0947, 197.9459, 408.0898, 255.4802,   0.4589,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[311.4969, 137.7059,  73.5579,  89.9154],\n",
      "        [389.0923, 226.7130,  37.9951,  57.5343]])\n",
      "xywhn: tensor([[0.6490, 0.2869, 0.1532, 0.1873],\n",
      "        [0.8106, 0.4723, 0.0792, 0.1199]])\n",
      "xyxy: tensor([[274.7180,  92.7482, 348.2759, 182.6637],\n",
      "        [370.0947, 197.9459, 408.0898, 255.4802]])\n",
      "xyxyn: tensor([[0.5723, 0.1932, 0.7256, 0.3805],\n",
      "        [0.7710, 0.4124, 0.8502, 0.5323]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240523_084456_jpg.rf.ac6643b426a5e37d9b94b9fe6ec84b13.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 3.])\n",
      "conf: tensor([0.7718, 0.3815])\n",
      "data: tensor([[1.7966e+02, 3.2427e+02, 3.2505e+02, 4.5424e+02, 7.7184e-01, 3.0000e+00],\n",
      "        [7.4316e-01, 2.2762e+02, 1.6219e+02, 4.7975e+02, 3.8150e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[252.3569, 389.2559, 145.3941, 129.9769],\n",
      "        [ 81.4650, 353.6858, 161.4436, 252.1235]])\n",
      "xywhn: tensor([[0.5257, 0.8109, 0.3029, 0.2708],\n",
      "        [0.1697, 0.7368, 0.3363, 0.5253]])\n",
      "xyxy: tensor([[179.6599, 324.2675, 325.0540, 454.2444],\n",
      "        [  0.7432, 227.6241, 162.1868, 479.7475]])\n",
      "xyxyn: tensor([[0.3743, 0.6756, 0.6772, 0.9463],\n",
      "        [0.0015, 0.4742, 0.3379, 0.9995]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/medicine-74-_jpg.rf.87cea9b0c8e17d1355262702336417ff.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.8126, 0.6942])\n",
      "data: tensor([[170.3665, 195.4702, 248.9122, 308.1839,   0.8126,   2.0000],\n",
      "        [236.5428, 161.6970, 314.1386, 255.5760,   0.6942,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[209.6393, 251.8270,  78.5458, 112.7137],\n",
      "        [275.3407, 208.6365,  77.5958,  93.8789]])\n",
      "xywhn: tensor([[0.4367, 0.5246, 0.1636, 0.2348],\n",
      "        [0.5736, 0.4347, 0.1617, 0.1956]])\n",
      "xyxy: tensor([[170.3665, 195.4702, 248.9122, 308.1839],\n",
      "        [236.5428, 161.6970, 314.1386, 255.5760]])\n",
      "xyxyn: tensor([[0.3549, 0.4072, 0.5186, 0.6420],\n",
      "        [0.4928, 0.3369, 0.6545, 0.5324]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104700_Pinterest_jpg.rf.05fe65188f7d4ca98dfc2595ed8a6e6e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.7994, 0.7859, 0.7504, 0.6752, 0.5931])\n",
      "data: tensor([[  2.1758, 227.4069, 218.3970, 359.4863,   0.7994,   0.0000],\n",
      "        [276.3871, 121.0144, 441.3721, 265.7102,   0.7859,   0.0000],\n",
      "        [241.2932, 296.2509, 430.2255, 409.8274,   0.7504,   0.0000],\n",
      "        [130.9716,  93.9464, 248.0578, 192.0112,   0.6752,   0.0000],\n",
      "        [ 32.6411,  88.6536, 147.5921, 189.7204,   0.5931,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[110.2864, 293.4466, 216.2212, 132.0793],\n",
      "        [358.8796, 193.3623, 164.9850, 144.6958],\n",
      "        [335.7593, 353.0391, 188.9324, 113.5765],\n",
      "        [189.5147, 142.9788, 117.0862,  98.0649],\n",
      "        [ 90.1166, 139.1870, 114.9510, 101.0668]])\n",
      "xywhn: tensor([[0.2298, 0.6113, 0.4505, 0.2752],\n",
      "        [0.7477, 0.4028, 0.3437, 0.3014],\n",
      "        [0.6995, 0.7355, 0.3936, 0.2366],\n",
      "        [0.3948, 0.2979, 0.2439, 0.2043],\n",
      "        [0.1877, 0.2900, 0.2395, 0.2106]])\n",
      "xyxy: tensor([[  2.1758, 227.4069, 218.3970, 359.4863],\n",
      "        [276.3871, 121.0144, 441.3721, 265.7102],\n",
      "        [241.2932, 296.2509, 430.2255, 409.8274],\n",
      "        [130.9716,  93.9464, 248.0578, 192.0112],\n",
      "        [ 32.6411,  88.6536, 147.5921, 189.7204]])\n",
      "xyxyn: tensor([[0.0045, 0.4738, 0.4550, 0.7489],\n",
      "        [0.5758, 0.2521, 0.9195, 0.5536],\n",
      "        [0.5027, 0.6172, 0.8963, 0.8538],\n",
      "        [0.2729, 0.1957, 0.5168, 0.4000],\n",
      "        [0.0680, 0.1847, 0.3075, 0.3953]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130131_jpg.rf.9ae016c3e2aa90ca0a84dd3ed5606726.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.7795])\n",
      "data: tensor([[354.9751, 170.6517, 428.7930, 263.2407,   0.7795,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[391.8840, 216.9462,  73.8179,  92.5890]])\n",
      "xywhn: tensor([[0.8164, 0.4520, 0.1538, 0.1929]])\n",
      "xyxy: tensor([[354.9751, 170.6517, 428.7930, 263.2407]])\n",
      "xyxyn: tensor([[0.7395, 0.3555, 0.8933, 0.5484]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Oseltamivir-45-MG-8-Copy_jpg.rf.08755f5aca06c3b6d3f376ceac474384.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9476, 0.9433])\n",
      "data: tensor([[250.0307,  11.2101, 478.3963, 478.5953,   0.9476,   2.0000],\n",
      "        [  2.1597,   6.1253, 253.6744, 479.5114,   0.9433,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[364.2135, 244.9027, 228.3655, 467.3852],\n",
      "        [127.9171, 242.8184, 251.5148, 473.3861]])\n",
      "xywhn: tensor([[0.7588, 0.5102, 0.4758, 0.9737],\n",
      "        [0.2665, 0.5059, 0.5240, 0.9862]])\n",
      "xyxy: tensor([[250.0307,  11.2101, 478.3963, 478.5953],\n",
      "        [  2.1597,   6.1253, 253.6744, 479.5114]])\n",
      "xyxyn: tensor([[0.5209, 0.0234, 0.9967, 0.9971],\n",
      "        [0.0045, 0.0128, 0.5285, 0.9990]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_101750_Pinterest_jpg.rf.f635423a64dc671c72cff9ec3d183a98.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 3., 3.])\n",
      "conf: tensor([0.7268, 0.6732, 0.2971])\n",
      "data: tensor([[1.4857e+01, 4.4735e+01, 2.5463e+02, 1.0593e+02, 7.2681e-01, 3.0000e+00],\n",
      "        [2.5957e+02, 7.7289e+01, 4.7746e+02, 1.5549e+02, 6.7324e-01, 3.0000e+00],\n",
      "        [1.1025e+01, 3.5535e+01, 2.6099e+02, 1.6103e+02, 2.9714e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[134.7434,  75.3300, 239.7719,  61.1905],\n",
      "        [368.5158, 116.3910, 217.8875,  78.2043],\n",
      "        [136.0075,  98.2805, 249.9644, 125.4915]])\n",
      "xywhn: tensor([[0.2807, 0.1569, 0.4995, 0.1275],\n",
      "        [0.7677, 0.2425, 0.4539, 0.1629],\n",
      "        [0.2833, 0.2048, 0.5208, 0.2614]])\n",
      "xyxy: tensor([[ 14.8575,  44.7347, 254.6294, 105.9252],\n",
      "        [259.5721,  77.2889, 477.4595, 155.4932],\n",
      "        [ 11.0253,  35.5347, 260.9897, 161.0262]])\n",
      "xyxyn: tensor([[0.0310, 0.0932, 0.5305, 0.2207],\n",
      "        [0.5408, 0.1610, 0.9947, 0.3239],\n",
      "        [0.0230, 0.0740, 0.5437, 0.3355]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100005_Pinterest_jpg.rf.91527503b88adf48f7f982ceabf71085.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8134])\n",
      "data: tensor([[ 16.5281,  35.1303, 455.6870, 316.6282,   0.8134,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[236.1075, 175.8793, 439.1588, 281.4979]])\n",
      "xywhn: tensor([[0.4919, 0.3664, 0.9149, 0.5865]])\n",
      "xyxy: tensor([[ 16.5281,  35.1303, 455.6870, 316.6282]])\n",
      "xyxyn: tensor([[0.0344, 0.0732, 0.9493, 0.6596]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/medicine-60-_jpg.rf.9cd381121b50c490f9db1682048e1702.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2.])\n",
      "conf: tensor([0.8111])\n",
      "data: tensor([[226.3008, 201.7453, 307.7997, 341.5325,   0.8111,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[267.0503, 271.6389,  81.4990, 139.7872]])\n",
      "xywhn: tensor([[0.5564, 0.5659, 0.1698, 0.2912]])\n",
      "xyxy: tensor([[226.3008, 201.7453, 307.7997, 341.5325]])\n",
      "xyxyn: tensor([[0.4715, 0.4203, 0.6412, 0.7115]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/paracetamol_071415111756_jpg.rf.24e4e04dbb8408b28bf87f08e9841d5e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.8620, 0.7756])\n",
      "data: tensor([[226.0369, 237.6619, 298.1660, 407.7132,   0.8620,   2.0000],\n",
      "        [154.2841, 242.7037, 248.0453, 398.8655,   0.7756,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[262.1015, 322.6875,  72.1291, 170.0513],\n",
      "        [201.1647, 320.7846,  93.7613, 156.1619]])\n",
      "xywhn: tensor([[0.5460, 0.6723, 0.1503, 0.3543],\n",
      "        [0.4191, 0.6683, 0.1953, 0.3253]])\n",
      "xyxy: tensor([[226.0369, 237.6619, 298.1660, 407.7132],\n",
      "        [154.2841, 242.7037, 248.0453, 398.8655]])\n",
      "xyxyn: tensor([[0.4709, 0.4951, 0.6212, 0.8494],\n",
      "        [0.3214, 0.5056, 0.5168, 0.8310]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151622_Pinterest_jpg.rf.d59265451b13ab2f6b4df572937cd6cc.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8895])\n",
      "data: tensor([[  2.1625,  43.4875, 480.0000, 199.7490,   0.8895,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[241.0813, 121.6183, 477.8375, 156.2615]])\n",
      "xywhn: tensor([[0.5023, 0.2534, 0.9955, 0.3255]])\n",
      "xyxy: tensor([[  2.1625,  43.4875, 480.0000, 199.7490]])\n",
      "xyxyn: tensor([[0.0045, 0.0906, 1.0000, 0.4161]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094006_Pinterest_jpg.rf.d6efba86e93b7dd5a04aaac80b56abc6.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.6344, 0.6104])\n",
      "data: tensor([[264.5829,  29.7776, 424.1732, 139.5498,   0.6344,   0.0000],\n",
      "        [295.2050, 137.3483, 423.0076, 239.2055,   0.6104,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[344.3781,  84.6637, 159.5903, 109.7723],\n",
      "        [359.1063, 188.2769, 127.8026, 101.8571]])\n",
      "xywhn: tensor([[0.7175, 0.1764, 0.3325, 0.2287],\n",
      "        [0.7481, 0.3922, 0.2663, 0.2122]])\n",
      "xyxy: tensor([[264.5829,  29.7776, 424.1732, 139.5498],\n",
      "        [295.2050, 137.3483, 423.0076, 239.2055]])\n",
      "xyxyn: tensor([[0.5512, 0.0620, 0.8837, 0.2907],\n",
      "        [0.6150, 0.2861, 0.8813, 0.4983]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104736_Pinterest_jpg.rf.a4049f1fa8d41eb8836778816bd4101c.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8213, 0.6417, 0.6267, 0.5390, 0.5276, 0.4744, 0.3828])\n",
      "data: tensor([[2.0631e+01, 1.2859e+02, 1.8348e+02, 2.5473e+02, 8.2133e-01, 0.0000e+00],\n",
      "        [7.0620e+01, 3.0083e+02, 2.1174e+02, 4.0887e+02, 6.4175e-01, 0.0000e+00],\n",
      "        [2.4538e+02, 1.4990e-01, 3.7283e+02, 7.1501e+01, 6.2670e-01, 0.0000e+00],\n",
      "        [2.9792e+02, 3.1696e+02, 4.0964e+02, 3.9168e+02, 5.3899e-01, 0.0000e+00],\n",
      "        [2.4311e+02, 1.5170e+02, 3.3111e+02, 2.2418e+02, 5.2758e-01, 0.0000e+00],\n",
      "        [3.2501e+02, 1.2156e+02, 4.4136e+02, 2.2212e+02, 4.7440e-01, 0.0000e+00],\n",
      "        [3.6204e+02, 2.5885e+02, 4.7829e+02, 3.3438e+02, 3.8275e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[102.0579, 191.6602, 162.8538, 126.1384],\n",
      "        [141.1777, 354.8494, 141.1163, 108.0431],\n",
      "        [309.1053,  35.8257, 127.4581,  71.3515],\n",
      "        [353.7780, 354.3184, 111.7155,  74.7159],\n",
      "        [287.1102, 187.9399,  88.0047,  72.4789],\n",
      "        [383.1850, 171.8360, 116.3540, 100.5615],\n",
      "        [420.1662, 296.6135, 116.2452,  75.5360]])\n",
      "xywhn: tensor([[0.2126, 0.3993, 0.3393, 0.2628],\n",
      "        [0.2941, 0.7393, 0.2940, 0.2251],\n",
      "        [0.6440, 0.0746, 0.2655, 0.1486],\n",
      "        [0.7370, 0.7382, 0.2327, 0.1557],\n",
      "        [0.5981, 0.3915, 0.1833, 0.1510],\n",
      "        [0.7983, 0.3580, 0.2424, 0.2095],\n",
      "        [0.8753, 0.6179, 0.2422, 0.1574]])\n",
      "xyxy: tensor([[2.0631e+01, 1.2859e+02, 1.8348e+02, 2.5473e+02],\n",
      "        [7.0620e+01, 3.0083e+02, 2.1174e+02, 4.0887e+02],\n",
      "        [2.4538e+02, 1.4990e-01, 3.7283e+02, 7.1501e+01],\n",
      "        [2.9792e+02, 3.1696e+02, 4.0964e+02, 3.9168e+02],\n",
      "        [2.4311e+02, 1.5170e+02, 3.3111e+02, 2.2418e+02],\n",
      "        [3.2501e+02, 1.2156e+02, 4.4136e+02, 2.2212e+02],\n",
      "        [3.6204e+02, 2.5885e+02, 4.7829e+02, 3.3438e+02]])\n",
      "xyxyn: tensor([[4.2981e-02, 2.6790e-01, 3.8226e-01, 5.3069e-01],\n",
      "        [1.4712e-01, 6.2672e-01, 4.4112e-01, 8.5181e-01],\n",
      "        [5.1120e-01, 3.1230e-04, 7.7674e-01, 1.4896e-01],\n",
      "        [6.2067e-01, 6.6033e-01, 8.5341e-01, 8.1599e-01],\n",
      "        [5.0647e-01, 3.1604e-01, 6.8982e-01, 4.6704e-01],\n",
      "        [6.7710e-01, 2.5324e-01, 9.1950e-01, 4.6274e-01],\n",
      "        [7.5426e-01, 5.3926e-01, 9.9644e-01, 6.9663e-01]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/celecoxib-200-MG-1-Copy-Copy_jpg.rf.6e0d084f6b13571a845e61d481d49bf5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9506, 0.9398])\n",
      "data: tensor([[ 18.6940, 224.8168, 477.3094, 451.7153,   0.9506,   2.0000],\n",
      "        [ 15.2457,   2.9988, 475.4850, 223.3279,   0.9398,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[248.0017, 338.2661, 458.6154, 226.8984],\n",
      "        [245.3654, 113.1633, 460.2393, 220.3291]])\n",
      "xywhn: tensor([[0.5167, 0.7047, 0.9554, 0.4727],\n",
      "        [0.5112, 0.2358, 0.9588, 0.4590]])\n",
      "xyxy: tensor([[ 18.6940, 224.8168, 477.3094, 451.7153],\n",
      "        [ 15.2457,   2.9988, 475.4850, 223.3279]])\n",
      "xyxyn: tensor([[0.0389, 0.4684, 0.9944, 0.9411],\n",
      "        [0.0318, 0.0062, 0.9906, 0.4653]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240522_210242_SHEIN_jpg.rf.19a0a1a7d15a6b5d27865175847cedb3.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 3., 3.])\n",
      "conf: tensor([0.7436, 0.2833, 0.2676])\n",
      "data: tensor([[0.0000e+00, 1.4796e+02, 4.8000e+02, 2.6586e+02, 7.4360e-01, 3.0000e+00],\n",
      "        [0.0000e+00, 4.6579e+01, 4.8000e+02, 1.6680e+02, 2.8334e-01, 3.0000e+00],\n",
      "        [0.0000e+00, 4.2719e+01, 4.8000e+02, 2.6800e+02, 2.6759e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[240.0000, 206.9109, 480.0000, 117.8976],\n",
      "        [240.0000, 106.6881, 480.0000, 120.2176],\n",
      "        [240.0000, 155.3597, 480.0000, 225.2820]])\n",
      "xywhn: tensor([[0.5000, 0.4311, 1.0000, 0.2456],\n",
      "        [0.5000, 0.2223, 1.0000, 0.2505],\n",
      "        [0.5000, 0.3237, 1.0000, 0.4693]])\n",
      "xyxy: tensor([[  0.0000, 147.9621, 480.0000, 265.8598],\n",
      "        [  0.0000,  46.5793, 480.0000, 166.7968],\n",
      "        [  0.0000,  42.7187, 480.0000, 268.0007]])\n",
      "xyxyn: tensor([[0.0000, 0.3083, 1.0000, 0.5539],\n",
      "        [0.0000, 0.0970, 1.0000, 0.3475],\n",
      "        [0.0000, 0.0890, 1.0000, 0.5583]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130657_jpg.rf.ca066f3d08dee76f345b3027f8c6106c.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.8913, 0.4552, 0.2910])\n",
      "data: tensor([[6.2416e+01, 9.1276e+01, 1.1390e+02, 2.0124e+02, 8.9133e-01, 1.0000e+00],\n",
      "        [2.5988e+02, 2.3037e+02, 3.2796e+02, 3.5012e+02, 4.5521e-01, 1.0000e+00],\n",
      "        [2.6014e+02, 2.6862e+02, 3.2414e+02, 3.5041e+02, 2.9101e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[ 88.1574, 146.2596,  51.4829, 109.9678],\n",
      "        [293.9195, 290.2451,  68.0720, 119.7542],\n",
      "        [292.1416, 309.5152,  63.9953,  81.7825]])\n",
      "xywhn: tensor([[0.1837, 0.3047, 0.1073, 0.2291],\n",
      "        [0.6123, 0.6047, 0.1418, 0.2495],\n",
      "        [0.6086, 0.6448, 0.1333, 0.1704]])\n",
      "xyxy: tensor([[ 62.4159,  91.2757, 113.8988, 201.2435],\n",
      "        [259.8835, 230.3680, 327.9555, 350.1221],\n",
      "        [260.1439, 268.6240, 324.1392, 350.4064]])\n",
      "xyxyn: tensor([[0.1300, 0.1902, 0.2373, 0.4193],\n",
      "        [0.5414, 0.4799, 0.6832, 0.7294],\n",
      "        [0.5420, 0.5596, 0.6753, 0.7300]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130245_jpg.rf.638a5954c3ab2c07c3a9c4f21e5de07c.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.6469])\n",
      "data: tensor([[244.2662, 229.6946, 331.1170, 314.2632,   0.6469,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[287.6916, 271.9789,  86.8508,  84.5686]])\n",
      "xywhn: tensor([[0.5994, 0.5666, 0.1809, 0.1762]])\n",
      "xyxy: tensor([[244.2662, 229.6946, 331.1170, 314.2632]])\n",
      "xyxyn: tensor([[0.5089, 0.4785, 0.6898, 0.6547]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/aprepitant-80-MG-12-_jpg.rf.e9223c2227d2d8a162869e955e51c09f.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9340, 0.9102])\n",
      "data: tensor([[2.0848e+00, 1.9115e+02, 4.7930e+02, 4.0897e+02, 9.3402e-01, 2.0000e+00],\n",
      "        [6.0250e+00, 4.3248e-01, 4.7892e+02, 1.8574e+02, 9.1019e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[240.6934, 300.0602, 477.2172, 217.8112],\n",
      "        [242.4742,  93.0870, 472.8984, 185.3090]])\n",
      "xywhn: tensor([[0.5014, 0.6251, 0.9942, 0.4538],\n",
      "        [0.5052, 0.1939, 0.9852, 0.3861]])\n",
      "xyxy: tensor([[2.0848e+00, 1.9115e+02, 4.7930e+02, 4.0897e+02],\n",
      "        [6.0250e+00, 4.3248e-01, 4.7892e+02, 1.8574e+02]])\n",
      "xyxyn: tensor([[4.3434e-03, 3.9824e-01, 9.9855e-01, 8.5201e-01],\n",
      "        [1.2552e-02, 9.0100e-04, 9.9776e-01, 3.8696e-01]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100634_Pinterest_jpg.rf.932e3a30f4e76d777f00ced384d4b0c8.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.6778, 0.4272])\n",
      "data: tensor([[202.3161,  47.2269, 422.7300, 212.8484,   0.6778,   0.0000],\n",
      "        [ 75.0916,  80.5205, 314.0532, 218.5484,   0.4272,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[312.5231, 130.0377, 220.4140, 165.6215],\n",
      "        [194.5724, 149.5345, 238.9616, 138.0279]])\n",
      "xywhn: tensor([[0.6511, 0.2709, 0.4592, 0.3450],\n",
      "        [0.4054, 0.3115, 0.4978, 0.2876]])\n",
      "xyxy: tensor([[202.3161,  47.2269, 422.7300, 212.8484],\n",
      "        [ 75.0916,  80.5205, 314.0532, 218.5484]])\n",
      "xyxyn: tensor([[0.4215, 0.0984, 0.8807, 0.4434],\n",
      "        [0.1564, 0.1678, 0.6543, 0.4553]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104713_Pinterest_jpg.rf.c107d6a88579d7246a043ac4ba16084c.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0.])\n",
      "conf: tensor([0.8641, 0.8555, 0.8339, 0.7091])\n",
      "data: tensor([[ 15.7440,  62.8986, 226.8140, 190.8136,   0.8641,   0.0000],\n",
      "        [ 15.1688, 226.4427, 214.8616, 402.8479,   0.8555,   0.0000],\n",
      "        [281.7057,  65.6462, 450.6623, 191.1844,   0.8339,   0.0000],\n",
      "        [263.6668, 254.8758, 453.8884, 398.2620,   0.7091,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[121.2790, 126.8561, 211.0699, 127.9150],\n",
      "        [115.0152, 314.6453, 199.6928, 176.4053],\n",
      "        [366.1840, 128.4153, 168.9567, 125.5381],\n",
      "        [358.7776, 326.5689, 190.2216, 143.3862]])\n",
      "xywhn: tensor([[0.2527, 0.2643, 0.4397, 0.2665],\n",
      "        [0.2396, 0.6555, 0.4160, 0.3675],\n",
      "        [0.7629, 0.2675, 0.3520, 0.2615],\n",
      "        [0.7475, 0.6804, 0.3963, 0.2987]])\n",
      "xyxy: tensor([[ 15.7440,  62.8986, 226.8140, 190.8136],\n",
      "        [ 15.1688, 226.4427, 214.8616, 402.8479],\n",
      "        [281.7057,  65.6462, 450.6623, 191.1844],\n",
      "        [263.6668, 254.8758, 453.8884, 398.2620]])\n",
      "xyxyn: tensor([[0.0328, 0.1310, 0.4725, 0.3975],\n",
      "        [0.0316, 0.4718, 0.4476, 0.8393],\n",
      "        [0.5869, 0.1368, 0.9389, 0.3983],\n",
      "        [0.5493, 0.5310, 0.9456, 0.8297]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150523_Pinterest_jpg.rf.46269cce6643d673eecbf89643281211.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.6381, 0.6156, 0.5820, 0.5462, 0.5400, 0.5134, 0.5061, 0.4906, 0.2824])\n",
      "data: tensor([[3.4230e+02, 6.2869e+01, 4.7802e+02, 1.4033e+02, 6.3806e-01, 0.0000e+00],\n",
      "        [1.7027e+02, 1.4782e+02, 3.1142e+02, 2.1371e+02, 6.1564e-01, 0.0000e+00],\n",
      "        [1.7965e+02, 5.7641e+01, 3.3040e+02, 1.1797e+02, 5.8203e-01, 0.0000e+00],\n",
      "        [2.2094e+01, 4.3080e+01, 1.6905e+02, 1.0414e+02, 5.4615e-01, 0.0000e+00],\n",
      "        [1.2548e+01, 1.2914e+02, 1.5415e+02, 2.0382e+02, 5.3997e-01, 0.0000e+00],\n",
      "        [2.0435e+01, 2.2610e+02, 1.3581e+02, 3.0612e+02, 5.1335e-01, 0.0000e+00],\n",
      "        [3.3940e+02, 1.6210e+02, 4.5812e+02, 2.3588e+02, 5.0610e-01, 0.0000e+00],\n",
      "        [3.2669e+02, 2.5546e+02, 4.5501e+02, 3.3215e+02, 4.9065e-01, 0.0000e+00],\n",
      "        [1.6902e+02, 2.3942e+02, 2.9595e+02, 3.1753e+02, 2.8241e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([9, 6])\n",
      "xywh: tensor([[410.1570, 101.5983, 135.7161,  77.4588],\n",
      "        [240.8465, 180.7640, 141.1551,  65.8980],\n",
      "        [255.0263,  87.8072, 150.7458,  60.3324],\n",
      "        [ 95.5721,  73.6101, 146.9569,  61.0604],\n",
      "        [ 83.3494, 166.4842, 141.6027,  74.6797],\n",
      "        [ 78.1226, 266.1132, 115.3743,  80.0229],\n",
      "        [398.7604, 198.9901, 118.7235,  73.7798],\n",
      "        [390.8469, 293.8042, 128.3232,  76.6865],\n",
      "        [232.4881, 278.4771, 126.9303,  78.1066]])\n",
      "xywhn: tensor([[0.8545, 0.2117, 0.2827, 0.1614],\n",
      "        [0.5018, 0.3766, 0.2941, 0.1373],\n",
      "        [0.5313, 0.1829, 0.3141, 0.1257],\n",
      "        [0.1991, 0.1534, 0.3062, 0.1272],\n",
      "        [0.1736, 0.3468, 0.2950, 0.1556],\n",
      "        [0.1628, 0.5544, 0.2404, 0.1667],\n",
      "        [0.8308, 0.4146, 0.2473, 0.1537],\n",
      "        [0.8143, 0.6121, 0.2673, 0.1598],\n",
      "        [0.4844, 0.5802, 0.2644, 0.1627]])\n",
      "xyxy: tensor([[342.2990,  62.8689, 478.0150, 140.3277],\n",
      "        [170.2689, 147.8150, 311.4240, 213.7130],\n",
      "        [179.6534,  57.6410, 330.3992, 117.9733],\n",
      "        [ 22.0936,  43.0799, 169.0505, 104.1404],\n",
      "        [ 12.5480, 129.1443, 154.1507, 203.8241],\n",
      "        [ 20.4354, 226.1017, 135.8098, 306.1246],\n",
      "        [339.3987, 162.1002, 458.1222, 235.8800],\n",
      "        [326.6853, 255.4610, 455.0085, 332.1475],\n",
      "        [169.0230, 239.4239, 295.9532, 317.5304]])\n",
      "xyxyn: tensor([[0.7131, 0.1310, 0.9959, 0.2923],\n",
      "        [0.3547, 0.3079, 0.6488, 0.4452],\n",
      "        [0.3743, 0.1201, 0.6883, 0.2458],\n",
      "        [0.0460, 0.0897, 0.3522, 0.2170],\n",
      "        [0.0261, 0.2691, 0.3211, 0.4246],\n",
      "        [0.0426, 0.4710, 0.2829, 0.6378],\n",
      "        [0.7071, 0.3377, 0.9544, 0.4914],\n",
      "        [0.6806, 0.5322, 0.9479, 0.6920],\n",
      "        [0.3521, 0.4988, 0.6166, 0.6615]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151832_Pinterest_jpg.rf.ed3a795c855437cb7dc3bcf7804d5c45.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8648])\n",
      "data: tensor([[4.5441e-02, 6.5157e+01, 4.7868e+02, 2.2124e+02, 8.6478e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[239.3649, 143.2000, 478.6390, 156.0863]])\n",
      "xywhn: tensor([[0.4987, 0.2983, 0.9972, 0.3252]])\n",
      "xyxy: tensor([[4.5441e-02, 6.5157e+01, 4.7868e+02, 2.2124e+02]])\n",
      "xyxyn: tensor([[9.4668e-05, 1.3574e-01, 9.9726e-01, 4.6092e-01]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_105518_Pinterest_jpg.rf.2822ddeaa3b16aff497526a1fbeb9925.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 3., 3., 3., 3., 3.])\n",
      "conf: tensor([0.8737, 0.8360, 0.7502, 0.6961, 0.6269, 0.2946])\n",
      "data: tensor([[2.4209e+02, 3.9720e+01, 4.6796e+02, 1.1542e+02, 8.7373e-01, 3.0000e+00],\n",
      "        [2.4496e+02, 3.1175e+02, 4.6492e+02, 3.8036e+02, 8.3599e-01, 3.0000e+00],\n",
      "        [1.0339e+01, 3.3042e+02, 2.3661e+02, 3.9195e+02, 7.5022e-01, 3.0000e+00],\n",
      "        [2.4758e+02, 1.6507e+02, 4.5999e+02, 2.2913e+02, 6.9608e-01, 3.0000e+00],\n",
      "        [2.0693e+01, 1.6554e+02, 2.3388e+02, 2.7162e+02, 6.2693e-01, 3.0000e+00],\n",
      "        [2.4393e+02, 2.2041e+02, 4.0488e+02, 2.6714e+02, 2.9459e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[355.0236,  77.5698, 225.8722,  75.6992],\n",
      "        [354.9386, 346.0591, 219.9599,  68.6085],\n",
      "        [123.4746, 361.1850, 226.2721,  61.5287],\n",
      "        [353.7828, 197.0988, 212.4052,  64.0665],\n",
      "        [127.2848, 218.5789, 213.1841, 106.0796],\n",
      "        [324.4052, 243.7716, 160.9519,  46.7321]])\n",
      "xywhn: tensor([[0.7396, 0.1616, 0.4706, 0.1577],\n",
      "        [0.7395, 0.7210, 0.4582, 0.1429],\n",
      "        [0.2572, 0.7525, 0.4714, 0.1282],\n",
      "        [0.7370, 0.4106, 0.4425, 0.1335],\n",
      "        [0.2652, 0.4554, 0.4441, 0.2210],\n",
      "        [0.6758, 0.5079, 0.3353, 0.0974]])\n",
      "xyxy: tensor([[242.0875,  39.7202, 467.9597, 115.4193],\n",
      "        [244.9586, 311.7548, 464.9185, 380.3633],\n",
      "        [ 10.3386, 330.4207, 236.6107, 391.9493],\n",
      "        [247.5802, 165.0655, 459.9854, 229.1320],\n",
      "        [ 20.6928, 165.5391, 233.8768, 271.6187],\n",
      "        [243.9292, 220.4056, 404.8811, 267.1377]])\n",
      "xyxyn: tensor([[0.5043, 0.0828, 0.9749, 0.2405],\n",
      "        [0.5103, 0.6495, 0.9686, 0.7924],\n",
      "        [0.0215, 0.6884, 0.4929, 0.8166],\n",
      "        [0.5158, 0.3439, 0.9583, 0.4774],\n",
      "        [0.0431, 0.3449, 0.4872, 0.5659],\n",
      "        [0.5082, 0.4592, 0.8435, 0.5565]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/sitagliptin-50-MG-5-_jpg.rf.977549fff0f745a7c615e2405fb85d75.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9363, 0.9278])\n",
      "data: tensor([[250.3598,  66.5530, 477.9907, 397.1126,   0.9363,   2.0000],\n",
      "        [ 17.2684,  67.7861, 247.6867, 394.4349,   0.9278,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[364.1752, 231.8328, 227.6309, 330.5596],\n",
      "        [132.4776, 231.1105, 230.4183, 326.6489]])\n",
      "xywhn: tensor([[0.7587, 0.4830, 0.4742, 0.6887],\n",
      "        [0.2760, 0.4815, 0.4800, 0.6805]])\n",
      "xyxy: tensor([[250.3598,  66.5530, 477.9907, 397.1126],\n",
      "        [ 17.2684,  67.7861, 247.6867, 394.4349]])\n",
      "xyxyn: tensor([[0.5216, 0.1387, 0.9958, 0.8273],\n",
      "        [0.0360, 0.1412, 0.5160, 0.8217]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094351_Pinterest_jpg.rf.7a453cfb53f1cd99b563b51ea48c4d0a.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([])\n",
      "conf: tensor([])\n",
      "data: tensor([], size=(0, 6))\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([0, 6])\n",
      "xywh: tensor([], size=(0, 4))\n",
      "xywhn: tensor([], size=(0, 4))\n",
      "xyxy: tensor([], size=(0, 4))\n",
      "xyxyn: tensor([], size=(0, 4))\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104744_Pinterest_jpg.rf.9cd2998d16d0745111adbd2459963b4d.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8641, 0.7752, 0.7739, 0.6133, 0.4699])\n",
      "data: tensor([[245.3615,  16.9770, 466.3530, 150.1742,   0.8641,   0.0000],\n",
      "        [ 13.7214,  88.1262, 190.7917, 217.0110,   0.7752,   0.0000],\n",
      "        [ 21.2802, 252.8117, 228.2668, 366.4231,   0.7739,   0.0000],\n",
      "        [285.4012, 181.1194, 434.8911, 293.8168,   0.6133,   0.0000],\n",
      "        [ 12.6860, 340.8685, 137.3241, 403.3163,   0.4699,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[355.8572,  83.5756, 220.9914, 133.1972],\n",
      "        [102.2565, 152.5686, 177.0703, 128.8848],\n",
      "        [124.7735, 309.6174, 206.9866, 113.6114],\n",
      "        [360.1462, 237.4681, 149.4899, 112.6974],\n",
      "        [ 75.0051, 372.0924, 124.6381,  62.4479]])\n",
      "xywhn: tensor([[0.7414, 0.1741, 0.4604, 0.2775],\n",
      "        [0.2130, 0.3179, 0.3689, 0.2685],\n",
      "        [0.2599, 0.6450, 0.4312, 0.2367],\n",
      "        [0.7503, 0.4947, 0.3114, 0.2348],\n",
      "        [0.1563, 0.7752, 0.2597, 0.1301]])\n",
      "xyxy: tensor([[245.3615,  16.9770, 466.3530, 150.1742],\n",
      "        [ 13.7214,  88.1262, 190.7917, 217.0110],\n",
      "        [ 21.2802, 252.8117, 228.2668, 366.4231],\n",
      "        [285.4012, 181.1194, 434.8911, 293.8168],\n",
      "        [ 12.6860, 340.8685, 137.3241, 403.3163]])\n",
      "xyxyn: tensor([[0.5112, 0.0354, 0.9716, 0.3129],\n",
      "        [0.0286, 0.1836, 0.3975, 0.4521],\n",
      "        [0.0443, 0.5267, 0.4756, 0.7634],\n",
      "        [0.5946, 0.3773, 0.9060, 0.6121],\n",
      "        [0.0264, 0.7101, 0.2861, 0.8402]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104545_Pinterest_jpg.rf.edb3f5428691b42a13cdd211de4c37f5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0.])\n",
      "conf: tensor([0.8383, 0.7911, 0.6561, 0.6142])\n",
      "data: tensor([[ 23.3255, 211.3656, 219.1442, 351.6413,   0.8383,   0.0000],\n",
      "        [ 22.8130,  94.0623, 223.0537, 186.5864,   0.7911,   0.0000],\n",
      "        [263.6471,  20.9916, 443.8989, 114.7244,   0.6561,   0.0000],\n",
      "        [288.0394, 164.1100, 428.7707, 295.2032,   0.6142,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[121.2348, 281.5034, 195.8187, 140.2757],\n",
      "        [122.9333, 140.3244, 200.2407,  92.5241],\n",
      "        [353.7730,  67.8580, 180.2518,  93.7328],\n",
      "        [358.4050, 229.6566, 140.7313, 131.0932]])\n",
      "xywhn: tensor([[0.2526, 0.5865, 0.4080, 0.2922],\n",
      "        [0.2561, 0.2923, 0.4172, 0.1928],\n",
      "        [0.7370, 0.1414, 0.3755, 0.1953],\n",
      "        [0.7467, 0.4785, 0.2932, 0.2731]])\n",
      "xyxy: tensor([[ 23.3255, 211.3656, 219.1442, 351.6413],\n",
      "        [ 22.8130,  94.0623, 223.0537, 186.5864],\n",
      "        [263.6471,  20.9916, 443.8989, 114.7244],\n",
      "        [288.0394, 164.1100, 428.7707, 295.2032]])\n",
      "xyxyn: tensor([[0.0486, 0.4403, 0.4566, 0.7326],\n",
      "        [0.0475, 0.1960, 0.4647, 0.3887],\n",
      "        [0.5493, 0.0437, 0.9248, 0.2390],\n",
      "        [0.6001, 0.3419, 0.8933, 0.6150]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Oseltamivir-45-MG-11-Copy_jpg.rf.fb0cec1a7bf5e6816bf1502ddfe29ccf.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9467, 0.9451])\n",
      "data: tensor([[  0.0000, 226.2357, 479.1921, 450.9861,   0.9467,   2.0000],\n",
      "        [  0.0000,   2.5319, 478.9498, 223.9476,   0.9451,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[239.5961, 338.6109, 479.1921, 224.7504],\n",
      "        [239.4749, 113.2397, 478.9498, 221.4157]])\n",
      "xywhn: tensor([[0.4992, 0.7054, 0.9983, 0.4682],\n",
      "        [0.4989, 0.2359, 0.9978, 0.4613]])\n",
      "xyxy: tensor([[  0.0000, 226.2357, 479.1921, 450.9861],\n",
      "        [  0.0000,   2.5319, 478.9498, 223.9476]])\n",
      "xyxyn: tensor([[0.0000, 0.4713, 0.9983, 0.9396],\n",
      "        [0.0000, 0.0053, 0.9978, 0.4666]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151456_Pinterest_jpg.rf.3c2687f58fea3ebf803df37d80ec2fd6.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8306])\n",
      "data: tensor([[  0.0000,  49.3212, 480.0000, 203.0600,   0.8306,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[240.0000, 126.1906, 480.0000, 153.7388]])\n",
      "xywhn: tensor([[0.5000, 0.2629, 1.0000, 0.3203]])\n",
      "xyxy: tensor([[  0.0000,  49.3212, 480.0000, 203.0600]])\n",
      "xyxyn: tensor([[0.0000, 0.1028, 1.0000, 0.4230]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_095224_Pinterest_jpg.rf.339767c03b35612ec381c1455ee67116.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8163, 0.7764])\n",
      "data: tensor([[2.1864e+02, 2.2668e+01, 4.7845e+02, 2.3113e+02, 8.1634e-01, 0.0000e+00],\n",
      "        [2.1489e-01, 2.7287e+01, 2.3367e+02, 2.3755e+02, 7.7641e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[348.5432, 126.8971, 259.8110, 208.4585],\n",
      "        [116.9434, 132.4161, 233.4570, 210.2585]])\n",
      "xywhn: tensor([[0.7261, 0.2644, 0.5413, 0.4343],\n",
      "        [0.2436, 0.2759, 0.4864, 0.4380]])\n",
      "xyxy: tensor([[2.1864e+02, 2.2668e+01, 4.7845e+02, 2.3113e+02],\n",
      "        [2.1489e-01, 2.7287e+01, 2.3367e+02, 2.3755e+02]])\n",
      "xyxyn: tensor([[4.5550e-01, 4.7225e-02, 9.9677e-01, 4.8151e-01],\n",
      "        [4.4769e-04, 5.6848e-02, 4.8682e-01, 4.9489e-01]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/celecoxib-200-MG-7-_jpg.rf.41866895c696f1930075b04aded809ee.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2.])\n",
      "conf: tensor([0.9540])\n",
      "data: tensor([[  0.0000,   0.0000, 477.3004, 479.8640,   0.9540,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[238.6502, 239.9320, 477.3004, 479.8640]])\n",
      "xywhn: tensor([[0.4972, 0.4999, 0.9944, 0.9997]])\n",
      "xyxy: tensor([[  0.0000,   0.0000, 477.3004, 479.8640]])\n",
      "xyxyn: tensor([[0.0000, 0.0000, 0.9944, 0.9997]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150258_Pinterest_jpg.rf.29fb34842b49ce9eb71abe3d9c578384.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8111, 0.6781, 0.6608, 0.3486, 0.2825])\n",
      "data: tensor([[2.2354e+01, 9.4244e+01, 2.2338e+02, 2.3717e+02, 8.1108e-01, 0.0000e+00],\n",
      "        [3.5439e+02, 2.5303e+02, 4.6108e+02, 3.1823e+02, 6.7814e-01, 0.0000e+00],\n",
      "        [2.7760e+02, 1.2489e+02, 4.4969e+02, 2.1815e+02, 6.6080e-01, 0.0000e+00],\n",
      "        [1.7307e+01, 3.6978e+02, 9.6015e+01, 4.2120e+02, 3.4860e-01, 0.0000e+00],\n",
      "        [1.0097e+02, 3.5481e+02, 1.9057e+02, 4.1748e+02, 2.8248e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[122.8693, 165.7063, 201.0306, 142.9249],\n",
      "        [407.7344, 285.6298, 106.6943,  65.1974],\n",
      "        [363.6458, 171.5186, 172.0870,  93.2661],\n",
      "        [ 56.6607, 395.4901,  78.7081,  51.4158],\n",
      "        [145.7713, 386.1495,  89.6034,  62.6700]])\n",
      "xywhn: tensor([[0.2560, 0.3452, 0.4188, 0.2978],\n",
      "        [0.8494, 0.5951, 0.2223, 0.1358],\n",
      "        [0.7576, 0.3573, 0.3585, 0.1943],\n",
      "        [0.1180, 0.8239, 0.1640, 0.1071],\n",
      "        [0.3037, 0.8045, 0.1867, 0.1306]])\n",
      "xyxy: tensor([[ 22.3540,  94.2438, 223.3846, 237.1687],\n",
      "        [354.3873, 253.0311, 461.0816, 318.2285],\n",
      "        [277.6022, 124.8855, 449.6893, 218.1516],\n",
      "        [ 17.3066, 369.7822,  96.0147, 421.1980],\n",
      "        [100.9696, 354.8145, 190.5730, 417.4845]])\n",
      "xyxyn: tensor([[0.0466, 0.1963, 0.4654, 0.4941],\n",
      "        [0.7383, 0.5271, 0.9606, 0.6630],\n",
      "        [0.5783, 0.2602, 0.9369, 0.4545],\n",
      "        [0.0361, 0.7704, 0.2000, 0.8775],\n",
      "        [0.2104, 0.7392, 0.3970, 0.8698]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240531_092731_Chrome_jpg.rf.10cc7c733f67ad1331f00ed881e737ee.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "conf: tensor([0.7864, 0.7482, 0.7396, 0.7332, 0.7180, 0.6948, 0.6830, 0.6321, 0.6211, 0.5971, 0.5546, 0.5115, 0.4666, 0.4176, 0.4020, 0.2770, 0.2749, 0.2694])\n",
      "data: tensor([[8.6833e+01, 8.7157e+01, 1.5217e+02, 1.4492e+02, 7.8644e-01, 2.0000e+00],\n",
      "        [3.9918e+02, 9.0806e+01, 4.6365e+02, 1.4843e+02, 7.4820e-01, 2.0000e+00],\n",
      "        [5.8303e+00, 8.8220e+01, 7.6294e+01, 1.4525e+02, 7.3956e-01, 2.0000e+00],\n",
      "        [3.2614e+02, 9.2664e+01, 3.8837e+02, 1.4565e+02, 7.3320e-01, 2.0000e+00],\n",
      "        [2.4886e+02, 1.7576e+02, 3.1236e+02, 2.2551e+02, 7.1804e-01, 2.0000e+00],\n",
      "        [1.7015e+02, 1.7492e+02, 2.3212e+02, 2.2563e+02, 6.9476e-01, 2.0000e+00],\n",
      "        [1.0074e+01, 1.7152e+02, 7.2403e+01, 2.2650e+02, 6.8303e-01, 2.0000e+00],\n",
      "        [2.4385e+02, 9.2245e+01, 3.1207e+02, 1.4459e+02, 6.3206e-01, 2.0000e+00],\n",
      "        [1.6908e+02, 9.4187e+01, 2.3144e+02, 1.4419e+02, 6.2110e-01, 2.0000e+00],\n",
      "        [1.4904e+01, 2.5451e+02, 8.6455e+01, 2.8581e+02, 5.9715e-01, 2.0000e+00],\n",
      "        [8.3753e+01, 1.5530e+02, 1.5704e+02, 2.2454e+02, 5.5463e-01, 2.0000e+00],\n",
      "        [3.3129e+02, 1.8234e+02, 3.8301e+02, 2.2402e+02, 5.1146e-01, 2.0000e+00],\n",
      "        [3.8247e+02, 2.5119e+02, 4.6392e+02, 2.8898e+02, 4.6658e-01, 2.0000e+00],\n",
      "        [2.6473e+02, 2.5636e+02, 3.3309e+02, 2.8881e+02, 4.1764e-01, 2.0000e+00],\n",
      "        [4.0995e+02, 1.9185e+02, 4.5937e+02, 2.2427e+02, 4.0203e-01, 2.0000e+00],\n",
      "        [1.2895e+02, 2.4784e+02, 2.1998e+02, 2.8855e+02, 2.7705e-01, 2.0000e+00],\n",
      "        [3.0989e+02, 4.1364e+02, 3.9139e+02, 4.3673e+02, 2.7492e-01, 2.0000e+00],\n",
      "        [4.1541e+02, 1.9384e+02, 4.5352e+02, 2.2286e+02, 2.6944e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([18, 6])\n",
      "xywh: tensor([[119.4993, 116.0362,  65.3331,  57.7592],\n",
      "        [431.4184, 119.6198,  64.4716,  57.6270],\n",
      "        [ 41.0624, 116.7362,  70.4642,  57.0322],\n",
      "        [357.2574, 119.1579,  62.2341,  52.9889],\n",
      "        [280.6117, 200.6340,  63.4988,  49.7487],\n",
      "        [201.1309, 200.2759,  61.9700,  50.7025],\n",
      "        [ 41.2385, 199.0078,  62.3298,  54.9813],\n",
      "        [277.9589, 118.4199,  68.2199,  52.3500],\n",
      "        [200.2592, 119.1868,  62.3660,  49.9997],\n",
      "        [ 50.6794, 270.1629,  71.5502,  31.2980],\n",
      "        [120.3939, 189.9234,  73.2827,  69.2424],\n",
      "        [357.1502, 203.1789,  51.7258,  41.6868],\n",
      "        [423.1953, 270.0834,  81.4430,  37.7922],\n",
      "        [298.9067, 272.5874,  68.3605,  32.4503],\n",
      "        [434.6563, 208.0615,  49.4213,  32.4214],\n",
      "        [174.4658, 268.1928,  91.0317,  40.7046],\n",
      "        [350.6398, 425.1879,  81.4939,  23.0872],\n",
      "        [434.4637, 208.3511,  38.1055,  29.0198]])\n",
      "xywhn: tensor([[0.2490, 0.2417, 0.1361, 0.1203],\n",
      "        [0.8988, 0.2492, 0.1343, 0.1201],\n",
      "        [0.0855, 0.2432, 0.1468, 0.1188],\n",
      "        [0.7443, 0.2482, 0.1297, 0.1104],\n",
      "        [0.5846, 0.4180, 0.1323, 0.1036],\n",
      "        [0.4190, 0.4172, 0.1291, 0.1056],\n",
      "        [0.0859, 0.4146, 0.1299, 0.1145],\n",
      "        [0.5791, 0.2467, 0.1421, 0.1091],\n",
      "        [0.4172, 0.2483, 0.1299, 0.1042],\n",
      "        [0.1056, 0.5628, 0.1491, 0.0652],\n",
      "        [0.2508, 0.3957, 0.1527, 0.1443],\n",
      "        [0.7441, 0.4233, 0.1078, 0.0868],\n",
      "        [0.8817, 0.5627, 0.1697, 0.0787],\n",
      "        [0.6227, 0.5679, 0.1424, 0.0676],\n",
      "        [0.9055, 0.4335, 0.1030, 0.0675],\n",
      "        [0.3635, 0.5587, 0.1896, 0.0848],\n",
      "        [0.7305, 0.8858, 0.1698, 0.0481],\n",
      "        [0.9051, 0.4341, 0.0794, 0.0605]])\n",
      "xyxy: tensor([[ 86.8328,  87.1566, 152.1659, 144.9158],\n",
      "        [399.1826,  90.8063, 463.6542, 148.4333],\n",
      "        [  5.8303,  88.2201,  76.2945, 145.2523],\n",
      "        [326.1403,  92.6635, 388.3744, 145.6524],\n",
      "        [248.8623, 175.7596, 312.3611, 225.5083],\n",
      "        [170.1459, 174.9247, 232.1159, 225.6272],\n",
      "        [ 10.0736, 171.5171,  72.4034, 226.4984],\n",
      "        [243.8489,  92.2449, 312.0688, 144.5949],\n",
      "        [169.0762,  94.1870, 231.4423, 144.1867],\n",
      "        [ 14.9043, 254.5139,  86.4545, 285.8119],\n",
      "        [ 83.7525, 155.3022, 157.0353, 224.5446],\n",
      "        [331.2874, 182.3355, 383.0131, 224.0223],\n",
      "        [382.4738, 251.1873, 463.9167, 288.9795],\n",
      "        [264.7264, 256.3622, 333.0869, 288.8126],\n",
      "        [409.9457, 191.8508, 459.3669, 224.2722],\n",
      "        [128.9500, 247.8405, 219.9816, 288.5451],\n",
      "        [309.8928, 413.6443, 391.3867, 436.7315],\n",
      "        [415.4110, 193.8412, 453.5165, 222.8610]])\n",
      "xyxyn: tensor([[0.1809, 0.1816, 0.3170, 0.3019],\n",
      "        [0.8316, 0.1892, 0.9659, 0.3092],\n",
      "        [0.0121, 0.1838, 0.1589, 0.3026],\n",
      "        [0.6795, 0.1930, 0.8091, 0.3034],\n",
      "        [0.5185, 0.3662, 0.6508, 0.4698],\n",
      "        [0.3545, 0.3644, 0.4836, 0.4701],\n",
      "        [0.0210, 0.3573, 0.1508, 0.4719],\n",
      "        [0.5080, 0.1922, 0.6501, 0.3012],\n",
      "        [0.3522, 0.1962, 0.4822, 0.3004],\n",
      "        [0.0311, 0.5302, 0.1801, 0.5954],\n",
      "        [0.1745, 0.3235, 0.3272, 0.4678],\n",
      "        [0.6902, 0.3799, 0.7979, 0.4667],\n",
      "        [0.7968, 0.5233, 0.9665, 0.6020],\n",
      "        [0.5515, 0.5341, 0.6939, 0.6017],\n",
      "        [0.8541, 0.3997, 0.9570, 0.4672],\n",
      "        [0.2686, 0.5163, 0.4583, 0.6011],\n",
      "        [0.6456, 0.8618, 0.8154, 0.9099],\n",
      "        [0.8654, 0.4038, 0.9448, 0.4643]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/carvedilol-3-125-MG-24-Copy_jpg.rf.573785b008430ecf6d1e7b918d201b8e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2.])\n",
      "conf: tensor([0.8593])\n",
      "data: tensor([[  0.0000,  39.3748, 478.6054, 467.6194,   0.8593,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[239.3027, 253.4971, 478.6054, 428.2446]])\n",
      "xywhn: tensor([[0.4985, 0.5281, 0.9971, 0.8922]])\n",
      "xyxy: tensor([[  0.0000,  39.3748, 478.6054, 467.6194]])\n",
      "xyxyn: tensor([[0.0000, 0.0820, 0.9971, 0.9742]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240523_145320_jpg.rf.01b765efffec5d2bccc497afc368f7bb.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 1.])\n",
      "conf: tensor([0.6624, 0.3959])\n",
      "data: tensor([[8.7107e+01, 1.6901e+02, 4.1589e+02, 3.4869e+02, 6.6239e-01, 3.0000e+00],\n",
      "        [1.0405e+02, 1.8420e+02, 2.5705e+02, 3.3705e+02, 3.9589e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[251.4995, 258.8518, 328.7854, 179.6853],\n",
      "        [180.5497, 260.6266, 152.9969, 152.8484]])\n",
      "xywhn: tensor([[0.5240, 0.5393, 0.6850, 0.3743],\n",
      "        [0.3761, 0.5430, 0.3187, 0.3184]])\n",
      "xyxy: tensor([[ 87.1068, 169.0092, 415.8922, 348.6945],\n",
      "        [104.0512, 184.2024, 257.0482, 337.0508]])\n",
      "xyxyn: tensor([[0.1815, 0.3521, 0.8664, 0.7264],\n",
      "        [0.2168, 0.3838, 0.5355, 0.7022]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130019_jpg.rf.4e88f21266dd21a9083c3a29a76ee661.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1.])\n",
      "conf: tensor([0.8292, 0.8271, 0.8168, 0.5221])\n",
      "data: tensor([[351.9425, 132.8951, 396.6549, 216.8805,   0.8292,   1.0000],\n",
      "        [343.4713, 243.2496, 418.0992, 337.8120,   0.8271,   1.0000],\n",
      "        [275.0831, 155.8244, 320.5821, 247.0544,   0.8168,   1.0000],\n",
      "        [148.5699, 199.7175, 204.3731, 254.9444,   0.5221,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[374.2987, 174.8878,  44.7124,  83.9854],\n",
      "        [380.7852, 290.5308,  74.6279,  94.5624],\n",
      "        [297.8326, 201.4394,  45.4990,  91.2300],\n",
      "        [176.4715, 227.3309,  55.8032,  55.2269]])\n",
      "xywhn: tensor([[0.7798, 0.3643, 0.0932, 0.1750],\n",
      "        [0.7933, 0.6053, 0.1555, 0.1970],\n",
      "        [0.6205, 0.4197, 0.0948, 0.1901],\n",
      "        [0.3676, 0.4736, 0.1163, 0.1151]])\n",
      "xyxy: tensor([[351.9425, 132.8951, 396.6549, 216.8805],\n",
      "        [343.4713, 243.2496, 418.0992, 337.8120],\n",
      "        [275.0831, 155.8244, 320.5821, 247.0544],\n",
      "        [148.5699, 199.7175, 204.3731, 254.9444]])\n",
      "xyxyn: tensor([[0.7332, 0.2769, 0.8264, 0.4518],\n",
      "        [0.7156, 0.5068, 0.8710, 0.7038],\n",
      "        [0.5731, 0.3246, 0.6679, 0.5147],\n",
      "        [0.3095, 0.4161, 0.4258, 0.5311]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151126_Pinterest_jpg.rf.1eeb83dbee7f29789548aa21a7f96165.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8549, 0.7034, 0.6415, 0.6401, 0.5943, 0.5914, 0.5482, 0.5144])\n",
      "data: tensor([[257.4815, 217.3974, 467.5406, 395.3934,   0.8549,   0.0000],\n",
      "        [334.5032,  89.8856, 453.7186, 192.1304,   0.7034,   0.0000],\n",
      "        [ 74.4528, 140.4841, 193.2604, 193.5358,   0.6415,   0.0000],\n",
      "        [248.8632,  56.2788, 361.6421, 156.1830,   0.6401,   0.0000],\n",
      "        [124.8719, 222.6755, 233.0545, 295.4968,   0.5943,   0.0000],\n",
      "        [ 26.1170, 227.8569, 123.6921, 303.7242,   0.5914,   0.0000],\n",
      "        [ 22.5391, 321.3137, 121.5639, 385.1161,   0.5482,   0.0000],\n",
      "        [131.6505, 300.0476, 233.0099, 369.7715,   0.5144,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([8, 6])\n",
      "xywh: tensor([[362.5110, 306.3954, 210.0591, 177.9960],\n",
      "        [394.1109, 141.0080, 119.2155, 102.2448],\n",
      "        [133.8566, 167.0099, 118.8076,  53.0518],\n",
      "        [305.2527, 106.2309, 112.7789,  99.9042],\n",
      "        [178.9632, 259.0861, 108.1826,  72.8213],\n",
      "        [ 74.9045, 265.7906,  97.5751,  75.8673],\n",
      "        [ 72.0515, 353.2149,  99.0248,  63.8024],\n",
      "        [182.3302, 334.9095, 101.3594,  69.7238]])\n",
      "xywhn: tensor([[0.7552, 0.6383, 0.4376, 0.3708],\n",
      "        [0.8211, 0.2938, 0.2484, 0.2130],\n",
      "        [0.2789, 0.3479, 0.2475, 0.1105],\n",
      "        [0.6359, 0.2213, 0.2350, 0.2081],\n",
      "        [0.3728, 0.5398, 0.2254, 0.1517],\n",
      "        [0.1561, 0.5537, 0.2033, 0.1581],\n",
      "        [0.1501, 0.7359, 0.2063, 0.1329],\n",
      "        [0.3799, 0.6977, 0.2112, 0.1453]])\n",
      "xyxy: tensor([[257.4815, 217.3974, 467.5406, 395.3934],\n",
      "        [334.5032,  89.8856, 453.7186, 192.1304],\n",
      "        [ 74.4528, 140.4841, 193.2604, 193.5358],\n",
      "        [248.8632,  56.2788, 361.6421, 156.1830],\n",
      "        [124.8719, 222.6755, 233.0545, 295.4968],\n",
      "        [ 26.1170, 227.8569, 123.6921, 303.7242],\n",
      "        [ 22.5391, 321.3137, 121.5639, 385.1161],\n",
      "        [131.6505, 300.0476, 233.0099, 369.7715]])\n",
      "xyxyn: tensor([[0.5364, 0.4529, 0.9740, 0.8237],\n",
      "        [0.6969, 0.1873, 0.9452, 0.4003],\n",
      "        [0.1551, 0.2927, 0.4026, 0.4032],\n",
      "        [0.5185, 0.1172, 0.7534, 0.3254],\n",
      "        [0.2601, 0.4639, 0.4855, 0.6156],\n",
      "        [0.0544, 0.4747, 0.2577, 0.6328],\n",
      "        [0.0470, 0.6694, 0.2533, 0.8023],\n",
      "        [0.2743, 0.6251, 0.4854, 0.7704]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100209_Pinterest_jpg.rf.f92117fe6f5e0ee4664fd49312a4d819.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8217, 0.5319])\n",
      "data: tensor([[ 36.9364,  22.9397, 299.8242, 224.7387,   0.8217,   0.0000],\n",
      "        [268.3865,  60.9889, 478.1503, 256.0525,   0.5319,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[168.3803, 123.8392, 262.8877, 201.7991],\n",
      "        [373.2684, 158.5207, 209.7638, 195.0636]])\n",
      "xywhn: tensor([[0.3508, 0.2580, 0.5477, 0.4204],\n",
      "        [0.7776, 0.3303, 0.4370, 0.4064]])\n",
      "xyxy: tensor([[ 36.9364,  22.9397, 299.8242, 224.7387],\n",
      "        [268.3865,  60.9889, 478.1503, 256.0525]])\n",
      "xyxyn: tensor([[0.0770, 0.0478, 0.6246, 0.4682],\n",
      "        [0.5591, 0.1271, 0.9961, 0.5334]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_101020_Pinterest_jpg.rf.30479970832f04b364ed9f7a8c02b39a.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.9461])\n",
      "data: tensor([[  3.5898, 118.4295, 468.4948, 226.6426,   0.9461,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[236.0423, 172.5360, 464.9050, 108.2131]])\n",
      "xywhn: tensor([[0.4918, 0.3595, 0.9686, 0.2254]])\n",
      "xyxy: tensor([[  3.5898, 118.4295, 468.4948, 226.6426]])\n",
      "xyxyn: tensor([[0.0075, 0.2467, 0.9760, 0.4722]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_093900_Pinterest_jpg.rf.a03648da6828b3e127f7d0a1cba05da5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.7465, 0.7138])\n",
      "data: tensor([[ 68.2719,  34.6997, 312.2845, 200.7065,   0.7465,   0.0000],\n",
      "        [235.8628, 139.7635, 442.1004, 310.0994,   0.7138,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[190.2782, 117.7031, 244.0126, 166.0068],\n",
      "        [338.9816, 224.9315, 206.2376, 170.3358]])\n",
      "xywhn: tensor([[0.3964, 0.2452, 0.5084, 0.3458],\n",
      "        [0.7062, 0.4686, 0.4297, 0.3549]])\n",
      "xyxy: tensor([[ 68.2719,  34.6997, 312.2845, 200.7065],\n",
      "        [235.8628, 139.7635, 442.1004, 310.0994]])\n",
      "xyxyn: tensor([[0.1422, 0.0723, 0.6506, 0.4181],\n",
      "        [0.4914, 0.2912, 0.9210, 0.6460]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151456_Pinterest_jpg.rf.198c0f50d122e7083e8cda97754ce2c5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8293])\n",
      "data: tensor([[  0.0000,  49.3564, 480.0000, 203.0601,   0.8293,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[240.0000, 126.2083, 480.0000, 153.7036]])\n",
      "xywhn: tensor([[0.5000, 0.2629, 1.0000, 0.3202]])\n",
      "xyxy: tensor([[  0.0000,  49.3564, 480.0000, 203.0601]])\n",
      "xyxyn: tensor([[0.0000, 0.1028, 1.0000, 0.4230]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151310_Pinterest_jpg.rf.b8a04bf82ee4459c66d86ec92e0c2170.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0.])\n",
      "conf: tensor([0.8449, 0.7034, 0.4685, 0.2732])\n",
      "data: tensor([[3.8421e+01, 6.6040e+01, 2.5080e+02, 2.1736e+02, 8.4485e-01, 0.0000e+00],\n",
      "        [2.3354e+02, 3.1479e+02, 4.3099e+02, 4.2213e+02, 7.0335e-01, 0.0000e+00],\n",
      "        [2.6986e+02, 1.2191e+02, 4.7648e+02, 1.9938e+02, 4.6847e-01, 0.0000e+00],\n",
      "        [2.4595e+02, 2.1890e+02, 4.0996e+02, 2.8856e+02, 2.7322e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[144.6095, 141.7020, 212.3763, 151.3245],\n",
      "        [332.2659, 368.4590, 197.4496, 107.3322],\n",
      "        [373.1744, 160.6472, 206.6202,  77.4693],\n",
      "        [327.9584, 253.7316, 164.0079,  69.6535]])\n",
      "xywhn: tensor([[0.3013, 0.2952, 0.4425, 0.3153],\n",
      "        [0.6922, 0.7676, 0.4114, 0.2236],\n",
      "        [0.7774, 0.3347, 0.4305, 0.1614],\n",
      "        [0.6832, 0.5286, 0.3417, 0.1451]])\n",
      "xyxy: tensor([[ 38.4213,  66.0398, 250.7976, 217.3643],\n",
      "        [233.5411, 314.7930, 430.9907, 422.1251],\n",
      "        [269.8643, 121.9126, 476.4846, 199.3818],\n",
      "        [245.9545, 218.9048, 409.9624, 288.5583]])\n",
      "xyxyn: tensor([[0.0800, 0.1376, 0.5225, 0.4528],\n",
      "        [0.4865, 0.6558, 0.8979, 0.8794],\n",
      "        [0.5622, 0.2540, 0.9927, 0.4154],\n",
      "        [0.5124, 0.4561, 0.8541, 0.6012]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104736_Pinterest_jpg.rf.dce3b7e0254b4031c655d59e2e9dc545.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.7735, 0.5687, 0.5679, 0.5398, 0.5253, 0.5150, 0.2657])\n",
      "data: tensor([[3.5550e+01, 1.0453e+02, 2.0130e+02, 2.3235e+02, 7.7350e-01, 0.0000e+00],\n",
      "        [3.3875e+02, 2.9568e+02, 4.6471e+02, 3.6315e+02, 5.6870e-01, 0.0000e+00],\n",
      "        [4.8036e+01, 2.7619e+02, 1.8376e+02, 3.8398e+02, 5.6786e-01, 0.0000e+00],\n",
      "        [2.6889e+02, 3.3833e+02, 3.8575e+02, 4.1056e+02, 5.3976e-01, 0.0000e+00],\n",
      "        [2.7975e+02, 1.2872e+01, 4.1827e+02, 9.3284e+01, 5.2529e-01, 0.0000e+00],\n",
      "        [3.2080e+02, 1.6200e+02, 4.5544e+02, 2.4104e+02, 5.1498e-01, 0.0000e+00],\n",
      "        [2.5415e+02, 1.6444e+02, 3.4698e+02, 2.2859e+02, 2.6574e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[118.4249, 168.4391, 165.7492, 127.8128],\n",
      "        [401.7347, 329.4169, 125.9601,  67.4758],\n",
      "        [115.8987, 330.0883, 135.7251, 107.7927],\n",
      "        [327.3193, 374.4445, 116.8531,  72.2236],\n",
      "        [349.0052,  53.0778, 138.5195,  80.4123],\n",
      "        [388.1230, 201.5220, 134.6401,  79.0369],\n",
      "        [300.5688, 196.5146,  92.8285,  64.1576]])\n",
      "xywhn: tensor([[0.2467, 0.3509, 0.3453, 0.2663],\n",
      "        [0.8369, 0.6863, 0.2624, 0.1406],\n",
      "        [0.2415, 0.6877, 0.2828, 0.2246],\n",
      "        [0.6819, 0.7801, 0.2434, 0.1505],\n",
      "        [0.7271, 0.1106, 0.2886, 0.1675],\n",
      "        [0.8086, 0.4198, 0.2805, 0.1647],\n",
      "        [0.6262, 0.4094, 0.1934, 0.1337]])\n",
      "xyxy: tensor([[ 35.5503, 104.5327, 201.2995, 232.3455],\n",
      "        [338.7547, 295.6790, 464.7148, 363.1548],\n",
      "        [ 48.0361, 276.1919, 183.7612, 383.9846],\n",
      "        [268.8928, 338.3327, 385.7459, 410.5563],\n",
      "        [279.7455,  12.8717, 418.2650,  93.2840],\n",
      "        [320.8030, 162.0035, 455.4431, 241.0405],\n",
      "        [254.1546, 164.4358, 346.9831, 228.5934]])\n",
      "xyxyn: tensor([[0.0741, 0.2178, 0.4194, 0.4841],\n",
      "        [0.7057, 0.6160, 0.9682, 0.7566],\n",
      "        [0.1001, 0.5754, 0.3828, 0.8000],\n",
      "        [0.5602, 0.7049, 0.8036, 0.8553],\n",
      "        [0.5828, 0.0268, 0.8714, 0.1943],\n",
      "        [0.6683, 0.3375, 0.9488, 0.5022],\n",
      "        [0.5295, 0.3426, 0.7229, 0.4762]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130241_jpg.rf.c58ff21c0649e4340851bd63156bf94a.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1.])\n",
      "conf: tensor([0.7858, 0.5008, 0.4186])\n",
      "data: tensor([[176.3808, 114.4276, 223.9719, 227.1284,   0.7858,   1.0000],\n",
      "        [140.5889, 114.8663, 223.2562, 226.3879,   0.5008,   1.0000],\n",
      "        [281.1490, 203.1340, 364.2537, 301.1177,   0.4186,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[200.1763, 170.7780,  47.5912, 112.7008],\n",
      "        [181.9225, 170.6271,  82.6673, 111.5216],\n",
      "        [322.7014, 252.1259,  83.1046,  97.9836]])\n",
      "xywhn: tensor([[0.4170, 0.3558, 0.0991, 0.2348],\n",
      "        [0.3790, 0.3555, 0.1722, 0.2323],\n",
      "        [0.6723, 0.5253, 0.1731, 0.2041]])\n",
      "xyxy: tensor([[176.3808, 114.4276, 223.9719, 227.1284],\n",
      "        [140.5889, 114.8663, 223.2562, 226.3879],\n",
      "        [281.1490, 203.1340, 364.2537, 301.1177]])\n",
      "xyxyn: tensor([[0.3675, 0.2384, 0.4666, 0.4732],\n",
      "        [0.2929, 0.2393, 0.4651, 0.4716],\n",
      "        [0.5857, 0.4232, 0.7589, 0.6273]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/carvedilol-3-125-MG-7-Copy-Copy_jpg.rf.f224a03f1268e2e7cba8bb1e0c61ef88.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9194, 0.8900])\n",
      "data: tensor([[  0.8429,  87.1192, 201.2350, 381.4908,   0.9194,   2.0000],\n",
      "        [193.3499,  85.8247, 389.9301, 378.5526,   0.8900,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[101.0390, 234.3050, 200.3922, 294.3716],\n",
      "        [291.6400, 232.1887, 196.5802, 292.7279]])\n",
      "xywhn: tensor([[0.2105, 0.4881, 0.4175, 0.6133],\n",
      "        [0.6076, 0.4837, 0.4095, 0.6098]])\n",
      "xyxy: tensor([[  0.8429,  87.1192, 201.2350, 381.4908],\n",
      "        [193.3499,  85.8247, 389.9301, 378.5526]])\n",
      "xyxyn: tensor([[0.0018, 0.1815, 0.4192, 0.7948],\n",
      "        [0.4028, 0.1788, 0.8124, 0.7887]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100534_Pinterest_jpg.rf.adf2b6f4f6f943ebb52e3bfd31669b27.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8705])\n",
      "data: tensor([[141.1859,  31.4924, 404.3459, 192.0454,   0.8705,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[272.7659, 111.7689, 263.1600, 160.5530]])\n",
      "xywhn: tensor([[0.5683, 0.2329, 0.5483, 0.3345]])\n",
      "xyxy: tensor([[141.1859,  31.4924, 404.3459, 192.0454]])\n",
      "xyxyn: tensor([[0.2941, 0.0656, 0.8424, 0.4001]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100750_Pinterest_jpg.rf.4cb27e5a068f39a818e36ce67cdcb15e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8542, 0.7996])\n",
      "data: tensor([[ 18.4925,  24.8087, 233.8274, 158.2686,   0.8542,   0.0000],\n",
      "        [228.1103,  20.0028, 428.6506, 150.7771,   0.7996,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[126.1599,  91.5387, 215.3349, 133.4599],\n",
      "        [328.3804,  85.3899, 200.5403, 130.7743]])\n",
      "xywhn: tensor([[0.2628, 0.1907, 0.4486, 0.2780],\n",
      "        [0.6841, 0.1779, 0.4178, 0.2724]])\n",
      "xyxy: tensor([[ 18.4925,  24.8087, 233.8274, 158.2686],\n",
      "        [228.1103,  20.0028, 428.6506, 150.7771]])\n",
      "xyxyn: tensor([[0.0385, 0.0517, 0.4871, 0.3297],\n",
      "        [0.4752, 0.0417, 0.8930, 0.3141]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151302_Pinterest_jpg.rf.4556a63d2d343c8594614cabbe2ee6c9.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.6618, 0.6143, 0.5656, 0.5590, 0.5320, 0.5048, 0.2531])\n",
      "data: tensor([[7.9472e+01, 1.3348e+02, 2.2827e+02, 1.9405e+02, 6.6182e-01, 0.0000e+00],\n",
      "        [1.3819e+01, 1.4559e+02, 1.1874e+02, 2.1029e+02, 6.1427e-01, 0.0000e+00],\n",
      "        [1.6855e+01, 5.3116e+01, 1.9656e+02, 1.1524e+02, 5.6562e-01, 0.0000e+00],\n",
      "        [3.0045e+02, 2.3079e+02, 4.1816e+02, 3.3908e+02, 5.5901e-01, 0.0000e+00],\n",
      "        [2.7539e+01, 2.3909e+02, 1.7387e+02, 3.2661e+02, 5.3204e-01, 0.0000e+00],\n",
      "        [2.4884e+02, 8.9403e+01, 4.2198e+02, 1.8092e+02, 5.0481e-01, 0.0000e+00],\n",
      "        [3.7273e+02, 1.4499e+02, 4.5538e+02, 1.8436e+02, 2.5305e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[153.8720, 163.7637, 148.7991,  60.5752],\n",
      "        [ 66.2796, 177.9411, 104.9204,  64.6933],\n",
      "        [106.7066,  84.1760, 179.7029,  62.1194],\n",
      "        [359.3022, 284.9383, 117.7108, 108.2912],\n",
      "        [100.7033, 282.8485, 146.3295,  87.5156],\n",
      "        [335.4051, 135.1621, 173.1399,  91.5173],\n",
      "        [414.0536, 164.6732,  82.6533,  39.3661]])\n",
      "xywhn: tensor([[0.3206, 0.3412, 0.3100, 0.1262],\n",
      "        [0.1381, 0.3707, 0.2186, 0.1348],\n",
      "        [0.2223, 0.1754, 0.3744, 0.1294],\n",
      "        [0.7485, 0.5936, 0.2452, 0.2256],\n",
      "        [0.2098, 0.5893, 0.3049, 0.1823],\n",
      "        [0.6988, 0.2816, 0.3607, 0.1907],\n",
      "        [0.8626, 0.3431, 0.1722, 0.0820]])\n",
      "xyxy: tensor([[ 79.4725, 133.4761, 228.2716, 194.0514],\n",
      "        [ 13.8194, 145.5944, 118.7398, 210.2877],\n",
      "        [ 16.8551,  53.1163, 196.5580, 115.2357],\n",
      "        [300.4468, 230.7927, 418.1575, 339.0839],\n",
      "        [ 27.5385, 239.0907, 173.8680, 326.6064],\n",
      "        [248.8352,  89.4035, 421.9751, 180.9208],\n",
      "        [372.7270, 144.9902, 455.3802, 184.3563]])\n",
      "xyxyn: tensor([[0.1656, 0.2781, 0.4756, 0.4043],\n",
      "        [0.0288, 0.3033, 0.2474, 0.4381],\n",
      "        [0.0351, 0.1107, 0.4095, 0.2401],\n",
      "        [0.6259, 0.4808, 0.8712, 0.7064],\n",
      "        [0.0574, 0.4981, 0.3622, 0.6804],\n",
      "        [0.5184, 0.1863, 0.8791, 0.3769],\n",
      "        [0.7765, 0.3021, 0.9487, 0.3841]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151657_Pinterest_jpg.rf.2566b4ab8664fe4b6a5b74df2cd500dd.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 3., 3., 3., 3., 3., 3.])\n",
      "conf: tensor([0.8730, 0.7946, 0.7767, 0.6533, 0.5399, 0.4861, 0.2729])\n",
      "data: tensor([[9.7686e+00, 2.7282e+02, 2.3776e+02, 3.4732e+02, 8.7303e-01, 3.0000e+00],\n",
      "        [2.2546e+01, 1.2334e+02, 2.2591e+02, 1.8993e+02, 7.9456e-01, 3.0000e+00],\n",
      "        [2.4466e+02, 1.9831e+01, 4.1725e+02, 9.1470e+01, 7.7673e-01, 3.0000e+00],\n",
      "        [2.6249e+02, 3.0379e+02, 4.4878e+02, 3.6224e+02, 6.5332e-01, 3.0000e+00],\n",
      "        [2.4741e+02, 1.7032e+02, 4.3874e+02, 2.2721e+02, 5.3991e-01, 3.0000e+00],\n",
      "        [1.3275e+01, 1.6520e+01, 2.2255e+02, 8.1883e+01, 4.8613e-01, 3.0000e+00],\n",
      "        [3.7740e+02, 6.0942e+01, 4.5121e+02, 1.0564e+02, 2.7295e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[123.7634, 310.0684, 227.9896,  74.5061],\n",
      "        [124.2261, 156.6354, 203.3603,  66.5935],\n",
      "        [330.9554,  55.6505, 172.5946,  71.6389],\n",
      "        [355.6346, 333.0175, 186.2958,  58.4464],\n",
      "        [343.0774, 198.7642, 191.3328,  56.8983],\n",
      "        [117.9140,  49.2017, 209.2786,  65.3624],\n",
      "        [414.3080,  83.2893,  73.8123,  44.6936]])\n",
      "xywhn: tensor([[0.2578, 0.6460, 0.4750, 0.1552],\n",
      "        [0.2588, 0.3263, 0.4237, 0.1387],\n",
      "        [0.6895, 0.1159, 0.3596, 0.1492],\n",
      "        [0.7409, 0.6938, 0.3881, 0.1218],\n",
      "        [0.7147, 0.4141, 0.3986, 0.1185],\n",
      "        [0.2457, 0.1025, 0.4360, 0.1362],\n",
      "        [0.8631, 0.1735, 0.1538, 0.0931]])\n",
      "xyxy: tensor([[  9.7686, 272.8153, 237.7581, 347.3214],\n",
      "        [ 22.5459, 123.3386, 225.9063, 189.9321],\n",
      "        [244.6581,  19.8311, 417.2527,  91.4700],\n",
      "        [262.4867, 303.7943, 448.7825, 362.2407],\n",
      "        [247.4110, 170.3151, 438.7438, 227.2134],\n",
      "        [ 13.2747,  16.5204, 222.5533,  81.8829],\n",
      "        [377.4019,  60.9424, 451.2141, 105.6361]])\n",
      "xyxyn: tensor([[0.0204, 0.5684, 0.4953, 0.7236],\n",
      "        [0.0470, 0.2570, 0.4706, 0.3957],\n",
      "        [0.5097, 0.0413, 0.8693, 0.1906],\n",
      "        [0.5468, 0.6329, 0.9350, 0.7547],\n",
      "        [0.5154, 0.3548, 0.9140, 0.4734],\n",
      "        [0.0277, 0.0344, 0.4637, 0.1706],\n",
      "        [0.7863, 0.1270, 0.9400, 0.2201]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/celecoxib-200-MG-11-Copy-Copy_jpg.rf.15a92bfbd3268f93ab421e286e008d5a.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9322, 0.9288])\n",
      "data: tensor([[9.5348e+00, 1.9114e+02, 4.7998e+02, 4.0723e+02, 9.3222e-01, 2.0000e+00],\n",
      "        [5.0987e+00, 2.9568e-01, 4.7914e+02, 1.8592e+02, 9.2880e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[244.7581, 299.1882, 470.4466, 216.0931],\n",
      "        [242.1185,  93.1064, 474.0395, 185.6215]])\n",
      "xywhn: tensor([[0.5099, 0.6233, 0.9801, 0.4502],\n",
      "        [0.5044, 0.1940, 0.9876, 0.3867]])\n",
      "xyxy: tensor([[9.5348e+00, 1.9114e+02, 4.7998e+02, 4.0723e+02],\n",
      "        [5.0987e+00, 2.9568e-01, 4.7914e+02, 1.8592e+02]])\n",
      "xyxyn: tensor([[1.9864e-02, 3.9821e-01, 9.9996e-01, 8.4841e-01],\n",
      "        [1.0622e-02, 6.1601e-04, 9.9820e-01, 3.8733e-01]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240522_210046_SHEIN_jpg.rf.f75e6fd9c68f57eedb85d63572d32117.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2.])\n",
      "conf: tensor([0.3570])\n",
      "data: tensor([[4.8954e+01, 7.3188e+01, 4.1340e+02, 3.9288e+02, 3.5699e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[231.1750, 233.0347, 364.4427, 319.6937]])\n",
      "xywhn: tensor([[0.4816, 0.4855, 0.7593, 0.6660]])\n",
      "xyxy: tensor([[ 48.9537,  73.1879, 413.3964, 392.8816]])\n",
      "xyxyn: tensor([[0.1020, 0.1525, 0.8612, 0.8185]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/medicine-4-_jpg.rf.20b9cb609c87be6f6dff2e78af16926d.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2., 2.])\n",
      "conf: tensor([0.7845, 0.7572, 0.7278])\n",
      "data: tensor([[191.4315, 218.7152, 282.9088, 320.1214,   0.7845,   2.0000],\n",
      "        [213.4989,  95.4912, 327.7713, 197.1914,   0.7572,   2.0000],\n",
      "        [112.7292, 245.9978, 214.2588, 354.2511,   0.7278,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[237.1702, 269.4183,  91.4774, 101.4062],\n",
      "        [270.6351, 146.3413, 114.2724, 101.7002],\n",
      "        [163.4940, 300.1245, 101.5296, 108.2533]])\n",
      "xywhn: tensor([[0.4941, 0.5613, 0.1906, 0.2113],\n",
      "        [0.5638, 0.3049, 0.2381, 0.2119],\n",
      "        [0.3406, 0.6253, 0.2115, 0.2255]])\n",
      "xyxy: tensor([[191.4315, 218.7152, 282.9088, 320.1214],\n",
      "        [213.4989,  95.4912, 327.7713, 197.1914],\n",
      "        [112.7292, 245.9978, 214.2588, 354.2511]])\n",
      "xyxyn: tensor([[0.3988, 0.4557, 0.5894, 0.6669],\n",
      "        [0.4448, 0.1989, 0.6829, 0.4108],\n",
      "        [0.2349, 0.5125, 0.4464, 0.7380]])\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "model = YOLO('runs/detect/train/weights/best.pt')  # Path to your trained model\n",
    "\n",
    "# Path to the test images\n",
    "test_images_path = '/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images'\n",
    "\n",
    "# Get all test images\n",
    "test_images = [os.path.join(test_images_path, img) for img in os.listdir(test_images_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Run inference on the test set\n",
    "results = model.predict(test_images, save=True, save_txt=True, save_conf=True, save_crop=False)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"Image: {result.path}\")\n",
    "    print(f\"Predictions: {result.boxes}\")\n",
    "\n",
    "# Results will be saved in the 'runs' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@329.346] global /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_f6tvh9615u/croot/opencv-suite_1691620375715/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 288x480 (no detections), 76.0ms\n",
      "Speed: 3.1ms preprocess, 76.0ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 46.3ms\n",
      "Speed: 1.1ms preprocess, 46.3ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 47.6ms\n",
      "Speed: 1.1ms preprocess, 47.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.0ms\n",
      "Speed: 1.6ms preprocess, 45.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 0.9ms preprocess, 38.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.0ms\n",
      "Speed: 1.3ms preprocess, 44.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.5ms\n",
      "Speed: 1.2ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.8ms\n",
      "Speed: 1.3ms preprocess, 39.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.0ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.9ms\n",
      "Speed: 2.1ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.9ms\n",
      "Speed: 1.0ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.2ms\n",
      "Speed: 1.3ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 62.5ms\n",
      "Speed: 1.2ms preprocess, 62.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.6ms\n",
      "Speed: 1.3ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.3ms\n",
      "Speed: 1.3ms preprocess, 39.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.4ms\n",
      "Speed: 1.1ms preprocess, 39.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 36.8ms\n",
      "Speed: 1.0ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.7ms\n",
      "Speed: 1.4ms preprocess, 38.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.6ms\n",
      "Speed: 1.1ms preprocess, 37.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 36.7ms\n",
      "Speed: 1.1ms preprocess, 36.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.6ms\n",
      "Speed: 1.3ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.2ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.2ms\n",
      "Speed: 1.3ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.4ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.6ms\n",
      "Speed: 1.1ms preprocess, 42.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.3ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 56.9ms\n",
      "Speed: 1.0ms preprocess, 56.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 101.4ms\n",
      "Speed: 2.7ms preprocess, 101.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.5ms\n",
      "Speed: 1.0ms preprocess, 41.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.0ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.1ms preprocess, 39.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.9ms\n",
      "Speed: 1.2ms preprocess, 37.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 36.9ms\n",
      "Speed: 1.0ms preprocess, 36.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.3ms\n",
      "Speed: 1.1ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.6ms\n",
      "Speed: 1.3ms preprocess, 38.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.2ms\n",
      "Speed: 1.3ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.2ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.5ms\n",
      "Speed: 1.1ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.6ms\n",
      "Speed: 1.1ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 36.8ms\n",
      "Speed: 1.1ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 1.2ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 61.5ms\n",
      "Speed: 1.0ms preprocess, 61.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 54.6ms\n",
      "Speed: 1.1ms preprocess, 54.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 59.9ms\n",
      "Speed: 1.7ms preprocess, 59.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 47.7ms\n",
      "Speed: 1.0ms preprocess, 47.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.9ms\n",
      "Speed: 10.7ms preprocess, 45.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.0ms\n",
      "Speed: 1.2ms preprocess, 39.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.9ms\n",
      "Speed: 1.2ms preprocess, 36.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.8ms\n",
      "Speed: 1.2ms preprocess, 36.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 75.0ms\n",
      "Speed: 1.1ms preprocess, 75.0ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 43.4ms\n",
      "Speed: 1.4ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 59.2ms\n",
      "Speed: 1.2ms preprocess, 59.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.6ms\n",
      "Speed: 1.3ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.7ms\n",
      "Speed: 1.1ms preprocess, 40.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.1ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 45.6ms\n",
      "Speed: 1.1ms preprocess, 45.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.6ms\n",
      "Speed: 1.2ms preprocess, 41.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.2ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.1ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.8ms\n",
      "Speed: 1.3ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.6ms\n",
      "Speed: 1.1ms preprocess, 40.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 0.9ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.1ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.7ms\n",
      "Speed: 1.4ms preprocess, 43.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 40.8ms\n",
      "Speed: 1.1ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.4ms\n",
      "Speed: 1.2ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.3ms\n",
      "Speed: 1.3ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.1ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.2ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 0.9ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.6ms\n",
      "Speed: 1.2ms preprocess, 43.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 60.5ms\n",
      "Speed: 1.0ms preprocess, 60.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.3ms\n",
      "Speed: 1.1ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.8ms\n",
      "Speed: 1.1ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 45.8ms\n",
      "Speed: 1.3ms preprocess, 45.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 42.1ms\n",
      "Speed: 1.2ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.6ms\n",
      "Speed: 1.2ms preprocess, 39.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.3ms\n",
      "Speed: 1.1ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 41.1ms\n",
      "Speed: 1.2ms preprocess, 41.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 40.4ms\n",
      "Speed: 1.2ms preprocess, 40.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 36.6ms\n",
      "Speed: 1.2ms preprocess, 36.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.2ms\n",
      "Speed: 1.3ms preprocess, 38.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.0ms\n",
      "Speed: 1.3ms preprocess, 41.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.8ms\n",
      "Speed: 1.2ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.2ms\n",
      "Speed: 1.0ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.3ms\n",
      "Speed: 1.4ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 45.9ms\n",
      "Speed: 1.0ms preprocess, 45.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.3ms\n",
      "Speed: 1.2ms preprocess, 40.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.4ms\n",
      "Speed: 1.2ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.4ms\n",
      "Speed: 1.2ms preprocess, 40.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.3ms\n",
      "Speed: 1.0ms preprocess, 37.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.2ms\n",
      "Speed: 1.1ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.9ms\n",
      "Speed: 0.9ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.1ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.0ms\n",
      "Speed: 1.3ms preprocess, 43.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 36.9ms\n",
      "Speed: 1.1ms preprocess, 36.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.0ms\n",
      "Speed: 1.0ms preprocess, 39.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.0ms\n",
      "Speed: 1.4ms preprocess, 41.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.1ms\n",
      "Speed: 1.2ms preprocess, 41.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.2ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.0ms\n",
      "Speed: 1.3ms preprocess, 40.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.2ms\n",
      "Speed: 1.1ms preprocess, 37.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.0ms\n",
      "Speed: 1.1ms preprocess, 40.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.3ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.2ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.2ms\n",
      "Speed: 1.4ms preprocess, 43.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.2ms\n",
      "Speed: 1.2ms preprocess, 44.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.3ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.2ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.5ms\n",
      "Speed: 1.2ms preprocess, 36.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.3ms\n",
      "Speed: 1.2ms preprocess, 40.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.9ms\n",
      "Speed: 1.3ms preprocess, 36.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 47.6ms\n",
      "Speed: 1.5ms preprocess, 47.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.2ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.6ms\n",
      "Speed: 1.1ms preprocess, 36.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.1ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.3ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.0ms\n",
      "Speed: 1.3ms preprocess, 39.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.1ms\n",
      "Speed: 1.2ms preprocess, 41.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.3ms\n",
      "Speed: 1.1ms preprocess, 40.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 44.3ms\n",
      "Speed: 1.1ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 37.1ms\n",
      "Speed: 1.2ms preprocess, 37.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.5ms\n",
      "Speed: 1.3ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.8ms\n",
      "Speed: 1.1ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.5ms\n",
      "Speed: 1.1ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 45.4ms\n",
      "Speed: 1.4ms preprocess, 45.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.8ms\n",
      "Speed: 1.0ms preprocess, 39.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 37.4ms\n",
      "Speed: 1.0ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 36.8ms\n",
      "Speed: 1.1ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 47.2ms\n",
      "Speed: 1.2ms preprocess, 47.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.5ms\n",
      "Speed: 1.2ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 36.9ms\n",
      "Speed: 1.2ms preprocess, 36.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.9ms\n",
      "Speed: 1.1ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.3ms\n",
      "Speed: 1.3ms preprocess, 41.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.8ms\n",
      "Speed: 1.3ms preprocess, 44.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.3ms\n",
      "Speed: 1.2ms preprocess, 40.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.5ms\n",
      "Speed: 1.0ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 58.3ms\n",
      "Speed: 1.1ms preprocess, 58.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 36.9ms\n",
      "Speed: 1.0ms preprocess, 36.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.2ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 0.9ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.8ms\n",
      "Speed: 1.4ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.0ms preprocess, 39.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.0ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.0ms\n",
      "Speed: 1.2ms preprocess, 40.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.3ms\n",
      "Speed: 1.0ms preprocess, 38.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.2ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.0ms\n",
      "Speed: 1.3ms preprocess, 45.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.8ms\n",
      "Speed: 1.3ms preprocess, 39.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.9ms\n",
      "Speed: 1.0ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.6ms\n",
      "Speed: 1.0ms preprocess, 43.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 59.1ms\n",
      "Speed: 1.3ms preprocess, 59.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.6ms\n",
      "Speed: 1.2ms preprocess, 41.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.4ms\n",
      "Speed: 1.0ms preprocess, 44.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.0ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 1.1ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.2ms preprocess, 38.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.0ms\n",
      "Speed: 1.3ms preprocess, 42.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.3ms preprocess, 40.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.3ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.2ms\n",
      "Speed: 1.2ms preprocess, 42.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.1ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.7ms\n",
      "Speed: 1.2ms preprocess, 36.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.9ms\n",
      "Speed: 1.0ms preprocess, 36.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.3ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.1ms\n",
      "Speed: 1.3ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.1ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.2ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.6ms\n",
      "Speed: 1.0ms preprocess, 40.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.9ms\n",
      "Speed: 1.0ms preprocess, 36.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.5ms\n",
      "Speed: 1.4ms preprocess, 41.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.2ms\n",
      "Speed: 1.1ms preprocess, 44.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.1ms\n",
      "Speed: 1.2ms preprocess, 42.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.5ms\n",
      "Speed: 1.4ms preprocess, 42.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.2ms\n",
      "Speed: 1.1ms preprocess, 42.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.2ms\n",
      "Speed: 1.0ms preprocess, 39.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.4ms\n",
      "Speed: 1.3ms preprocess, 44.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.1ms\n",
      "Speed: 1.0ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.8ms\n",
      "Speed: 1.2ms preprocess, 36.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.3ms\n",
      "Speed: 1.3ms preprocess, 45.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.6ms\n",
      "Speed: 1.1ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.4ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.6ms\n",
      "Speed: 1.3ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.3ms\n",
      "Speed: 1.1ms preprocess, 41.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.3ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.9ms\n",
      "Speed: 1.1ms preprocess, 40.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.2ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.3ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.2ms preprocess, 39.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.4ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.2ms\n",
      "Speed: 1.3ms preprocess, 40.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.9ms\n",
      "Speed: 1.1ms preprocess, 38.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.1ms preprocess, 39.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 1.3ms preprocess, 38.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.1ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 69.7ms\n",
      "Speed: 1.3ms preprocess, 69.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 84.7ms\n",
      "Speed: 1.4ms preprocess, 84.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 113.4ms\n",
      "Speed: 1.6ms preprocess, 113.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 139.0ms\n",
      "Speed: 1.7ms preprocess, 139.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 178.8ms\n",
      "Speed: 1.3ms preprocess, 178.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 39.0ms\n",
      "Speed: 1.1ms preprocess, 39.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.8ms\n",
      "Speed: 1.0ms preprocess, 40.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.8ms\n",
      "Speed: 1.0ms preprocess, 38.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.9ms\n",
      "Speed: 1.4ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 88.4ms\n",
      "Speed: 1.3ms preprocess, 88.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 76.6ms\n",
      "Speed: 1.5ms preprocess, 76.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.3ms\n",
      "Speed: 1.3ms preprocess, 42.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.2ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.5ms preprocess, 40.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.9ms\n",
      "Speed: 1.0ms preprocess, 37.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 61.0ms\n",
      "Speed: 1.0ms preprocess, 61.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 47.7ms\n",
      "Speed: 1.3ms preprocess, 47.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.5ms\n",
      "Speed: 1.3ms preprocess, 42.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.1ms\n",
      "Speed: 1.5ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.2ms preprocess, 40.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.6ms\n",
      "Speed: 1.1ms preprocess, 44.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 48.0ms\n",
      "Speed: 1.0ms preprocess, 48.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.2ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.6ms\n",
      "Speed: 1.4ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.3ms\n",
      "Speed: 1.0ms preprocess, 38.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 53.1ms\n",
      "Speed: 10.0ms preprocess, 53.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 1.1ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 42.1ms\n",
      "Speed: 1.3ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.2ms preprocess, 39.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.6ms\n",
      "Speed: 1.1ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.1ms\n",
      "Speed: 1.4ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.2ms\n",
      "Speed: 1.3ms preprocess, 43.2ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.6ms\n",
      "Speed: 1.0ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 36.9ms\n",
      "Speed: 1.3ms preprocess, 36.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.7ms\n",
      "Speed: 1.3ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 59.5ms\n",
      "Speed: 1.3ms preprocess, 59.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.2ms\n",
      "Speed: 1.1ms preprocess, 37.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 44.5ms\n",
      "Speed: 1.4ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.5ms\n",
      "Speed: 1.1ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.3ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.4ms\n",
      "Speed: 1.3ms preprocess, 43.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.5ms\n",
      "Speed: 1.0ms preprocess, 39.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 47.9ms\n",
      "Speed: 1.3ms preprocess, 47.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.0ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.9ms\n",
      "Speed: 1.0ms preprocess, 37.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.4ms\n",
      "Speed: 1.3ms preprocess, 45.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.9ms\n",
      "Speed: 1.1ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.0ms\n",
      "Speed: 1.1ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.8ms\n",
      "Speed: 1.3ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.0ms\n",
      "Speed: 1.3ms preprocess, 41.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.0ms\n",
      "Speed: 1.2ms preprocess, 41.0ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 36.8ms\n",
      "Speed: 1.1ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.9ms\n",
      "Speed: 1.1ms preprocess, 38.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.7ms\n",
      "Speed: 1.0ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.2ms\n",
      "Speed: 1.1ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 42.1ms\n",
      "Speed: 1.3ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.7ms\n",
      "Speed: 2.2ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.3ms\n",
      "Speed: 1.2ms preprocess, 39.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 36.8ms\n",
      "Speed: 1.1ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.1ms\n",
      "Speed: 1.1ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.4ms\n",
      "Speed: 1.0ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.8ms\n",
      "Speed: 1.0ms preprocess, 39.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.2ms\n",
      "Speed: 1.0ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.0ms\n",
      "Speed: 1.0ms preprocess, 40.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.7ms\n",
      "Speed: 1.3ms preprocess, 41.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.3ms\n",
      "Speed: 1.2ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 57.4ms\n",
      "Speed: 1.1ms preprocess, 57.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.3ms\n",
      "Speed: 1.2ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.0ms\n",
      "Speed: 1.2ms preprocess, 40.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 42.8ms\n",
      "Speed: 1.3ms preprocess, 42.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.0ms\n",
      "Speed: 1.2ms preprocess, 39.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.3ms\n",
      "Speed: 1.3ms preprocess, 37.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.8ms\n",
      "Speed: 1.3ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 62.5ms\n",
      "Speed: 1.4ms preprocess, 62.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 44.6ms\n",
      "Speed: 1.3ms preprocess, 44.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.7ms\n",
      "Speed: 1.3ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 46.1ms\n",
      "Speed: 1.2ms preprocess, 46.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.9ms\n",
      "Speed: 1.3ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 46.5ms\n",
      "Speed: 1.2ms preprocess, 46.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 42.1ms\n",
      "Speed: 1.1ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 45.0ms\n",
      "Speed: 1.3ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.6ms\n",
      "Speed: 1.3ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.9ms\n",
      "Speed: 1.1ms preprocess, 38.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.3ms\n",
      "Speed: 1.3ms preprocess, 43.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.2ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 46.4ms\n",
      "Speed: 1.3ms preprocess, 46.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.2ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.4ms\n",
      "Speed: 1.3ms preprocess, 45.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.9ms\n",
      "Speed: 1.2ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.5ms\n",
      "Speed: 1.0ms preprocess, 43.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 52.5ms\n",
      "Speed: 1.3ms preprocess, 52.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.7ms\n",
      "Speed: 1.3ms preprocess, 43.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.9ms preprocess, 38.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.2ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.7ms\n",
      "Speed: 1.2ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 47.4ms\n",
      "Speed: 1.1ms preprocess, 47.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.1ms\n",
      "Speed: 1.2ms preprocess, 38.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.3ms\n",
      "Speed: 1.1ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.0ms\n",
      "Speed: 1.1ms preprocess, 40.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.3ms\n",
      "Speed: 1.5ms preprocess, 41.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.1ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 44.5ms\n",
      "Speed: 1.2ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.4ms\n",
      "Speed: 1.4ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.2ms\n",
      "Speed: 1.0ms preprocess, 41.2ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 49.5ms\n",
      "Speed: 1.1ms preprocess, 49.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.1ms\n",
      "Speed: 1.1ms preprocess, 39.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 45.8ms\n",
      "Speed: 1.5ms preprocess, 45.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 38.5ms\n",
      "Speed: 1.2ms preprocess, 38.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 45.1ms\n",
      "Speed: 1.3ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 39.4ms\n",
      "Speed: 1.1ms preprocess, 39.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.1ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 37.1ms\n",
      "Speed: 1.1ms preprocess, 37.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 44.0ms\n",
      "Speed: 1.3ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.5ms\n",
      "Speed: 1.2ms preprocess, 38.5ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 44.4ms\n",
      "Speed: 1.3ms preprocess, 44.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.4ms\n",
      "Speed: 1.3ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.1ms\n",
      "Speed: 1.1ms preprocess, 37.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.5ms\n",
      "Speed: 1.2ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 74.5ms\n",
      "Speed: 2.4ms preprocess, 74.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.3ms\n",
      "Speed: 1.3ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 45.5ms\n",
      "Speed: 1.0ms preprocess, 45.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.1ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.4ms\n",
      "Speed: 1.3ms preprocess, 39.4ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 42.2ms\n",
      "Speed: 1.3ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.1ms\n",
      "Speed: 1.3ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.0ms preprocess, 38.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 45.1ms\n",
      "Speed: 1.4ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.3ms\n",
      "Speed: 1.0ms preprocess, 40.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.4ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 44.1ms\n",
      "Speed: 1.3ms preprocess, 44.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 57.4ms\n",
      "Speed: 1.1ms preprocess, 57.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.0ms\n",
      "Speed: 1.1ms preprocess, 39.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.7ms\n",
      "Speed: 1.2ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.2ms\n",
      "Speed: 1.3ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.4ms\n",
      "Speed: 1.7ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.5ms\n",
      "Speed: 1.0ms preprocess, 43.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.6ms\n",
      "Speed: 1.3ms preprocess, 40.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.2ms\n",
      "Speed: 1.3ms preprocess, 40.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.2ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.1ms\n",
      "Speed: 1.1ms preprocess, 40.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.4ms\n",
      "Speed: 1.3ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.3ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.0ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.3ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.3ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.2ms\n",
      "Speed: 1.3ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.8ms\n",
      "Speed: 1.4ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.3ms\n",
      "Speed: 1.3ms preprocess, 37.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.3ms preprocess, 39.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.0ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.2ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.3ms\n",
      "Speed: 0.9ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.2ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 60.1ms\n",
      "Speed: 1.0ms preprocess, 60.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.1ms\n",
      "Speed: 1.1ms preprocess, 45.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.5ms\n",
      "Speed: 1.3ms preprocess, 43.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.3ms\n",
      "Speed: 1.2ms preprocess, 43.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.1ms\n",
      "Speed: 1.1ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.2ms\n",
      "Speed: 1.0ms preprocess, 42.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.4ms\n",
      "Speed: 1.1ms preprocess, 40.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.0ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.2ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.2ms\n",
      "Speed: 1.3ms preprocess, 43.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 49.2ms\n",
      "Speed: 1.1ms preprocess, 49.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.9ms\n",
      "Speed: 1.1ms preprocess, 40.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.0ms\n",
      "Speed: 1.4ms preprocess, 42.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.4ms\n",
      "Speed: 1.1ms preprocess, 43.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.6ms\n",
      "Speed: 1.2ms preprocess, 37.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.2ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.6ms\n",
      "Speed: 1.3ms preprocess, 40.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.7ms\n",
      "Speed: 1.0ms preprocess, 42.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.9ms\n",
      "Speed: 1.3ms preprocess, 38.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.1ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 1.2ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 59.8ms\n",
      "Speed: 1.1ms preprocess, 59.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.1ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.0ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 1.5ms preprocess, 38.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.2ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.3ms preprocess, 39.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.2ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.0ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.2ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 59.8ms\n",
      "Speed: 1.1ms preprocess, 59.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.2ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.2ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 49.0ms\n",
      "Speed: 1.3ms preprocess, 49.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.2ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.0ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.7ms\n",
      "Speed: 1.2ms preprocess, 44.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.6ms\n",
      "Speed: 1.3ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.1ms\n",
      "Speed: 1.1ms preprocess, 40.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.1ms\n",
      "Speed: 1.2ms preprocess, 41.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.6ms\n",
      "Speed: 1.6ms preprocess, 44.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.6ms\n",
      "Speed: 1.2ms preprocess, 38.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.2ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.1ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.7ms\n",
      "Speed: 1.6ms preprocess, 45.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.3ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.8ms\n",
      "Speed: 1.1ms preprocess, 41.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.7ms\n",
      "Speed: 1.4ms preprocess, 43.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.1ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.1ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.0ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.3ms\n",
      "Speed: 1.1ms preprocess, 44.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.2ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.1ms\n",
      "Speed: 1.1ms preprocess, 45.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.0ms\n",
      "Speed: 1.0ms preprocess, 40.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.2ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.3ms\n",
      "Speed: 1.3ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.5ms\n",
      "Speed: 1.1ms preprocess, 43.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.4ms\n",
      "Speed: 1.1ms preprocess, 42.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.6ms\n",
      "Speed: 1.3ms preprocess, 43.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.3ms\n",
      "Speed: 1.0ms preprocess, 41.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.3ms preprocess, 38.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.1ms\n",
      "Speed: 1.0ms preprocess, 41.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 64.3ms\n",
      "Speed: 1.3ms preprocess, 64.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.1ms\n",
      "Speed: 1.1ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.1ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 65.1ms\n",
      "Speed: 1.4ms preprocess, 65.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 1.0ms preprocess, 38.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.5ms\n",
      "Speed: 1.1ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.2ms\n",
      "Speed: 1.2ms preprocess, 39.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.8ms\n",
      "Speed: 1.3ms preprocess, 41.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.2ms preprocess, 40.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.4ms\n",
      "Speed: 1.2ms preprocess, 45.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.1ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.7ms\n",
      "Speed: 1.3ms preprocess, 41.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.0ms\n",
      "Speed: 1.2ms preprocess, 41.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.5ms\n",
      "Speed: 1.2ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 63.7ms\n",
      "Speed: 1.1ms preprocess, 63.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.7ms\n",
      "Speed: 1.2ms preprocess, 41.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.4ms\n",
      "Speed: 1.4ms preprocess, 42.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "try:\n",
    "    model = YOLO('runs/detect/train2/weights/best.pt')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define the class names\n",
    "class_names = ['Hands', 'Pills', 'PillBoxes']\n",
    "\n",
    "# Function to draw bounding boxes with different colors\n",
    "def draw_bounding_boxes(image, results):\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    confs = results[0].boxes.conf.cpu().numpy()\n",
    "    classes = results[0].boxes.cls.cpu().numpy()\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        conf = confs[i]\n",
    "        cls = int(classes[i])\n",
    "        label = class_names[cls]\n",
    "        if label == 'Hands':\n",
    "            color = (0, 255, 0)  # Green\n",
    "        elif label == 'Pill':\n",
    "            color = (255, 0, 0)  # Red\n",
    "        else:\n",
    "            color = (0, 0, 255)  # Blue\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(image, f'{label} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "    return image\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default webcam, or specify the webcam index\n",
    "\n",
    "def update_frame():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading frame from webcam\")\n",
    "        return\n",
    "\n",
    "    # Perform inference with adjusted NMS settings\n",
    "    try:\n",
    "        results = model(frame)\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing inference: {e}\")\n",
    "        return\n",
    "\n",
    "    # Annotate the frame with detection results\n",
    "    frame = draw_bounding_boxes(frame, results)\n",
    "\n",
    "    # Convert the frame to an ImageTk object\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(frame_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "    # Update the label with the new image\n",
    "    lbl.imgtk = imgtk  # Keep a reference to the image to prevent garbage collection\n",
    "    lbl.configure(image=imgtk)\n",
    "\n",
    "    # Schedule the next update\n",
    "    lbl.after(10, update_frame)\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Webcam Detection\")\n",
    "\n",
    "# Create a label to display the video feed\n",
    "lbl = tk.Label(root)\n",
    "lbl.pack()\n",
    "\n",
    "# Start the Tkinter event loop and the video feed\n",
    "root.after(0, update_frame)\n",
    "root.mainloop()\n",
    "\n",
    "# Release the webcam\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "doing other things...\n",
      "Received from server: Hello from RPi loop\n",
      "doing other things...\n",
      "doing other things...\n",
      "doing other things...\n",
      "doing other things...\n",
      "doing other things...\n",
      "doing other things...\n",
      "Client disconnecting...\n",
      "Client stopped gracefully\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import threading\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "server_address = ('192.168.168.167', 1441)  # Connect to RPi (or other server) on ip ... and port ... (the port is set in server.py)\n",
    "# the ip address can also be the WiFi ip of your RPi, but this can change. You can print your WiFi IP on your LCD? (if needed)\n",
    "\n",
    "\n",
    "# Global vars for use in methods/threads\n",
    "client_socket = None\n",
    "receive_thread = None\n",
    "shutdown_flag = threading.Event() # see: https://docs.python.org/3/library/threading.html#event-objects\n",
    "\n",
    "\n",
    "def setup_socket_client():\n",
    "    global client_socket, receive_thread\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # create a socket instance\n",
    "    client_socket.connect(server_address) # connect to specified server\n",
    "    print(\"Connected to server\")\n",
    "\n",
    "    receive_thread = threading.Thread(target=receive_messages, args=(client_socket, shutdown_flag))\n",
    "    receive_thread.start()\n",
    "\n",
    "def receive_messages(sock, shutdown_flag):\n",
    "    sock.settimeout(1)  # Set a timeout on the socket so when can check shutdown_flag.is_set in the loop, instead of blocking\n",
    "    counter = 0 # count the incoming messages, part of demo\n",
    "    try:\n",
    "        while not shutdown_flag.is_set(): # as long as ctrl+c is not pressed\n",
    "            try:\n",
    "                data = sock.recv(1024) # try to receive 1024 bytes of data (maximum amount; can be less)\n",
    "                if not data: # when no data is received, try again (and shutdown flag is checked again)\n",
    "                    break\n",
    "                print(\"Received from server:\", data.decode()) # print the received data, or do something with it\n",
    "                counter += 1 # up the count by 1\n",
    "                response = \"{} message(s) received\".format(counter) # create a response string\n",
    "                sock.sendall(response.encode()) # encode and send the data\n",
    "            except socket.timeout: # when no data comes within timeout, try again\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        if not shutdown_flag.is_set():\n",
    "            print(f\"Connection error: {e}\")\n",
    "    finally:\n",
    "        sock.close()\n",
    "\n",
    "def main():\n",
    "    global client_socket, receive_thread\n",
    "\n",
    "    setup_socket_client()\n",
    "\n",
    "    if client_socket is None:\n",
    "        print(\"Not connected, is server running on {}:{}?\".format(server_address[0], server_address[1]))\n",
    "        sys.exit()\n",
    "    \n",
    "    # send \"hello I'm connected\" message\n",
    "    client_socket.sendall(\"Hello from AI / notebook\".encode()) # send a \"connected\" message from client > server\n",
    "        \n",
    "\n",
    "    try:\n",
    "        while True: # random loop for other things\n",
    "            time.sleep(6)\n",
    "            print(\"doing other things...\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Client disconnecting...\")\n",
    "        shutdown_flag.set()\n",
    "    finally:\n",
    "        client_socket.close()\n",
    "        receive_thread.join()\n",
    "        print(\"Client stopped gracefully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@141.347] global /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_f6tvh9615u/croot/opencv-suite_1691620375715/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server\n",
      "\n",
      "0: 288x480 (no detections), 53.1ms\n",
      "Speed: 3.2ms preprocess, 53.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 70.3ms\n",
      "Speed: 2.0ms preprocess, 70.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 48.9ms\n",
      "Speed: 1.4ms preprocess, 48.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 47.1ms\n",
      "Speed: 1.2ms preprocess, 47.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.1ms preprocess, 40.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.2ms\n",
      "Speed: 1.3ms preprocess, 44.2ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 47.5ms\n",
      "Speed: 1.4ms preprocess, 47.5ms inference, 6.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.4ms\n",
      "Speed: 1.2ms preprocess, 46.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.8ms\n",
      "Speed: 1.2ms preprocess, 42.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.8ms\n",
      "Speed: 1.1ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 PillBox, 40.8ms\n",
      "Speed: 1.4ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.6ms\n",
      "Speed: 1.3ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.0ms\n",
      "Speed: 1.3ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.0ms\n",
      "Speed: 1.5ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.2ms\n",
      "Speed: 1.1ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.7ms\n",
      "Speed: 1.1ms preprocess, 46.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.1ms\n",
      "Speed: 1.2ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.5ms\n",
      "Speed: 1.1ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.3ms\n",
      "Speed: 1.3ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.0ms\n",
      "Speed: 1.4ms preprocess, 42.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 44.5ms\n",
      "Speed: 1.1ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 PillBox, 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.9ms\n",
      "Speed: 1.1ms preprocess, 46.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.7ms\n",
      "Speed: 1.1ms preprocess, 46.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.4ms\n",
      "Speed: 1.3ms preprocess, 40.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.5ms\n",
      "Speed: 1.0ms preprocess, 45.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.9ms\n",
      "Speed: 1.0ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.2ms\n",
      "Speed: 1.2ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.9ms\n",
      "Speed: 1.5ms preprocess, 42.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.6ms\n",
      "Speed: 1.1ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.9ms\n",
      "Speed: 1.1ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.6ms\n",
      "Speed: 1.3ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.7ms\n",
      "Speed: 1.1ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 51.0ms\n",
      "Speed: 1.1ms preprocess, 51.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 143.0ms\n",
      "Speed: 2.6ms preprocess, 143.0ms inference, 0.9ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 178.4ms\n",
      "Speed: 4.3ms preprocess, 178.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.0ms\n",
      "Speed: 1.5ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.5ms\n",
      "Speed: 1.1ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 48.6ms\n",
      "Speed: 1.4ms preprocess, 48.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.2ms\n",
      "Speed: 1.3ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.2ms\n",
      "Speed: 1.3ms preprocess, 44.2ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.1ms\n",
      "Speed: 1.2ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.7ms\n",
      "Speed: 1.2ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.3ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 64.9ms\n",
      "Speed: 1.2ms preprocess, 64.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 50.7ms\n",
      "Speed: 1.6ms preprocess, 50.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.4ms\n",
      "Speed: 1.4ms preprocess, 45.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.1ms\n",
      "Speed: 1.2ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.8ms\n",
      "Speed: 1.3ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.8ms\n",
      "Speed: 1.3ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.3ms\n",
      "Speed: 1.2ms preprocess, 46.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 134.8ms\n",
      "Speed: 1.2ms preprocess, 134.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 52.6ms\n",
      "Speed: 1.1ms preprocess, 52.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 55.7ms\n",
      "Speed: 1.4ms preprocess, 55.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.6ms\n",
      "Speed: 1.1ms preprocess, 44.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 91.1ms\n",
      "Speed: 2.6ms preprocess, 91.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 48.6ms\n",
      "Speed: 1.3ms preprocess, 48.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.2ms\n",
      "Speed: 1.3ms preprocess, 44.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 65.4ms\n",
      "Speed: 1.2ms preprocess, 65.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.2ms\n",
      "Speed: 1.2ms preprocess, 46.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 71.0ms\n",
      "Speed: 1.2ms preprocess, 71.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 49.2ms\n",
      "Speed: 1.3ms preprocess, 49.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 114.8ms\n",
      "Speed: 1.5ms preprocess, 114.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.5ms\n",
      "Speed: 1.1ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.2ms\n",
      "Speed: 1.4ms preprocess, 45.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.6ms\n",
      "Speed: 1.4ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.8ms\n",
      "Speed: 1.3ms preprocess, 46.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.7ms\n",
      "Speed: 1.1ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.1ms\n",
      "Speed: 1.2ms preprocess, 42.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.2ms\n",
      "Speed: 1.1ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.2ms\n",
      "Speed: 1.4ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.0ms\n",
      "Speed: 1.1ms preprocess, 42.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.4ms\n",
      "Speed: 1.1ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.3ms\n",
      "Speed: 1.0ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.2ms\n",
      "Speed: 1.2ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.1ms\n",
      "Speed: 1.2ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.0ms\n",
      "Speed: 1.1ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.3ms\n",
      "Speed: 1.1ms preprocess, 45.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.8ms\n",
      "Speed: 1.2ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.3ms\n",
      "Speed: 1.2ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.6ms\n",
      "Speed: 1.2ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 73.8ms\n",
      "Speed: 1.2ms preprocess, 73.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 48.3ms\n",
      "Speed: 1.2ms preprocess, 48.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.7ms\n",
      "Speed: 1.3ms preprocess, 42.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.6ms\n",
      "Speed: 1.1ms preprocess, 44.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 48.9ms\n",
      "Speed: 1.1ms preprocess, 48.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.2ms\n",
      "Speed: 1.1ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.2ms\n",
      "Speed: 1.3ms preprocess, 42.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.7ms\n",
      "Speed: 1.1ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.7ms\n",
      "Speed: 1.4ms preprocess, 40.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.0ms\n",
      "Speed: 1.2ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 PillBox, 47.0ms\n",
      "Speed: 1.2ms preprocess, 47.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.0ms\n",
      "Speed: 1.3ms preprocess, 41.0ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.1ms\n",
      "Speed: 1.1ms preprocess, 40.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 44.1ms\n",
      "Speed: 1.3ms preprocess, 44.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 PillBox, 44.2ms\n",
      "Speed: 1.4ms preprocess, 44.2ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 47.3ms\n",
      "Speed: 1.2ms preprocess, 47.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.7ms\n",
      "Speed: 1.2ms preprocess, 43.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 PillBox, 41.3ms\n",
      "Speed: 1.0ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.6ms\n",
      "Speed: 1.4ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.1ms\n",
      "Speed: 1.1ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.7ms\n",
      "Speed: 1.1ms preprocess, 41.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.5ms\n",
      "Speed: 1.0ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.6ms\n",
      "Speed: 1.1ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.6ms\n",
      "Speed: 1.3ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.2ms\n",
      "Speed: 1.0ms preprocess, 43.2ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.8ms\n",
      "Speed: 1.3ms preprocess, 42.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.4ms\n",
      "Speed: 1.2ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.8ms\n",
      "Speed: 1.3ms preprocess, 42.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.0ms\n",
      "Speed: 1.1ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.7ms\n",
      "Speed: 1.1ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.5ms\n",
      "Speed: 1.4ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 PillBox, 40.9ms\n",
      "Speed: 1.2ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 PillBox, 41.7ms\n",
      "Speed: 1.0ms preprocess, 41.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.0ms\n",
      "Speed: 1.3ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.3ms\n",
      "Speed: 1.1ms preprocess, 42.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 45.2ms\n",
      "Speed: 1.3ms preprocess, 45.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.4ms\n",
      "Speed: 1.3ms preprocess, 40.4ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.4ms\n",
      "Speed: 1.2ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.8ms\n",
      "Speed: 1.0ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.5ms\n",
      "Speed: 1.2ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 54.8ms\n",
      "Speed: 1.4ms preprocess, 54.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 49.7ms\n",
      "Speed: 1.4ms preprocess, 49.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.9ms\n",
      "Speed: 1.1ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 65.6ms\n",
      "Speed: 1.2ms preprocess, 65.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.6ms\n",
      "Speed: 1.3ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 44.3ms\n",
      "Speed: 1.2ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.0ms\n",
      "Speed: 1.1ms preprocess, 42.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.3ms\n",
      "Speed: 1.2ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.6ms\n",
      "Speed: 1.1ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.2ms\n",
      "Speed: 1.1ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 46.7ms\n",
      "Speed: 2.2ms preprocess, 46.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.0ms\n",
      "Speed: 1.2ms preprocess, 43.0ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.8ms\n",
      "Speed: 1.0ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 44.5ms\n",
      "Speed: 1.1ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.9ms\n",
      "Speed: 1.1ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 45.2ms\n",
      "Speed: 1.1ms preprocess, 45.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 67.9ms\n",
      "Speed: 1.7ms preprocess, 67.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.2ms\n",
      "Speed: 1.0ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.7ms\n",
      "Speed: 1.1ms preprocess, 40.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.7ms\n",
      "Speed: 1.1ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.2ms\n",
      "Speed: 1.1ms preprocess, 41.2ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.7ms\n",
      "Speed: 1.3ms preprocess, 41.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.3ms\n",
      "Speed: 1.3ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 80.6ms\n",
      "Speed: 1.1ms preprocess, 80.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.5ms\n",
      "Speed: 1.0ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.2ms\n",
      "Speed: 1.1ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.6ms\n",
      "Speed: 1.5ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.5ms\n",
      "Speed: 1.1ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.7ms\n",
      "Speed: 1.1ms preprocess, 41.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 44.5ms\n",
      "Speed: 1.4ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.8ms\n",
      "Speed: 1.2ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 44.4ms\n",
      "Speed: 1.1ms preprocess, 44.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.0ms\n",
      "Speed: 1.0ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.0ms\n",
      "Speed: 1.2ms preprocess, 42.0ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.9ms\n",
      "Speed: 1.0ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.2ms\n",
      "Speed: 1.3ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.3ms\n",
      "Speed: 1.1ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.2ms\n",
      "Speed: 1.3ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.1ms\n",
      "Speed: 1.2ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.5ms\n",
      "Speed: 1.2ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.4ms\n",
      "Speed: 1.1ms preprocess, 41.4ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.6ms\n",
      "Speed: 1.2ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.3ms\n",
      "Speed: 1.0ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.8ms\n",
      "Speed: 1.2ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.8ms\n",
      "Speed: 1.2ms preprocess, 43.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.1ms\n",
      "Speed: 1.2ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.9ms\n",
      "Speed: 1.1ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.4ms\n",
      "Speed: 1.2ms preprocess, 40.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 44.9ms\n",
      "Speed: 1.2ms preprocess, 44.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.4ms\n",
      "Speed: 1.2ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.3ms\n",
      "Speed: 1.1ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.4ms\n",
      "Speed: 1.1ms preprocess, 40.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.2ms\n",
      "Speed: 1.2ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.6ms\n",
      "Speed: 1.1ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.3ms\n",
      "Speed: 1.3ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.7ms\n",
      "Speed: 1.0ms preprocess, 40.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.4ms\n",
      "Speed: 1.2ms preprocess, 42.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.6ms\n",
      "Speed: 1.2ms preprocess, 41.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.0ms\n",
      "Speed: 1.6ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.4ms\n",
      "Speed: 1.1ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.0ms\n",
      "Speed: 1.3ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.6ms\n",
      "Speed: 1.1ms preprocess, 46.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.3ms\n",
      "Speed: 1.2ms preprocess, 42.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.6ms\n",
      "Speed: 1.1ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.9ms\n",
      "Speed: 1.2ms preprocess, 44.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.3ms\n",
      "Speed: 1.3ms preprocess, 45.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.3ms\n",
      "Speed: 1.1ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.9ms\n",
      "Speed: 1.1ms preprocess, 44.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.6ms\n",
      "Speed: 1.1ms preprocess, 40.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.7ms\n",
      "Speed: 1.3ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 48.0ms\n",
      "Speed: 1.3ms preprocess, 48.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.8ms\n",
      "Speed: 1.1ms preprocess, 42.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.4ms\n",
      "Speed: 1.3ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.9ms\n",
      "Speed: 1.1ms preprocess, 41.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.0ms\n",
      "Speed: 1.5ms preprocess, 40.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.1ms\n",
      "Speed: 1.3ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.8ms\n",
      "Speed: 1.2ms preprocess, 41.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.8ms\n",
      "Speed: 1.1ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.4ms\n",
      "Speed: 1.2ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 46.0ms\n",
      "Speed: 1.4ms preprocess, 46.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.2ms\n",
      "Speed: 1.2ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.4ms\n",
      "Speed: 1.2ms preprocess, 42.4ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.3ms\n",
      "Speed: 1.1ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.0ms\n",
      "Speed: 1.1ms preprocess, 40.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.1ms\n",
      "Speed: 1.1ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.4ms\n",
      "Speed: 1.1ms preprocess, 40.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.0ms\n",
      "Speed: 1.2ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 65.2ms\n",
      "Speed: 1.0ms preprocess, 65.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.8ms\n",
      "Speed: 1.1ms preprocess, 43.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.9ms\n",
      "Speed: 1.1ms preprocess, 45.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 47.3ms\n",
      "Speed: 1.1ms preprocess, 47.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.7ms\n",
      "Speed: 1.1ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.5ms\n",
      "Speed: 1.2ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.6ms\n",
      "Speed: 1.2ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 50.7ms\n",
      "Speed: 1.4ms preprocess, 50.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 47.9ms\n",
      "Speed: 1.2ms preprocess, 47.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.8ms\n",
      "Speed: 1.2ms preprocess, 41.8ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.5ms\n",
      "Speed: 1.2ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 42.3ms\n",
      "Speed: 1.1ms preprocess, 42.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 45.5ms\n",
      "Speed: 1.2ms preprocess, 45.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.4ms\n",
      "Speed: 1.4ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.8ms\n",
      "Speed: 1.2ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.2ms\n",
      "Speed: 1.3ms preprocess, 41.2ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 47.0ms\n",
      "Speed: 1.9ms preprocess, 47.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.8ms\n",
      "Speed: 1.1ms preprocess, 43.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.2ms\n",
      "Speed: 1.1ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.6ms\n",
      "Speed: 1.1ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 42.4ms\n",
      "Speed: 1.0ms preprocess, 42.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 40.6ms\n",
      "Speed: 1.3ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 44.1ms\n",
      "Speed: 1.0ms preprocess, 44.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 41.9ms\n",
      "Speed: 1.0ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "Sent to Raspberry Pi: Wrong pill detected\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 39.9ms\n",
      "Speed: 1.2ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 1 PillBox, 43.7ms\n",
      "Speed: 1.3ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Pills, 1 PillBox, 47.1ms\n",
      "Speed: 1.2ms preprocess, 47.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.1ms\n",
      "Speed: 1.1ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 48.3ms\n",
      "Speed: 1.1ms preprocess, 48.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.0ms\n",
      "Speed: 1.1ms preprocess, 41.0ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.8ms\n",
      "Speed: 1.1ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 52.4ms\n",
      "Speed: 1.2ms preprocess, 52.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 49.1ms\n",
      "Speed: 1.5ms preprocess, 49.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.1ms\n",
      "Speed: 1.1ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 PillBox, 43.1ms\n",
      "Speed: 1.2ms preprocess, 43.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.8ms\n",
      "Speed: 1.0ms preprocess, 46.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 55.3ms\n",
      "Speed: 1.3ms preprocess, 55.3ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.8ms\n",
      "Speed: 1.2ms preprocess, 42.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 46.5ms\n",
      "Speed: 1.1ms preprocess, 46.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.2ms\n",
      "Speed: 0.9ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.9ms\n",
      "Speed: 1.2ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.7ms\n",
      "Speed: 1.1ms preprocess, 40.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.6ms\n",
      "Speed: 1.2ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.8ms\n",
      "Speed: 2.8ms preprocess, 45.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.3ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.6ms\n",
      "Speed: 1.1ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 65.2ms\n",
      "Speed: 1.3ms preprocess, 65.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.9ms\n",
      "Speed: 1.2ms preprocess, 45.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.6ms\n",
      "Speed: 1.0ms preprocess, 42.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.3ms\n",
      "Speed: 1.2ms preprocess, 41.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 46.0ms\n",
      "Speed: 1.3ms preprocess, 46.0ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.0ms preprocess, 41.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.5ms\n",
      "Speed: 1.1ms preprocess, 42.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.1ms\n",
      "Speed: 1.0ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.6ms\n",
      "Speed: 1.3ms preprocess, 42.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.2ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.8ms\n",
      "Speed: 1.1ms preprocess, 42.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 45.1ms\n",
      "Speed: 1.2ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 45.5ms\n",
      "Speed: 1.2ms preprocess, 45.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.6ms\n",
      "Speed: 1.1ms preprocess, 45.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.7ms\n",
      "Speed: 1.2ms preprocess, 40.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.5ms\n",
      "Speed: 1.1ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 45.2ms\n",
      "Speed: 1.3ms preprocess, 45.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.7ms\n",
      "Speed: 1.5ms preprocess, 42.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.8ms\n",
      "Speed: 1.1ms preprocess, 42.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.6ms\n",
      "Speed: 1.1ms preprocess, 40.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 65.5ms\n",
      "Speed: 1.3ms preprocess, 65.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 46.8ms\n",
      "Speed: 1.3ms preprocess, 46.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 42.4ms\n",
      "Speed: 1.1ms preprocess, 42.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.8ms\n",
      "Speed: 1.0ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.4ms\n",
      "Speed: 1.3ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 44.0ms\n",
      "Speed: 1.2ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.7ms\n",
      "Speed: 1.3ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 45.2ms\n",
      "Speed: 1.0ms preprocess, 45.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 44.2ms\n",
      "Speed: 1.3ms preprocess, 44.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 44.3ms\n",
      "Speed: 1.4ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 48.6ms\n",
      "Speed: 1.3ms preprocess, 48.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.2ms\n",
      "Speed: 1.2ms preprocess, 43.2ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 46.0ms\n",
      "Speed: 1.0ms preprocess, 46.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.3ms\n",
      "Speed: 1.1ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.9ms\n",
      "Speed: 1.1ms preprocess, 38.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 44.7ms\n",
      "Speed: 1.2ms preprocess, 44.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.5ms\n",
      "Speed: 1.5ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.9ms\n",
      "Speed: 1.2ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.5ms\n",
      "Speed: 1.0ms preprocess, 45.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.7ms\n",
      "Speed: 1.1ms preprocess, 40.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.1ms\n",
      "Speed: 1.3ms preprocess, 44.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 139.9ms\n",
      "Speed: 3.0ms preprocess, 139.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 66.7ms\n",
      "Speed: 2.6ms preprocess, 66.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 44.8ms\n",
      "Speed: 1.1ms preprocess, 44.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 48.0ms\n",
      "Speed: 1.2ms preprocess, 48.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 48.5ms\n",
      "Speed: 1.3ms preprocess, 48.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.7ms\n",
      "Speed: 1.4ms preprocess, 41.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.8ms\n",
      "Speed: 1.1ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 47.3ms\n",
      "Speed: 1.1ms preprocess, 47.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.6ms\n",
      "Speed: 1.0ms preprocess, 46.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.1ms\n",
      "Speed: 1.6ms preprocess, 46.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 49.6ms\n",
      "Speed: 1.4ms preprocess, 49.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 46.4ms\n",
      "Speed: 1.3ms preprocess, 46.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.5ms\n",
      "Speed: 1.2ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 49.2ms\n",
      "Speed: 1.4ms preprocess, 49.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.7ms\n",
      "Speed: 1.3ms preprocess, 42.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 48.6ms\n",
      "Speed: 1.8ms preprocess, 48.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 48.5ms\n",
      "Speed: 1.1ms preprocess, 48.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 42.5ms\n",
      "Speed: 1.1ms preprocess, 42.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.5ms\n",
      "Speed: 1.2ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "import socket\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "try:\n",
    "    model = YOLO('runs/detect/train2/weights/best.pt')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define the class names\n",
    "class_names = ['Hands', 'Pills', 'PillBoxes']\n",
    "\n",
    "# Server address and port for Raspberry Pi\n",
    "server_address = ('192.168.168.167', 1442)  # Change this to your Raspberry Pi IP and port\n",
    "\n",
    "# Global vars for use in methods/threads\n",
    "client_socket = None\n",
    "receive_thread = None\n",
    "shutdown_flag = threading.Event()\n",
    "last_notification_time = 0\n",
    "notification_interval = 1\n",
    "\n",
    "def setup_socket_client():\n",
    "    global client_socket, receive_thread\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  # Create a socket instance\n",
    "    client_socket.connect(server_address)  # Connect to specified server\n",
    "    print(\"Connected to server\")\n",
    "\n",
    "    receive_thread = threading.Thread(target=receive_messages, args=(client_socket, shutdown_flag))\n",
    "    receive_thread.start()\n",
    "\n",
    "def receive_messages(sock, shutdown_flag):\n",
    "    sock.settimeout(1)  # Set a timeout on the socket so we can check shutdown_flag.is_set in the loop, instead of blocking\n",
    "    try:\n",
    "        while not shutdown_flag.is_set():  # As long as ctrl+c is not pressed\n",
    "            try:\n",
    "                data = sock.recv(1024)  # Try to receive 1024 bytes of data (maximum amount; can be less)\n",
    "                if not data:  # When no data is received, try again (and shutdown flag is checked again)\n",
    "                    break\n",
    "                print(\"Received from server:\", data.decode())  # Print the received data, or do something with it\n",
    "            except socket.timeout:  # When no data comes within timeout, try again\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        if not shutdown_flag.is_set():\n",
    "            print(f\"Connection error: {e}\")\n",
    "    finally:\n",
    "        sock.close()\n",
    "\n",
    "# Function to draw bounding boxes with different colors\n",
    "def draw_bounding_boxes(image, results):\n",
    "    global last_notification_time\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    confs = results[0].boxes.conf.cpu().numpy()\n",
    "    classes = results[0].boxes.cls.cpu().numpy()\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        conf = confs[i]\n",
    "        cls = int(classes[i])\n",
    "        label = class_names[cls]\n",
    "        if label == 'Hands':\n",
    "            color = (0, 255, 0)  # Green\n",
    "        elif label == 'Pills':\n",
    "            color = (255, 0, 0)  # Red\n",
    "            # Send notification to Raspberry Pi when wrong pill is detected\n",
    "            current_time = time.time()\n",
    "            if current_time - last_notification_time >= notification_interval:\n",
    "                notify_raspberry_pi(\"Wrong pill detected\")\n",
    "                last_notification_time = current_time\n",
    "        else:\n",
    "            color = (0, 0, 255)  # Blue\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(image, f'{label} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "    return image\n",
    "\n",
    "# Function to notify Raspberry Pi\n",
    "def notify_raspberry_pi(message):\n",
    "    try:\n",
    "        client_socket.sendall(message.encode())\n",
    "        print(f\"Sent to Raspberry Pi: {message}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending message to Raspberry Pi: {e}\")\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default webcam, or specify the webcam index\n",
    "\n",
    "def update_frame():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading frame from webcam\")\n",
    "        return\n",
    "\n",
    "    # Perform inference with adjusted NMS settings\n",
    "    try:\n",
    "        results = model(frame)\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing inference: {e}\")\n",
    "        return\n",
    "\n",
    "    # Annotate the frame with detection results\n",
    "    frame = draw_bounding_boxes(frame, results)\n",
    "\n",
    "    # Convert the frame to an ImageTk object\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(frame_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "    # Update the label with the new image\n",
    "    lbl.imgtk = imgtk  # Keep a reference to the image to prevent garbage collection\n",
    "    lbl.configure(image=imgtk)\n",
    "\n",
    "    # Schedule the next update\n",
    "    lbl.after(10, update_frame)\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Webcam Detection\")\n",
    "\n",
    "# Create a label to display the video feed\n",
    "lbl = tk.Label(root)\n",
    "lbl.pack()\n",
    "\n",
    "# Setup the socket client and start the update loop\n",
    "setup_socket_client()\n",
    "root.after(0, update_frame)\n",
    "root.mainloop()\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "shutdown_flag.set()\n",
    "if client_socket:\n",
    "    client_socket.close()\n",
    "if receive_thread:\n",
    "    receive_thread.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
