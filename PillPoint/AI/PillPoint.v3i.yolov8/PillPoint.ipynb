{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train path: True\n",
      "Validation path: True\n",
      "Test path: True\n",
      " The number of images in training set: 1418\n",
      " The number of images in validation set: 195\n",
      " The number of images in test set: 98\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "with open('data.yaml', 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "print(\"Train path:\", os.path.exists(data['train']))\n",
    "print(\"Validation path:\", os.path.exists(data['val']))\n",
    "print(\"Test path:\", os.path.exists(data['test']))\n",
    "\n",
    "def count_images(directory):\n",
    "    supported_formats = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif')\n",
    "    return len([name for name in os.listdir(directory) if name.lower().endswith(supported_formats)])\n",
    "\n",
    "print(f\" The number of images in training set: {count_images(data['train'])}\")\n",
    "print(f\" The number of images in validation set: {count_images(data['val'])}\")\n",
    "print(f\" The number of images in test set: {count_images(data['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.227 ðŸš€ Python-3.9.18 torch-1.13.1 CPU (Apple M1)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/data.yaml, epochs=50, patience=50, batch=16, imgsz=480, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011628 parameters, 3011612 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/train/labels.cache... 1418 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1418/1418 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 540, len(boxes) = 4618. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/valid/labels.cache... 195 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195/195 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 82, len(boxes) = 581. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 480 train, 480 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.677      2.454      1.523         50        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:35<00:00,  3.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581       0.75       0.43      0.518      0.249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G      1.573      1.624      1.431         40        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:55<00:00,  4.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.622      0.588      0.631      0.339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G      1.599      1.618      1.449         49        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:19<00:00,  3.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:23<00:00,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.437      0.472      0.439      0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G      1.562      1.503      1.418         42        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:38<00:00,  3.81s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.589      0.605      0.626      0.306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G      1.541      1.427      1.401         58        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:37<00:00,  3.79s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.723      0.604      0.688      0.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G      1.524      1.369      1.396         96        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:14<00:00,  3.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.672      0.681       0.71      0.393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G      1.497      1.285      1.383         54        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:22<00:00,  3.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.721      0.625        0.7      0.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G      1.526      1.321      1.392         65        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:34<00:00,  3.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:19<00:00,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.645      0.695       0.71       0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G      1.471      1.228      1.357         54        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:39<00:00,  3.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.741      0.676      0.754      0.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G       1.44      1.162      1.344         30        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:24<00:00,  3.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:19<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.622      0.772       0.77       0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50         0G      1.433       1.15      1.342         35        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:22<00:00,  3.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.738       0.76      0.809      0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50         0G      1.416      1.089      1.308         56        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:24<00:00,  3.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.741      0.755      0.787      0.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G      1.406      1.077      1.306         51        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [24:48<00:00, 16.72s/it]   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.837      0.693      0.807      0.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G      1.398      1.064       1.31         59        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:14<00:00,  3.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.752      0.716      0.796      0.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50         0G      1.385      1.033      1.306         38        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:19<00:00,  3.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581       0.85      0.736      0.827      0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50         0G      1.394      1.025       1.31         73        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:05<00:00,  3.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:24<00:00,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.737      0.743      0.784      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50         0G      1.364          1      1.291         60        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:30<00:00,  3.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.768      0.725      0.809      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50         0G      1.361      1.002      1.288         45        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:30<00:00,  3.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.843      0.748      0.843      0.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50         0G       1.36     0.9759      1.286         68        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [06:03<00:00,  4.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.762      0.801      0.838      0.498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50         0G      1.337     0.9474      1.274         48        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [06:13<00:00,  4.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.839      0.769      0.849      0.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50         0G       1.35     0.9452      1.273         56        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:32<00:00,  3.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581       0.74      0.811      0.842        0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50         0G      1.332     0.9459      1.265         65        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:47<00:00,  3.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.853      0.776      0.855      0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50         0G      1.322     0.9239      1.266         79        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:31<00:00,  3.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.851       0.77      0.865      0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50         0G      1.298     0.8888      1.245         54        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:40<00:00,  3.83s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.835      0.794      0.863      0.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G       1.28     0.8676      1.248         51        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:24<00:00,  3.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.823      0.783      0.858      0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G      1.277     0.8636      1.238         42        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:41<00:00,  3.84s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.777      0.814      0.855      0.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50         0G      1.264     0.8473      1.227         66        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:34<00:00,  3.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581       0.82        0.8      0.861       0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50         0G      1.264     0.8405      1.236         69        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:19<00:00,  3.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.794       0.79      0.858      0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50         0G      1.267     0.8379      1.226         50        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:23<00:00,  3.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581       0.86      0.747      0.871      0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50         0G      1.248     0.8206      1.228         37        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:25<00:00,  3.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.729      0.842      0.847      0.527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G      1.235      0.829      1.216         93        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:31<00:00,  3.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.807      0.809      0.866      0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G      1.199     0.7883      1.202         36        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:44<00:00,  3.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.806      0.787      0.869      0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G      1.219     0.7941      1.205         60        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:41<00:00,  3.84s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.834      0.796      0.877      0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G      1.219     0.7823      1.208         57        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:26<00:00,  3.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.822      0.809      0.875      0.536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50         0G      1.215     0.7892      1.201         36        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:55<00:00,  3.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.846      0.802      0.874      0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50         0G      1.201      0.774       1.19         60        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:34<00:00,  3.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.845      0.778      0.873       0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50         0G      1.182     0.7516      1.181         55        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:32<00:00,  3.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.849      0.782      0.874      0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50         0G      1.184      0.754      1.181         41        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:24<00:00,  3.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.788      0.819      0.874       0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50         0G      1.152     0.7221      1.167         58        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:28<00:00,  3.69s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:19<00:00,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.799      0.796      0.875      0.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50         0G      1.164     0.7274       1.17         48        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:29<00:00,  3.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:19<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.857      0.792      0.879      0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50         0G      1.135     0.6611      1.159         27        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:33<00:00,  3.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.847      0.782      0.878      0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50         0G      1.119     0.6345       1.15         27        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:51<00:00,  3.95s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.852      0.773      0.874      0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50         0G      1.098     0.6295      1.148         58        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:37<00:00,  3.79s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581        0.8      0.821       0.88      0.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50         0G      1.088      0.615      1.133         40        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:39<00:00,  3.81s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.842      0.793      0.881      0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50         0G      1.075     0.5962      1.132         31        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:26<00:00,  3.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581       0.82      0.809      0.884      0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50         0G      1.062     0.5931      1.125         45        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:28<00:00,  3.69s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.845      0.793       0.88      0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50         0G      1.052     0.5899      1.121         55        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:32<00:00,  3.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.838      0.803      0.877      0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50         0G      1.046     0.5724      1.116         41        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:26<00:00,  3.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.818      0.838      0.891      0.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50         0G      1.032     0.5659      1.116         30        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:21<00:00,  3.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:18<00:00,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.798      0.835      0.884       0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50         0G      1.038     0.5678      1.115         44        480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [05:27<00:00,  3.68s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:17<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581      0.813      0.826      0.884      0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 5.202 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.227 ðŸš€ Python-3.9.18 torch-1.13.1 CPU (Apple M1)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:24<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        195        581       0.82      0.836      0.891      0.563\n",
      "                 Hands        195        261      0.895      0.874      0.954      0.556\n",
      "                  Open        195         82      0.688      0.695      0.738      0.383\n",
      "                  Pill        195        157      0.767      0.873      0.921       0.65\n",
      "               PillBox        195         81      0.932      0.901      0.951      0.664\n",
      "Speed: 1.1ms preprocess, 120.6ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "\n",
    "absolute_from_relative_path = Path('data.yaml').resolve()\n",
    "\n",
    "model = YOLO('yolov8n.pt')  # Load a pretrained model (recommended for training)\n",
    "results = model.train(data=absolute_from_relative_path, epochs=50, imgsz=480)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x480 23 Pills, 1: 480x480 1 Pill, 2: 480x480 8 Opens, 3: 480x480 2 Pills, 4: 480x480 1 PillBox, 5: 480x480 1 Hands, 6: 480x480 2 Pills, 7: 480x480 4 Handss, 8: 480x480 1 Hands, 9: 480x480 1 Hands, 10: 480x480 6 Handss, 11: 480x480 1 Hands, 12: 480x480 2 Pills, 13: 480x480 1 Hands, 14: 480x480 25 Pills, 15: 480x480 1 Open, 1 PillBox, 16: 480x480 1 Hands, 17: 480x480 2 Handss, 18: 480x480 1 Hands, 19: 480x480 11 Handss, 20: 480x480 2 Pills, 21: 480x480 17 Pills, 22: 480x480 12 Handss, 23: 480x480 4 Handss, 24: 480x480 2 Pills, 25: 480x480 1 PillBox, 26: 480x480 2 Pills, 27: 480x480 1 Hands, 28: 480x480 2 Pills, 29: 480x480 1 Hands, 30: 480x480 7 Opens, 31: 480x480 2 Pills, 32: 480x480 2 Pills, 33: 480x480 2 Handss, 34: 480x480 1 Hands, 35: 480x480 1 PillBox, 36: 480x480 2 Handss, 37: 480x480 7 PillBoxs, 38: 480x480 2 Handss, 39: 480x480 8 Opens, 40: 480x480 10 Handss, 41: 480x480 1 Pill, 42: 480x480 1 PillBox, 43: 480x480 2 Pills, 44: 480x480 6 Handss, 45: 480x480 8 Handss, 46: 480x480 2 Opens, 47: 480x480 1 PillBox, 48: 480x480 3 Pills, 49: 480x480 5 Handss, 50: 480x480 1 Open, 51: 480x480 2 Pills, 52: 480x480 2 PillBoxs, 53: 480x480 1 Hands, 54: 480x480 1 Pill, 55: 480x480 2 Pills, 56: 480x480 1 PillBox, 57: 480x480 2 Handss, 58: 480x480 7 Handss, 59: 480x480 2 Pills, 60: 480x480 1 PillBox, 61: 480x480 2 Opens, 62: 480x480 5 Opens, 63: 480x480 2 Pills, 64: 480x480 2 Handss, 65: 480x480 4 Handss, 66: 480x480 6 Handss, 67: 480x480 1 PillBox, 68: 480x480 5 PillBoxs, 69: 480x480 2 Pills, 70: 480x480 1 Hands, 71: 480x480 4 Handss, 72: 480x480 5 Handss, 73: 480x480 2 Pills, 74: 480x480 1 PillBox, 75: 480x480 2 Handss, 76: 480x480 1 Pill, 77: 480x480 4 Handss, 78: 480x480 16 Pills, 79: 480x480 1 Pill, 80: 480x480 1 Open, 1 PillBox, 81: 480x480 5 Opens, 82: 480x480 7 Handss, 83: 480x480 2 Handss, 84: 480x480 1 PillBox, 85: 480x480 2 Handss, 86: 480x480 1 PillBox, 87: 480x480 5 Handss, 88: 480x480 7 Handss, 89: 480x480 5 Opens, 90: 480x480 2 Pills, 91: 480x480 1 Hands, 92: 480x480 2 Handss, 93: 480x480 6 Handss, 94: 480x480 1 Open, 5 PillBoxs, 95: 480x480 2 Pills, 96: 480x480 1 PillBox, 97: 480x480 3 Pills, 9263.2ms\n",
      "Speed: 1.7ms preprocess, 94.5ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 480)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "98 labels saved to runs/detect/predict2/labels\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240531_092058_Chrome_jpg.rf.781e6428f262e4970964114b64111520.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "conf: tensor([0.7968, 0.7889, 0.7444, 0.7383, 0.7274, 0.7268, 0.7212, 0.7028, 0.6726, 0.6638, 0.6582, 0.6552, 0.6299, 0.6063, 0.5845, 0.5546, 0.5124, 0.5006, 0.4826, 0.4490, 0.3095, 0.3033, 0.3014])\n",
      "data: tensor([[1.2977e+02, 1.1355e+02, 1.6799e+02, 1.4084e+02, 7.9682e-01, 2.0000e+00],\n",
      "        [9.9019e+01, 1.4757e+02, 1.3927e+02, 1.7549e+02, 7.8891e-01, 2.0000e+00],\n",
      "        [2.7729e+02, 8.5498e+01, 3.1236e+02, 1.1036e+02, 7.4444e-01, 2.0000e+00],\n",
      "        [1.7656e+02, 1.6418e+02, 2.1887e+02, 1.9232e+02, 7.3827e-01, 2.0000e+00],\n",
      "        [2.8289e+02, 1.3889e+02, 3.1891e+02, 1.6285e+02, 7.2745e-01, 2.0000e+00],\n",
      "        [3.3051e+02, 1.2547e+02, 3.7471e+02, 1.5225e+02, 7.2679e-01, 2.0000e+00],\n",
      "        [3.6147e+02, 1.4342e+02, 4.0986e+02, 1.7455e+02, 7.2119e-01, 2.0000e+00],\n",
      "        [2.5267e+02, 1.7252e+02, 3.0553e+02, 2.0100e+02, 7.0275e-01, 2.0000e+00],\n",
      "        [1.2241e+02, 1.7842e+02, 1.6541e+02, 2.0627e+02, 6.7259e-01, 2.0000e+00],\n",
      "        [2.5146e+02, 1.1642e+02, 2.9581e+02, 1.4025e+02, 6.6379e-01, 2.0000e+00],\n",
      "        [3.3231e+02, 3.3945e+02, 3.7280e+02, 3.6536e+02, 6.5824e-01, 2.0000e+00],\n",
      "        [2.2978e+02, 1.4894e+02, 2.6930e+02, 1.7664e+02, 6.5518e-01, 2.0000e+00],\n",
      "        [2.1310e+02, 1.9424e+02, 2.3928e+02, 2.1144e+02, 6.2991e-01, 2.0000e+00],\n",
      "        [4.0499e+02, 3.3699e+02, 4.4649e+02, 3.6822e+02, 6.0632e-01, 2.0000e+00],\n",
      "        [2.0209e+02, 1.2388e+02, 2.4035e+02, 1.5085e+02, 5.8452e-01, 2.0000e+00],\n",
      "        [1.4447e+02, 1.3762e+02, 2.0184e+02, 1.6711e+02, 5.5458e-01, 2.0000e+00],\n",
      "        [3.0587e+02, 1.6365e+02, 3.5206e+02, 1.8628e+02, 5.1241e-01, 2.0000e+00],\n",
      "        [2.5991e+02, 3.3737e+02, 2.9869e+02, 3.6474e+02, 5.0059e-01, 2.0000e+00],\n",
      "        [6.2713e+01, 1.2437e+02, 1.1003e+02, 1.5226e+02, 4.8264e-01, 2.0000e+00],\n",
      "        [1.5093e+02, 1.9474e+02, 1.9893e+02, 2.2420e+02, 4.4898e-01, 2.0000e+00],\n",
      "        [3.0988e+02, 1.1287e+02, 3.3296e+02, 1.2565e+02, 3.0949e-01, 2.0000e+00],\n",
      "        [1.8599e+02, 1.1411e+02, 2.1005e+02, 1.2742e+02, 3.0329e-01, 2.0000e+00],\n",
      "        [2.2815e+02, 1.0027e+02, 2.6917e+02, 1.1704e+02, 3.0140e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([23, 6])\n",
      "xywh: tensor([[148.8805, 127.1919,  38.2161,  27.2925],\n",
      "        [119.1464, 161.5278,  40.2554,  27.9191],\n",
      "        [294.8229,  97.9273,  35.0670,  24.8579],\n",
      "        [197.7162, 178.2469,  42.3091,  28.1432],\n",
      "        [300.8982, 150.8718,  36.0170,  23.9574],\n",
      "        [352.6097, 138.8583,  44.1926,  26.7801],\n",
      "        [385.6635, 158.9814,  48.3954,  31.1327],\n",
      "        [279.1039, 186.7606,  52.8580,  28.4843],\n",
      "        [143.9114, 192.3455,  43.0070,  27.8445],\n",
      "        [273.6363, 128.3368,  44.3559,  23.8350],\n",
      "        [352.5565, 352.4028,  40.4950,  25.9111],\n",
      "        [249.5386, 162.7909,  39.5149,  27.7068],\n",
      "        [226.1899, 202.8391,  26.1849,  17.2042],\n",
      "        [425.7443, 352.6020,  41.5006,  31.2300],\n",
      "        [221.2209, 137.3666,  38.2550,  26.9686],\n",
      "        [173.1539, 152.3620,  57.3760,  29.4907],\n",
      "        [328.9652, 174.9661,  46.1986,  22.6285],\n",
      "        [279.3002, 351.0547,  38.7883,  27.3743],\n",
      "        [ 86.3726, 138.3155,  47.3201,  27.8982],\n",
      "        [174.9316, 209.4710,  48.0035,  29.4550],\n",
      "        [321.4211, 119.2553,  23.0851,  12.7805],\n",
      "        [198.0231, 120.7667,  24.0616,  13.3127],\n",
      "        [248.6613, 108.6545,  41.0214,  16.7778]])\n",
      "xywhn: tensor([[0.3102, 0.2650, 0.0796, 0.0569],\n",
      "        [0.2482, 0.3365, 0.0839, 0.0582],\n",
      "        [0.6142, 0.2040, 0.0731, 0.0518],\n",
      "        [0.4119, 0.3713, 0.0881, 0.0586],\n",
      "        [0.6269, 0.3143, 0.0750, 0.0499],\n",
      "        [0.7346, 0.2893, 0.0921, 0.0558],\n",
      "        [0.8035, 0.3312, 0.1008, 0.0649],\n",
      "        [0.5815, 0.3891, 0.1101, 0.0593],\n",
      "        [0.2998, 0.4007, 0.0896, 0.0580],\n",
      "        [0.5701, 0.2674, 0.0924, 0.0497],\n",
      "        [0.7345, 0.7342, 0.0844, 0.0540],\n",
      "        [0.5199, 0.3391, 0.0823, 0.0577],\n",
      "        [0.4712, 0.4226, 0.0546, 0.0358],\n",
      "        [0.8870, 0.7346, 0.0865, 0.0651],\n",
      "        [0.4609, 0.2862, 0.0797, 0.0562],\n",
      "        [0.3607, 0.3174, 0.1195, 0.0614],\n",
      "        [0.6853, 0.3645, 0.0962, 0.0471],\n",
      "        [0.5819, 0.7314, 0.0808, 0.0570],\n",
      "        [0.1799, 0.2882, 0.0986, 0.0581],\n",
      "        [0.3644, 0.4364, 0.1000, 0.0614],\n",
      "        [0.6696, 0.2484, 0.0481, 0.0266],\n",
      "        [0.4125, 0.2516, 0.0501, 0.0277],\n",
      "        [0.5180, 0.2264, 0.0855, 0.0350]])\n",
      "xyxy: tensor([[129.7724, 113.5456, 167.9885, 140.8382],\n",
      "        [ 99.0187, 147.5683, 139.2742, 175.4874],\n",
      "        [277.2894,  85.4983, 312.3564, 110.3562],\n",
      "        [176.5617, 164.1753, 218.8708, 192.3185],\n",
      "        [282.8897, 138.8931, 318.9066, 162.8505],\n",
      "        [330.5135, 125.4683, 374.7060, 152.2484],\n",
      "        [361.4658, 143.4151, 409.8612, 174.5478],\n",
      "        [252.6750, 172.5185, 305.5329, 201.0028],\n",
      "        [122.4079, 178.4232, 165.4149, 206.2677],\n",
      "        [251.4583, 116.4194, 295.8142, 140.2543],\n",
      "        [332.3090, 339.4472, 372.8040, 365.3583],\n",
      "        [229.7812, 148.9375, 269.2961, 176.6443],\n",
      "        [213.0974, 194.2371, 239.2823, 211.4412],\n",
      "        [404.9940, 336.9870, 446.4946, 368.2170],\n",
      "        [202.0935, 123.8823, 240.3484, 150.8509],\n",
      "        [144.4659, 137.6166, 201.8419, 167.1073],\n",
      "        [305.8659, 163.6519, 352.0645, 186.2803],\n",
      "        [259.9060, 337.3676, 298.6943, 364.7418],\n",
      "        [ 62.7126, 124.3664, 110.0327, 152.2646],\n",
      "        [150.9298, 194.7435, 198.9334, 224.1985],\n",
      "        [309.8785, 112.8650, 332.9636, 125.6455],\n",
      "        [185.9923, 114.1104, 210.0539, 127.4231],\n",
      "        [228.1505, 100.2656, 269.1720, 117.0434]])\n",
      "xyxyn: tensor([[0.2704, 0.2366, 0.3500, 0.2934],\n",
      "        [0.2063, 0.3074, 0.2902, 0.3656],\n",
      "        [0.5777, 0.1781, 0.6507, 0.2299],\n",
      "        [0.3678, 0.3420, 0.4560, 0.4007],\n",
      "        [0.5894, 0.2894, 0.6644, 0.3393],\n",
      "        [0.6886, 0.2614, 0.7806, 0.3172],\n",
      "        [0.7531, 0.2988, 0.8539, 0.3636],\n",
      "        [0.5264, 0.3594, 0.6365, 0.4188],\n",
      "        [0.2550, 0.3717, 0.3446, 0.4297],\n",
      "        [0.5239, 0.2425, 0.6163, 0.2922],\n",
      "        [0.6923, 0.7072, 0.7767, 0.7612],\n",
      "        [0.4787, 0.3103, 0.5610, 0.3680],\n",
      "        [0.4440, 0.4047, 0.4985, 0.4405],\n",
      "        [0.8437, 0.7021, 0.9302, 0.7671],\n",
      "        [0.4210, 0.2581, 0.5007, 0.3143],\n",
      "        [0.3010, 0.2867, 0.4205, 0.3481],\n",
      "        [0.6372, 0.3409, 0.7335, 0.3881],\n",
      "        [0.5415, 0.7028, 0.6223, 0.7599],\n",
      "        [0.1307, 0.2591, 0.2292, 0.3172],\n",
      "        [0.3144, 0.4057, 0.4144, 0.4671],\n",
      "        [0.6456, 0.2351, 0.6937, 0.2618],\n",
      "        [0.3875, 0.2377, 0.4376, 0.2655],\n",
      "        [0.4753, 0.2089, 0.5608, 0.2438]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/aprepitant-80-MG-13-_jpg.rf.22dd3b2a60a04bd33d9c2465b5705ccb.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2.])\n",
      "conf: tensor([0.8868])\n",
      "data: tensor([[115.9309,   0.0000, 379.8198, 480.0000,   0.8868,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[247.8753, 240.0000, 263.8889, 480.0000]])\n",
      "xywhn: tensor([[0.5164, 0.5000, 0.5498, 1.0000]])\n",
      "xyxy: tensor([[115.9309,   0.0000, 379.8198, 480.0000]])\n",
      "xyxyn: tensor([[0.2415, 0.0000, 0.7913, 1.0000]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130555_jpg.rf.af69b3438569e435289ac3ccf52b2022.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.8282, 0.7082, 0.6858, 0.6853, 0.6618, 0.4694, 0.4617, 0.3478])\n",
      "data: tensor([[3.3357e+02, 1.7840e+02, 3.6697e+02, 2.4372e+02, 8.2824e-01, 1.0000e+00],\n",
      "        [1.4770e+02, 2.8933e+02, 1.8753e+02, 3.4328e+02, 7.0817e-01, 1.0000e+00],\n",
      "        [2.6515e+02, 3.0505e+02, 3.0454e+02, 3.4136e+02, 6.8579e-01, 1.0000e+00],\n",
      "        [1.8816e+02, 2.8310e+02, 2.2842e+02, 3.4285e+02, 6.8528e-01, 1.0000e+00],\n",
      "        [3.3651e+02, 2.6567e+02, 3.9613e+02, 3.4304e+02, 6.6181e-01, 1.0000e+00],\n",
      "        [3.3774e+02, 2.6598e+02, 3.8007e+02, 3.4222e+02, 4.6945e-01, 1.0000e+00],\n",
      "        [3.3903e+02, 2.8452e+02, 3.9508e+02, 3.4176e+02, 4.6167e-01, 1.0000e+00],\n",
      "        [2.6393e+02, 2.7329e+02, 3.0514e+02, 3.4175e+02, 3.4778e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([8, 6])\n",
      "xywh: tensor([[350.2668, 211.0609,  33.4019,  65.3123],\n",
      "        [167.6194, 316.3010,  39.8306,  53.9514],\n",
      "        [284.8436, 323.2057,  39.3936,  36.3020],\n",
      "        [208.2929, 312.9770,  40.2593,  59.7501],\n",
      "        [366.3203, 304.3555,  59.6179,  77.3733],\n",
      "        [358.9030, 304.1012,  42.3243,  76.2388],\n",
      "        [367.0524, 313.1375,  56.0467,  57.2366],\n",
      "        [284.5375, 307.5225,  41.2065,  68.4629]])\n",
      "xywhn: tensor([[0.7297, 0.4397, 0.0696, 0.1361],\n",
      "        [0.3492, 0.6590, 0.0830, 0.1124],\n",
      "        [0.5934, 0.6733, 0.0821, 0.0756],\n",
      "        [0.4339, 0.6520, 0.0839, 0.1245],\n",
      "        [0.7632, 0.6341, 0.1242, 0.1612],\n",
      "        [0.7477, 0.6335, 0.0882, 0.1588],\n",
      "        [0.7647, 0.6524, 0.1168, 0.1192],\n",
      "        [0.5928, 0.6407, 0.0858, 0.1426]])\n",
      "xyxy: tensor([[333.5659, 178.4048, 366.9678, 243.7170],\n",
      "        [147.7040, 289.3253, 187.5347, 343.2767],\n",
      "        [265.1469, 305.0547, 304.5404, 341.3567],\n",
      "        [188.1632, 283.1020, 228.4225, 342.8521],\n",
      "        [336.5114, 265.6688, 396.1293, 343.0421],\n",
      "        [337.7408, 265.9818, 380.0652, 342.2206],\n",
      "        [339.0291, 284.5192, 395.0758, 341.7557],\n",
      "        [263.9342, 273.2911, 305.1407, 341.7540]])\n",
      "xyxyn: tensor([[0.6949, 0.3717, 0.7645, 0.5077],\n",
      "        [0.3077, 0.6028, 0.3907, 0.7152],\n",
      "        [0.5524, 0.6355, 0.6345, 0.7112],\n",
      "        [0.3920, 0.5898, 0.4759, 0.7143],\n",
      "        [0.7011, 0.5535, 0.8253, 0.7147],\n",
      "        [0.7036, 0.5541, 0.7918, 0.7130],\n",
      "        [0.7063, 0.5927, 0.8231, 0.7120],\n",
      "        [0.5499, 0.5694, 0.6357, 0.7120]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/carvedilol-3-125-MG-5-Copy_jpg.rf.50227a2a80eace14d269a6924ebfe514.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.8950, 0.8583])\n",
      "data: tensor([[110.1733,   1.5444, 402.8686, 229.4131,   0.8950,   2.0000],\n",
      "        [111.8253, 223.9083, 401.0232, 460.8676,   0.8583,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[256.5209, 115.4788, 292.6953, 227.8687],\n",
      "        [256.4243, 342.3880, 289.1979, 236.9594]])\n",
      "xywhn: tensor([[0.5344, 0.2406, 0.6098, 0.4747],\n",
      "        [0.5342, 0.7133, 0.6025, 0.4937]])\n",
      "xyxy: tensor([[110.1733,   1.5444, 402.8686, 229.4131],\n",
      "        [111.8253, 223.9083, 401.0232, 460.8676]])\n",
      "xyxyn: tensor([[0.2295, 0.0032, 0.8393, 0.4779],\n",
      "        [0.2330, 0.4665, 0.8355, 0.9601]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_152007_Pinterest_jpg.rf.3eb8f0440cf6fe6d34dece18755607a5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8929])\n",
      "data: tensor([[ 40.1897,  87.4451, 480.0000, 288.6127,   0.8929,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[260.0948, 188.0289, 439.8104, 201.1676]])\n",
      "xywhn: tensor([[0.5419, 0.3917, 0.9163, 0.4191]])\n",
      "xyxy: tensor([[ 40.1897,  87.4451, 480.0000, 288.6127]])\n",
      "xyxyn: tensor([[0.0837, 0.1822, 1.0000, 0.6013]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100005_Pinterest_jpg.rf.678060ab21c6e3bf0ec5f687ddc8cded.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8663])\n",
      "data: tensor([[  9.3664,  30.7403, 441.4281, 325.0646,   0.8663,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[225.3972, 177.9025, 432.0617, 294.3243]])\n",
      "xywhn: tensor([[0.4696, 0.3706, 0.9001, 0.6132]])\n",
      "xyxy: tensor([[  9.3664,  30.7403, 441.4281, 325.0646]])\n",
      "xyxyn: tensor([[0.0195, 0.0640, 0.9196, 0.6772]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Oseltamivir-45-MG-6-_jpg.rf.755a2091fd9eeabd66a03d888e510d9e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9305, 0.9262])\n",
      "data: tensor([[ 19.5225, 192.3377, 472.7257, 396.8190,   0.9305,   2.0000],\n",
      "        [ 29.0465,   1.4044, 466.7997, 198.6639,   0.9262,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[246.1241, 294.5784, 453.2032, 204.4813],\n",
      "        [247.9231, 100.0342, 437.7532, 197.2595]])\n",
      "xywhn: tensor([[0.5128, 0.6137, 0.9442, 0.4260],\n",
      "        [0.5165, 0.2084, 0.9120, 0.4110]])\n",
      "xyxy: tensor([[ 19.5225, 192.3377, 472.7257, 396.8190],\n",
      "        [ 29.0465,   1.4044, 466.7997, 198.6639]])\n",
      "xyxyn: tensor([[0.0407, 0.4007, 0.9848, 0.8267],\n",
      "        [0.0605, 0.0029, 0.9725, 0.4139]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150803_Pinterest_jpg.rf.0889529e31ab9be5218171909eee43d1.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0.])\n",
      "conf: tensor([0.9104, 0.8868, 0.8410, 0.5437])\n",
      "data: tensor([[245.0404, 115.3820, 343.9383, 190.2526,   0.9104,   0.0000],\n",
      "        [338.7384,  94.4295, 441.2810, 174.3593,   0.8868,   0.0000],\n",
      "        [270.8044, 239.0964, 462.2420, 379.1738,   0.8410,   0.0000],\n",
      "        [ 68.5104, 183.7769, 192.5761, 309.1123,   0.5437,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[294.4893, 152.8173,  98.8979,  74.8706],\n",
      "        [390.0097, 134.3944, 102.5427,  79.9298],\n",
      "        [366.5232, 309.1351, 191.4376, 140.0775],\n",
      "        [130.5433, 246.4446, 124.0658, 125.3354]])\n",
      "xywhn: tensor([[0.6135, 0.3184, 0.2060, 0.1560],\n",
      "        [0.8125, 0.2800, 0.2136, 0.1665],\n",
      "        [0.7636, 0.6440, 0.3988, 0.2918],\n",
      "        [0.2720, 0.5134, 0.2585, 0.2611]])\n",
      "xyxy: tensor([[245.0404, 115.3820, 343.9383, 190.2526],\n",
      "        [338.7384,  94.4295, 441.2810, 174.3593],\n",
      "        [270.8044, 239.0964, 462.2420, 379.1738],\n",
      "        [ 68.5104, 183.7769, 192.5761, 309.1123]])\n",
      "xyxyn: tensor([[0.5105, 0.2404, 0.7165, 0.3964],\n",
      "        [0.7057, 0.1967, 0.9193, 0.3632],\n",
      "        [0.5642, 0.4981, 0.9630, 0.7899],\n",
      "        [0.1427, 0.3829, 0.4012, 0.6440]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094931_Pinterest_jpg.rf.6360e2b2e54ee14bd88b38b44226b8bc.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8263])\n",
      "data: tensor([[ 44.4488,  64.1098, 395.5571, 334.9132,   0.8263,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[220.0029, 199.5115, 351.1083, 270.8034]])\n",
      "xywhn: tensor([[0.4583, 0.4156, 0.7315, 0.5642]])\n",
      "xyxy: tensor([[ 44.4488,  64.1098, 395.5571, 334.9132]])\n",
      "xyxyn: tensor([[0.0926, 0.1336, 0.8241, 0.6977]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100054_Pinterest_jpg.rf.8a92ff4e2a379268128ca46352988fb6.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8774])\n",
      "data: tensor([[  4.9886,  11.6595, 466.3792, 349.6022,   0.8774,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[235.6839, 180.6309, 461.3906, 337.9426]])\n",
      "xywhn: tensor([[0.4910, 0.3763, 0.9612, 0.7040]])\n",
      "xyxy: tensor([[  4.9886,  11.6595, 466.3792, 349.6022]])\n",
      "xyxyn: tensor([[0.0104, 0.0243, 0.9716, 0.7283]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150739_Pinterest_jpg.rf.98db28037e042799dc6000369bf51a4a.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8840, 0.8515, 0.8177, 0.8076, 0.7744, 0.7527])\n",
      "data: tensor([[ 12.9550,  13.5575, 232.0915, 128.9693,   0.8840,   0.0000],\n",
      "        [263.8718,  10.5404, 471.0369, 128.7045,   0.8515,   0.0000],\n",
      "        [ 26.8416, 131.5855, 217.7339, 241.1135,   0.8177,   0.0000],\n",
      "        [ 24.2125, 241.7568, 226.6831, 351.8516,   0.8076,   0.0000],\n",
      "        [255.8685, 130.5929, 459.2295, 240.5045,   0.7744,   0.0000],\n",
      "        [261.6081, 239.3687, 462.6945, 366.4884,   0.7527,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[122.5232,  71.2634, 219.1365, 115.4118],\n",
      "        [367.4543,  69.6224, 207.1652, 118.1641],\n",
      "        [122.2878, 186.3495, 190.8923, 109.5280],\n",
      "        [125.4478, 296.8042, 202.4706, 110.0948],\n",
      "        [357.5490, 185.5487, 203.3609, 109.9116],\n",
      "        [362.1513, 302.9286, 201.0864, 127.1197]])\n",
      "xywhn: tensor([[0.2553, 0.1485, 0.4565, 0.2404],\n",
      "        [0.7655, 0.1450, 0.4316, 0.2462],\n",
      "        [0.2548, 0.3882, 0.3977, 0.2282],\n",
      "        [0.2613, 0.6183, 0.4218, 0.2294],\n",
      "        [0.7449, 0.3866, 0.4237, 0.2290],\n",
      "        [0.7545, 0.6311, 0.4189, 0.2648]])\n",
      "xyxy: tensor([[ 12.9550,  13.5575, 232.0915, 128.9693],\n",
      "        [263.8718,  10.5404, 471.0369, 128.7045],\n",
      "        [ 26.8416, 131.5855, 217.7339, 241.1135],\n",
      "        [ 24.2125, 241.7568, 226.6831, 351.8516],\n",
      "        [255.8685, 130.5929, 459.2295, 240.5045],\n",
      "        [261.6081, 239.3687, 462.6945, 366.4884]])\n",
      "xyxyn: tensor([[0.0270, 0.0282, 0.4835, 0.2687],\n",
      "        [0.5497, 0.0220, 0.9813, 0.2681],\n",
      "        [0.0559, 0.2741, 0.4536, 0.5023],\n",
      "        [0.0504, 0.5037, 0.4723, 0.7330],\n",
      "        [0.5331, 0.2721, 0.9567, 0.5011],\n",
      "        [0.5450, 0.4987, 0.9639, 0.7635]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100013_Pinterest_jpg.rf.991941b3808fa9249bd2f078893337ca.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8160])\n",
      "data: tensor([[ 43.7339,  61.9423, 459.1706, 306.7650,   0.8160,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[251.4522, 184.3537, 415.4368, 244.8227]])\n",
      "xywhn: tensor([[0.5239, 0.3841, 0.8655, 0.5100]])\n",
      "xyxy: tensor([[ 43.7339,  61.9423, 459.1706, 306.7650]])\n",
      "xyxyn: tensor([[0.0911, 0.1290, 0.9566, 0.6391]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/pitavastatin-1-MG-11-Copy_jpg.rf.804c18270ec8d3a154ad02cdd4db59ff.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.8946, 0.8863])\n",
      "data: tensor([[248.9680,  55.7669, 479.8444, 335.1594,   0.8946,   2.0000],\n",
      "        [ 16.6148,  52.3426, 252.4028, 334.7625,   0.8863,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[364.4062, 195.4632, 230.8764, 279.3925],\n",
      "        [134.5088, 193.5525, 235.7880, 282.4199]])\n",
      "xywhn: tensor([[0.7592, 0.4072, 0.4810, 0.5821],\n",
      "        [0.2802, 0.4032, 0.4912, 0.5884]])\n",
      "xyxy: tensor([[248.9680,  55.7669, 479.8444, 335.1594],\n",
      "        [ 16.6148,  52.3426, 252.4028, 334.7625]])\n",
      "xyxyn: tensor([[0.5187, 0.1162, 0.9997, 0.6982],\n",
      "        [0.0346, 0.1090, 0.5258, 0.6974]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100642_Pinterest_jpg.rf.692bd27b877db6ec312b2d38e61c8563.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8007])\n",
      "data: tensor([[166.3193,  28.0254, 475.7556, 160.0574,   0.8007,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[321.0375,  94.0414, 309.4363, 132.0320]])\n",
      "xywhn: tensor([[0.6688, 0.1959, 0.6447, 0.2751]])\n",
      "xyxy: tensor([[166.3193,  28.0254, 475.7556, 160.0574]])\n",
      "xyxyn: tensor([[0.3465, 0.0584, 0.9912, 0.3335]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240531_092741_Chrome_jpg.rf.e1bd8ccabc60c6c116526de7a789f207.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "conf: tensor([0.7780, 0.7737, 0.7613, 0.7462, 0.7432, 0.7346, 0.6955, 0.6827, 0.6689, 0.6461, 0.6120, 0.6067, 0.5996, 0.5953, 0.5749, 0.5687, 0.5527, 0.5136, 0.5073, 0.4994, 0.4800, 0.4315, 0.3635, 0.2901, 0.2684])\n",
      "data: tensor([[9.4511e+01, 1.7560e+02, 1.6193e+02, 2.0406e+02, 7.7797e-01, 2.0000e+00],\n",
      "        [4.5079e+01, 1.1064e+02, 1.1758e+02, 1.7966e+02, 7.7371e-01, 2.0000e+00],\n",
      "        [3.2887e+02, 1.3758e+02, 3.7542e+02, 1.5799e+02, 7.6131e-01, 2.0000e+00],\n",
      "        [2.5478e+02, 1.4650e+02, 3.2024e+02, 1.7391e+02, 7.4619e-01, 2.0000e+00],\n",
      "        [1.4075e+02, 1.9672e+02, 2.1080e+02, 2.2717e+02, 7.4318e-01, 2.0000e+00],\n",
      "        [1.7795e+02, 1.5992e+02, 2.4608e+02, 1.8891e+02, 7.3464e-01, 2.0000e+00],\n",
      "        [3.0977e+02, 1.6638e+02, 3.7073e+02, 1.9398e+02, 6.9551e-01, 2.0000e+00],\n",
      "        [2.2585e+02, 1.8034e+02, 3.0029e+02, 2.1242e+02, 6.8267e-01, 2.0000e+00],\n",
      "        [3.6888e+02, 3.5253e+02, 3.9736e+02, 3.7348e+02, 6.6889e-01, 2.0000e+00],\n",
      "        [2.7371e+02, 3.4991e+02, 3.0475e+02, 3.7510e+02, 6.4613e-01, 2.0000e+00],\n",
      "        [3.3766e+02, 3.5107e+02, 3.6716e+02, 3.7492e+02, 6.1196e-01, 2.0000e+00],\n",
      "        [3.0632e+02, 3.4990e+02, 3.3650e+02, 3.7258e+02, 6.0672e-01, 2.0000e+00],\n",
      "        [1.9476e+02, 1.2001e+02, 2.8850e+02, 1.5145e+02, 5.9959e-01, 2.0000e+00],\n",
      "        [1.2610e+02, 9.9708e+01, 1.8763e+02, 1.6708e+02, 5.9532e-01, 2.0000e+00],\n",
      "        [3.5463e+01, 3.5519e+02, 7.2803e+01, 3.8350e+02, 5.7491e-01, 2.0000e+00],\n",
      "        [2.9656e+02, 1.1984e+02, 3.2636e+02, 1.3657e+02, 5.6871e-01, 2.0000e+00],\n",
      "        [4.0176e+02, 3.4927e+02, 4.3170e+02, 3.7440e+02, 5.5271e-01, 2.0000e+00],\n",
      "        [2.4624e+02, 3.5248e+02, 2.7361e+02, 3.7173e+02, 5.1364e-01, 2.0000e+00],\n",
      "        [4.2054e+02, 4.1777e+02, 4.5974e+02, 4.4912e+02, 5.0728e-01, 2.0000e+00],\n",
      "        [1.0515e+02, 3.5909e+02, 1.5009e+02, 3.8488e+02, 4.9939e-01, 2.0000e+00],\n",
      "        [1.8080e+02, 3.5417e+02, 2.2171e+02, 3.8560e+02, 4.8000e-01, 2.0000e+00],\n",
      "        [3.7539e+02, 1.4712e+02, 4.4033e+02, 1.8409e+02, 4.3147e-01, 2.0000e+00],\n",
      "        [3.8253e+02, 4.1691e+02, 4.2344e+02, 4.3654e+02, 3.6349e-01, 2.0000e+00],\n",
      "        [2.8530e+02, 4.1800e+02, 3.2263e+02, 4.5167e+02, 2.9009e-01, 2.0000e+00],\n",
      "        [3.5137e+02, 4.1824e+02, 3.8633e+02, 4.5015e+02, 2.6837e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([25, 6])\n",
      "xywh: tensor([[128.2181, 189.8264,  67.4148,  28.4607],\n",
      "        [ 81.3287, 145.1538,  72.4987,  69.0182],\n",
      "        [352.1461, 147.7862,  46.5564,  20.4085],\n",
      "        [287.5087, 160.2011,  65.4608,  27.4108],\n",
      "        [175.7710, 211.9467,  70.0513,  30.4553],\n",
      "        [212.0175, 174.4122,  68.1296,  28.9865],\n",
      "        [340.2455, 180.1797,  60.9606,  27.5911],\n",
      "        [263.0693, 196.3766,  74.4482,  32.0782],\n",
      "        [383.1216, 363.0056,  28.4851,  20.9504],\n",
      "        [289.2304, 362.5068,  31.0439,  25.1887],\n",
      "        [352.4130, 362.9930,  29.5012,  23.8490],\n",
      "        [321.4059, 361.2414,  30.1796,  22.6842],\n",
      "        [241.6310, 135.7305,  93.7369,  31.4378],\n",
      "        [156.8654, 133.3939,  61.5270,  67.3723],\n",
      "        [ 54.1330, 369.3478,  37.3394,  28.3063],\n",
      "        [311.4606, 128.2029,  29.8082,  16.7323],\n",
      "        [416.7303, 361.8380,  29.9409,  25.1336],\n",
      "        [259.9272, 362.1064,  27.3682,  19.2452],\n",
      "        [440.1404, 433.4448,  39.1973,  31.3555],\n",
      "        [127.6197, 371.9878,  44.9456,  25.7883],\n",
      "        [201.2512, 369.8842,  40.9088,  31.4219],\n",
      "        [407.8608, 165.6036,  64.9339,  36.9719],\n",
      "        [402.9885, 426.7274,  40.9102,  19.6338],\n",
      "        [303.9653, 434.8336,  37.3362,  33.6696],\n",
      "        [368.8477, 434.1965,  34.9594,  31.9055]])\n",
      "xywhn: tensor([[0.2671, 0.3955, 0.1404, 0.0593],\n",
      "        [0.1694, 0.3024, 0.1510, 0.1438],\n",
      "        [0.7336, 0.3079, 0.0970, 0.0425],\n",
      "        [0.5990, 0.3338, 0.1364, 0.0571],\n",
      "        [0.3662, 0.4416, 0.1459, 0.0634],\n",
      "        [0.4417, 0.3634, 0.1419, 0.0604],\n",
      "        [0.7088, 0.3754, 0.1270, 0.0575],\n",
      "        [0.5481, 0.4091, 0.1551, 0.0668],\n",
      "        [0.7982, 0.7563, 0.0593, 0.0436],\n",
      "        [0.6026, 0.7552, 0.0647, 0.0525],\n",
      "        [0.7342, 0.7562, 0.0615, 0.0497],\n",
      "        [0.6696, 0.7526, 0.0629, 0.0473],\n",
      "        [0.5034, 0.2828, 0.1953, 0.0655],\n",
      "        [0.3268, 0.2779, 0.1282, 0.1404],\n",
      "        [0.1128, 0.7695, 0.0778, 0.0590],\n",
      "        [0.6489, 0.2671, 0.0621, 0.0349],\n",
      "        [0.8682, 0.7538, 0.0624, 0.0524],\n",
      "        [0.5415, 0.7544, 0.0570, 0.0401],\n",
      "        [0.9170, 0.9030, 0.0817, 0.0653],\n",
      "        [0.2659, 0.7750, 0.0936, 0.0537],\n",
      "        [0.4193, 0.7706, 0.0852, 0.0655],\n",
      "        [0.8497, 0.3450, 0.1353, 0.0770],\n",
      "        [0.8396, 0.8890, 0.0852, 0.0409],\n",
      "        [0.6333, 0.9059, 0.0778, 0.0701],\n",
      "        [0.7684, 0.9046, 0.0728, 0.0665]])\n",
      "xyxy: tensor([[ 94.5107, 175.5961, 161.9255, 204.0568],\n",
      "        [ 45.0794, 110.6447, 117.5781, 179.6629],\n",
      "        [328.8679, 137.5819, 375.4243, 157.9904],\n",
      "        [254.7783, 146.4958, 320.2391, 173.9065],\n",
      "        [140.7454, 196.7190, 210.7967, 227.1743],\n",
      "        [177.9527, 159.9189, 246.0823, 188.9054],\n",
      "        [309.7653, 166.3842, 370.7258, 193.9752],\n",
      "        [225.8452, 180.3375, 300.2934, 212.4158],\n",
      "        [368.8790, 352.5304, 397.3641, 373.4808],\n",
      "        [273.7084, 349.9124, 304.7524, 375.1011],\n",
      "        [337.6624, 351.0685, 367.1636, 374.9175],\n",
      "        [306.3161, 349.8993, 336.4957, 372.5835],\n",
      "        [194.7625, 120.0116, 288.4994, 151.4494],\n",
      "        [126.1019,  99.7078, 187.6289, 167.0801],\n",
      "        [ 35.4633, 355.1947,  72.8027, 383.5010],\n",
      "        [296.5565, 119.8367, 326.3647, 136.5690],\n",
      "        [401.7599, 349.2712, 431.7008, 374.4048],\n",
      "        [246.2431, 352.4838, 273.6112, 371.7290],\n",
      "        [420.5417, 417.7671, 459.7390, 449.1226],\n",
      "        [105.1470, 359.0936, 150.0925, 384.8820],\n",
      "        [180.7968, 354.1733, 221.7055, 385.5952],\n",
      "        [375.3938, 147.1177, 440.3277, 184.0895],\n",
      "        [382.5334, 416.9106, 423.4435, 436.5443],\n",
      "        [285.2972, 417.9988, 322.6334, 451.6684],\n",
      "        [351.3680, 418.2437, 386.3273, 450.1493]])\n",
      "xyxyn: tensor([[0.1969, 0.3658, 0.3373, 0.4251],\n",
      "        [0.0939, 0.2305, 0.2450, 0.3743],\n",
      "        [0.6851, 0.2866, 0.7821, 0.3291],\n",
      "        [0.5308, 0.3052, 0.6672, 0.3623],\n",
      "        [0.2932, 0.4098, 0.4392, 0.4733],\n",
      "        [0.3707, 0.3332, 0.5127, 0.3936],\n",
      "        [0.6453, 0.3466, 0.7723, 0.4041],\n",
      "        [0.4705, 0.3757, 0.6256, 0.4425],\n",
      "        [0.7685, 0.7344, 0.8278, 0.7781],\n",
      "        [0.5702, 0.7290, 0.6349, 0.7815],\n",
      "        [0.7035, 0.7314, 0.7649, 0.7811],\n",
      "        [0.6382, 0.7290, 0.7010, 0.7762],\n",
      "        [0.4058, 0.2500, 0.6010, 0.3155],\n",
      "        [0.2627, 0.2077, 0.3909, 0.3481],\n",
      "        [0.0739, 0.7400, 0.1517, 0.7990],\n",
      "        [0.6178, 0.2497, 0.6799, 0.2845],\n",
      "        [0.8370, 0.7276, 0.8994, 0.7800],\n",
      "        [0.5130, 0.7343, 0.5700, 0.7744],\n",
      "        [0.8761, 0.8703, 0.9578, 0.9357],\n",
      "        [0.2191, 0.7481, 0.3127, 0.8018],\n",
      "        [0.3767, 0.7379, 0.4619, 0.8033],\n",
      "        [0.7821, 0.3065, 0.9173, 0.3835],\n",
      "        [0.7969, 0.8686, 0.8822, 0.9095],\n",
      "        [0.5944, 0.8708, 0.6722, 0.9410],\n",
      "        [0.7320, 0.8713, 0.8048, 0.9378]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240523_083222_jpg.rf.9141a62bb5b647846b42f9450d5d24d3.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 1.])\n",
      "conf: tensor([0.8458, 0.3489])\n",
      "data: tensor([[8.5851e+01, 1.6208e+02, 2.5990e+02, 2.9796e+02, 8.4582e-01, 3.0000e+00],\n",
      "        [1.5638e+02, 9.2674e-02, 2.2486e+02, 3.8248e+01, 3.4894e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[172.8766, 230.0221, 174.0508, 135.8776],\n",
      "        [190.6207,  19.1704,  68.4817,  38.1555]])\n",
      "xywhn: tensor([[0.3602, 0.4792, 0.3626, 0.2831],\n",
      "        [0.3971, 0.0399, 0.1427, 0.0795]])\n",
      "xyxy: tensor([[8.5851e+01, 1.6208e+02, 2.5990e+02, 2.9796e+02],\n",
      "        [1.5638e+02, 9.2674e-02, 2.2486e+02, 3.8248e+01]])\n",
      "xyxyn: tensor([[1.7886e-01, 3.3767e-01, 5.4146e-01, 6.2075e-01],\n",
      "        [3.2579e-01, 1.9307e-04, 4.6846e-01, 7.9684e-02]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100541_Pinterest_jpg.rf.f795c5c218b86692db9deb559a52e184.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8266])\n",
      "data: tensor([[ 19.9703,  15.2195, 475.0381, 211.6304,   0.8266,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[247.5042, 113.4249, 455.0678, 196.4109]])\n",
      "xywhn: tensor([[0.5156, 0.2363, 0.9481, 0.4092]])\n",
      "xyxy: tensor([[ 19.9703,  15.2195, 475.0381, 211.6304]])\n",
      "xyxyn: tensor([[0.0416, 0.0317, 0.9897, 0.4409]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094840_Pinterest_jpg.rf.6b3b6c05405ef5828f547c07bfdd1342.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8662, 0.8298])\n",
      "data: tensor([[ 56.8363,  47.0268, 264.2776, 227.5852,   0.8662,   0.0000],\n",
      "        [253.5043,  76.2480, 442.0080, 218.3112,   0.8298,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[160.5569, 137.3060, 207.4413, 180.5584],\n",
      "        [347.7562, 147.2796, 188.5036, 142.0632]])\n",
      "xywhn: tensor([[0.3345, 0.2861, 0.4322, 0.3762],\n",
      "        [0.7245, 0.3068, 0.3927, 0.2960]])\n",
      "xyxy: tensor([[ 56.8363,  47.0268, 264.2776, 227.5852],\n",
      "        [253.5043,  76.2480, 442.0080, 218.3112]])\n",
      "xyxyn: tensor([[0.1184, 0.0980, 0.5506, 0.4741],\n",
      "        [0.5281, 0.1589, 0.9208, 0.4548]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_095258_Pinterest_jpg.rf.8a5b9ceff5e2adacd867f3cdad37948e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8785])\n",
      "data: tensor([[ 70.0349,   4.6488, 474.8163, 290.0848,   0.8785,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[272.4256, 147.3668, 404.7815, 285.4360]])\n",
      "xywhn: tensor([[0.5676, 0.3070, 0.8433, 0.5947]])\n",
      "xyxy: tensor([[ 70.0349,   4.6488, 474.8163, 290.0848]])\n",
      "xyxyn: tensor([[0.1459, 0.0097, 0.9892, 0.6043]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150947_Pinterest_jpg.rf.c2ee92c5ffb3ea3782f6541d7aa8f511.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8518, 0.7891, 0.7874, 0.7644, 0.7624, 0.6984, 0.6462, 0.6318, 0.5255, 0.2751, 0.2702])\n",
      "data: tensor([[2.7806e+02, 2.0377e+02, 4.5463e+02, 3.3142e+02, 8.5180e-01, 0.0000e+00],\n",
      "        [9.3073e+01, 1.0013e+02, 1.7660e+02, 1.9398e+02, 7.8906e-01, 0.0000e+00],\n",
      "        [8.0740e+01, 2.5932e+02, 2.3669e+02, 2.9672e+02, 7.8743e-01, 0.0000e+00],\n",
      "        [2.9705e+02, 1.1329e+02, 4.4696e+02, 1.7392e+02, 7.6442e-01, 0.0000e+00],\n",
      "        [2.5995e+02, 8.3634e+01, 3.7116e+02, 1.3029e+02, 7.6236e-01, 0.0000e+00],\n",
      "        [4.7116e+01, 1.4961e+02, 1.2537e+02, 2.3252e+02, 6.9840e-01, 0.0000e+00],\n",
      "        [1.2999e+02, 3.5858e+02, 2.5845e+02, 3.9945e+02, 6.4620e-01, 0.0000e+00],\n",
      "        [1.0042e+02, 2.9171e+02, 2.5242e+02, 3.5936e+02, 6.3177e-01, 0.0000e+00],\n",
      "        [5.1166e+01, 3.2583e+02, 1.7083e+02, 3.8560e+02, 5.2553e-01, 0.0000e+00],\n",
      "        [1.0137e+02, 2.9258e+02, 2.4611e+02, 3.3657e+02, 2.7513e-01, 0.0000e+00],\n",
      "        [2.8466e+02, 7.7499e-02, 4.0418e+02, 4.2449e+01, 2.7024e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([11, 6])\n",
      "xywh: tensor([[366.3420, 267.5924, 176.5674, 127.6530],\n",
      "        [134.8338, 147.0545,  83.5226,  93.8467],\n",
      "        [158.7156, 278.0210, 155.9514,  37.4055],\n",
      "        [372.0017, 143.6030, 149.9133,  60.6337],\n",
      "        [315.5538, 106.9619, 111.2155,  46.6558],\n",
      "        [ 86.2455, 191.0649,  78.2581,  82.9011],\n",
      "        [194.2210, 379.0165, 128.4534,  40.8751],\n",
      "        [176.4237, 325.5344, 151.9988,  67.6447],\n",
      "        [110.9957, 355.7134, 119.6600,  59.7767],\n",
      "        [173.7366, 314.5756, 144.7396,  43.9982],\n",
      "        [344.4209,  21.2631, 119.5164,  42.3712]])\n",
      "xywhn: tensor([[0.7632, 0.5575, 0.3678, 0.2659],\n",
      "        [0.2809, 0.3064, 0.1740, 0.1955],\n",
      "        [0.3307, 0.5792, 0.3249, 0.0779],\n",
      "        [0.7750, 0.2992, 0.3123, 0.1263],\n",
      "        [0.6574, 0.2228, 0.2317, 0.0972],\n",
      "        [0.1797, 0.3981, 0.1630, 0.1727],\n",
      "        [0.4046, 0.7896, 0.2676, 0.0852],\n",
      "        [0.3675, 0.6782, 0.3167, 0.1409],\n",
      "        [0.2312, 0.7411, 0.2493, 0.1245],\n",
      "        [0.3620, 0.6554, 0.3015, 0.0917],\n",
      "        [0.7175, 0.0443, 0.2490, 0.0883]])\n",
      "xyxy: tensor([[2.7806e+02, 2.0377e+02, 4.5463e+02, 3.3142e+02],\n",
      "        [9.3073e+01, 1.0013e+02, 1.7660e+02, 1.9398e+02],\n",
      "        [8.0740e+01, 2.5932e+02, 2.3669e+02, 2.9672e+02],\n",
      "        [2.9705e+02, 1.1329e+02, 4.4696e+02, 1.7392e+02],\n",
      "        [2.5995e+02, 8.3634e+01, 3.7116e+02, 1.3029e+02],\n",
      "        [4.7116e+01, 1.4961e+02, 1.2537e+02, 2.3252e+02],\n",
      "        [1.2999e+02, 3.5858e+02, 2.5845e+02, 3.9945e+02],\n",
      "        [1.0042e+02, 2.9171e+02, 2.5242e+02, 3.5936e+02],\n",
      "        [5.1166e+01, 3.2583e+02, 1.7083e+02, 3.8560e+02],\n",
      "        [1.0137e+02, 2.9258e+02, 2.4611e+02, 3.3657e+02],\n",
      "        [2.8466e+02, 7.7499e-02, 4.0418e+02, 4.2449e+01]])\n",
      "xyxyn: tensor([[5.7929e-01, 4.2451e-01, 9.4714e-01, 6.9046e-01],\n",
      "        [1.9390e-01, 2.0861e-01, 3.6791e-01, 4.0412e-01],\n",
      "        [1.6821e-01, 5.4025e-01, 4.9311e-01, 6.1817e-01],\n",
      "        [6.1884e-01, 2.3601e-01, 9.3116e-01, 3.6233e-01],\n",
      "        [5.4155e-01, 1.7424e-01, 7.7325e-01, 2.7144e-01],\n",
      "        [9.8159e-02, 3.1170e-01, 2.6120e-01, 4.8441e-01],\n",
      "        [2.7082e-01, 7.4704e-01, 5.3843e-01, 8.3220e-01],\n",
      "        [2.0922e-01, 6.0773e-01, 5.2588e-01, 7.4866e-01],\n",
      "        [1.0660e-01, 6.7880e-01, 3.5589e-01, 8.0334e-01],\n",
      "        [2.1118e-01, 6.0953e-01, 5.1272e-01, 7.0120e-01],\n",
      "        [5.9305e-01, 1.6146e-04, 8.4204e-01, 8.8435e-02]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/5b9bd9351f00002c002100a3_jpeg.rf.828c3ec3faf48a09dd4bbd6ded4c937f.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.6740, 0.5960])\n",
      "data: tensor([[223.1641, 257.2995, 269.9712, 318.0333,   0.6740,   2.0000],\n",
      "        [234.0351, 307.1235, 274.0934, 377.3228,   0.5960,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[246.5676, 287.6664,  46.8071,  60.7338],\n",
      "        [254.0642, 342.2231,  40.0583,  70.1993]])\n",
      "xywhn: tensor([[0.5137, 0.5993, 0.0975, 0.1265],\n",
      "        [0.5293, 0.7130, 0.0835, 0.1462]])\n",
      "xyxy: tensor([[223.1641, 257.2995, 269.9712, 318.0333],\n",
      "        [234.0351, 307.1235, 274.0934, 377.3228]])\n",
      "xyxyn: tensor([[0.4649, 0.5360, 0.5624, 0.6626],\n",
      "        [0.4876, 0.6398, 0.5710, 0.7861]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240531_092725_Chrome_jpg.rf.b20d5c6254aecfad2709f702c15e0e24.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "conf: tensor([0.9189, 0.9040, 0.8531, 0.8342, 0.7884, 0.7593, 0.7340, 0.7182, 0.7128, 0.6840, 0.6518, 0.5385, 0.5197, 0.4845, 0.3594, 0.3257, 0.2709])\n",
      "data: tensor([[1.9918e+02, 1.5118e+02, 3.3439e+02, 2.2303e+02, 9.1893e-01, 2.0000e+00],\n",
      "        [3.3874e+02, 1.7926e+02, 4.7878e+02, 2.4760e+02, 9.0404e-01, 2.0000e+00],\n",
      "        [3.9452e+02, 8.0209e+01, 4.7113e+02, 1.5222e+02, 8.5307e-01, 2.0000e+00],\n",
      "        [1.2787e+02, 2.3816e+02, 1.9278e+02, 3.0063e+02, 8.3424e-01, 2.0000e+00],\n",
      "        [3.2937e+00, 1.1863e+02, 9.2398e+01, 1.5839e+02, 7.8842e-01, 2.0000e+00],\n",
      "        [1.7159e+02, 8.0648e+01, 2.4350e+02, 1.5648e+02, 7.5930e-01, 2.0000e+00],\n",
      "        [1.2461e+00, 8.2418e+01, 9.4243e+01, 1.1947e+02, 7.3403e-01, 2.0000e+00],\n",
      "        [2.8138e+02, 2.2363e+02, 3.5244e+02, 3.0266e+02, 7.1816e-01, 2.0000e+00],\n",
      "        [3.1420e+02, 1.1924e+02, 3.9891e+02, 1.8906e+02, 7.1285e-01, 2.0000e+00],\n",
      "        [2.0518e+00, 1.5562e+02, 1.1629e+02, 2.0722e+02, 6.8402e-01, 2.0000e+00],\n",
      "        [8.8940e-01, 2.3982e+02, 7.0058e+01, 2.8435e+02, 6.5177e-01, 2.0000e+00],\n",
      "        [2.4328e+02, 8.1010e+01, 3.1124e+02, 1.5436e+02, 5.3850e-01, 2.0000e+00],\n",
      "        [1.8964e+02, 2.2631e+02, 2.8628e+02, 2.9925e+02, 5.1969e-01, 2.0000e+00],\n",
      "        [6.9809e+01, 2.3916e+02, 1.3148e+02, 3.0189e+02, 4.8454e-01, 2.0000e+00],\n",
      "        [3.4489e+02, 2.4096e+02, 4.7955e+02, 3.0024e+02, 3.5943e-01, 2.0000e+00],\n",
      "        [9.7375e+01, 8.1195e+01, 1.7030e+02, 1.5450e+02, 3.2574e-01, 2.0000e+00],\n",
      "        [3.0800e+02, 8.1101e+01, 3.9786e+02, 1.1975e+02, 2.7092e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([17, 6])\n",
      "xywh: tensor([[266.7877, 187.1027, 135.2129,  71.8495],\n",
      "        [408.7626, 213.4280, 140.0431,  68.3361],\n",
      "        [432.8245, 116.2156,  76.6118,  72.0134],\n",
      "        [160.3245, 269.3956,  64.9081,  62.4753],\n",
      "        [ 47.8458, 138.5089,  89.1043,  39.7604],\n",
      "        [207.5466, 118.5661,  71.9053,  75.8367],\n",
      "        [ 47.7445, 100.9461,  92.9968,  37.0569],\n",
      "        [316.9101, 263.1463,  71.0594,  79.0283],\n",
      "        [356.5574, 154.1506,  84.7068,  69.8246],\n",
      "        [ 59.1694, 181.4221, 114.2353,  51.5965],\n",
      "        [ 35.4737, 262.0870,  69.1686,  44.5314],\n",
      "        [277.2629, 117.6838,  67.9622,  73.3480],\n",
      "        [237.9564, 262.7769,  96.6375,  72.9369],\n",
      "        [100.6455, 270.5258,  61.6737,  62.7277],\n",
      "        [412.2192, 270.5986, 134.6616,  59.2748],\n",
      "        [133.8364, 117.8482,  72.9229,  73.3065],\n",
      "        [352.9293, 100.4265,  89.8604,  38.6505]])\n",
      "xywhn: tensor([[0.5558, 0.3898, 0.2817, 0.1497],\n",
      "        [0.8516, 0.4446, 0.2918, 0.1424],\n",
      "        [0.9017, 0.2421, 0.1596, 0.1500],\n",
      "        [0.3340, 0.5612, 0.1352, 0.1302],\n",
      "        [0.0997, 0.2886, 0.1856, 0.0828],\n",
      "        [0.4324, 0.2470, 0.1498, 0.1580],\n",
      "        [0.0995, 0.2103, 0.1937, 0.0772],\n",
      "        [0.6602, 0.5482, 0.1480, 0.1646],\n",
      "        [0.7428, 0.3211, 0.1765, 0.1455],\n",
      "        [0.1233, 0.3780, 0.2380, 0.1075],\n",
      "        [0.0739, 0.5460, 0.1441, 0.0928],\n",
      "        [0.5776, 0.2452, 0.1416, 0.1528],\n",
      "        [0.4957, 0.5475, 0.2013, 0.1520],\n",
      "        [0.2097, 0.5636, 0.1285, 0.1307],\n",
      "        [0.8588, 0.5637, 0.2805, 0.1235],\n",
      "        [0.2788, 0.2455, 0.1519, 0.1527],\n",
      "        [0.7353, 0.2092, 0.1872, 0.0805]])\n",
      "xyxy: tensor([[199.1812, 151.1779, 334.3941, 223.0274],\n",
      "        [338.7410, 179.2600, 478.7841, 247.5961],\n",
      "        [394.5186,  80.2089, 471.1304, 152.2223],\n",
      "        [127.8704, 238.1579, 192.7785, 300.6332],\n",
      "        [  3.2937, 118.6287,  92.3980, 158.3891],\n",
      "        [171.5940,  80.6477, 243.4993, 156.4844],\n",
      "        [  1.2461,  82.4176,  94.2430, 119.4745],\n",
      "        [281.3804, 223.6322, 352.4398, 302.6605],\n",
      "        [314.2040, 119.2383, 398.9108, 189.0629],\n",
      "        [  2.0518, 155.6239, 116.2870, 207.2203],\n",
      "        [  0.8894, 239.8213,  70.0580, 284.3527],\n",
      "        [243.2818,  81.0098, 311.2440, 154.3578],\n",
      "        [189.6376, 226.3085, 286.2751, 299.2454],\n",
      "        [ 69.8087, 239.1620, 131.4824, 301.8897],\n",
      "        [344.8884, 240.9612, 479.5500, 300.2360],\n",
      "        [ 97.3750,  81.1949, 170.2979, 154.5015],\n",
      "        [307.9991,  81.1012, 397.8595, 119.7517]])\n",
      "xyxyn: tensor([[0.4150, 0.3150, 0.6967, 0.4646],\n",
      "        [0.7057, 0.3735, 0.9975, 0.5158],\n",
      "        [0.8219, 0.1671, 0.9815, 0.3171],\n",
      "        [0.2664, 0.4962, 0.4016, 0.6263],\n",
      "        [0.0069, 0.2471, 0.1925, 0.3300],\n",
      "        [0.3575, 0.1680, 0.5073, 0.3260],\n",
      "        [0.0026, 0.1717, 0.1963, 0.2489],\n",
      "        [0.5862, 0.4659, 0.7342, 0.6305],\n",
      "        [0.6546, 0.2484, 0.8311, 0.3939],\n",
      "        [0.0043, 0.3242, 0.2423, 0.4317],\n",
      "        [0.0019, 0.4996, 0.1460, 0.5924],\n",
      "        [0.5068, 0.1688, 0.6484, 0.3216],\n",
      "        [0.3951, 0.4715, 0.5964, 0.6234],\n",
      "        [0.1454, 0.4983, 0.2739, 0.6289],\n",
      "        [0.7185, 0.5020, 0.9991, 0.6255],\n",
      "        [0.2029, 0.1692, 0.3548, 0.3219],\n",
      "        [0.6417, 0.1690, 0.8289, 0.2495]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150324_Pinterest_jpg.rf.afc10224901d4c8f0a634bc99a768535.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.7938, 0.7854, 0.7636, 0.6824, 0.6248, 0.6051, 0.5971, 0.5862, 0.5466, 0.4869, 0.4543, 0.2760])\n",
      "data: tensor([[6.7029e+01, 8.3124e+01, 2.3241e+02, 1.8938e+02, 7.9379e-01, 0.0000e+00],\n",
      "        [6.7871e+01, 2.2709e+02, 2.1697e+02, 2.7082e+02, 7.8536e-01, 0.0000e+00],\n",
      "        [2.5316e+01, 2.6420e+02, 1.9057e+02, 3.1136e+02, 7.6363e-01, 0.0000e+00],\n",
      "        [4.0821e+02, 2.0076e+02, 4.7393e+02, 2.3653e+02, 6.8236e-01, 0.0000e+00],\n",
      "        [3.2177e+02, 1.4715e+02, 4.0176e+02, 1.9168e+02, 6.2484e-01, 0.0000e+00],\n",
      "        [2.5189e+02, 1.8808e+02, 3.1812e+02, 2.2458e+02, 6.0515e-01, 0.0000e+00],\n",
      "        [4.1132e+02, 1.1187e+02, 4.7877e+02, 1.6003e+02, 5.9711e-01, 0.0000e+00],\n",
      "        [3.3304e+02, 1.0699e+02, 4.0321e+02, 1.4620e+02, 5.8619e-01, 0.0000e+00],\n",
      "        [4.1394e+02, 1.5719e+02, 4.7647e+02, 1.9786e+02, 5.4658e-01, 0.0000e+00],\n",
      "        [2.4257e+02, 2.4615e+02, 2.9955e+02, 2.7196e+02, 4.8695e-01, 0.0000e+00],\n",
      "        [2.5720e+02, 1.4117e+02, 3.1955e+02, 1.8484e+02, 4.5426e-01, 0.0000e+00],\n",
      "        [3.2400e+02, 1.9283e+02, 3.9510e+02, 2.2816e+02, 2.7604e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([12, 6])\n",
      "xywh: tensor([[149.7202, 136.2525, 165.3829, 106.2570],\n",
      "        [142.4211, 248.9544, 149.0994,  43.7323],\n",
      "        [107.9447, 287.7843, 165.2580,  47.1597],\n",
      "        [441.0691, 218.6495,  65.7239,  35.7703],\n",
      "        [361.7683, 169.4129,  79.9919,  44.5347],\n",
      "        [285.0050, 206.3286,  66.2292,  36.4967],\n",
      "        [445.0430, 135.9497,  67.4490,  48.1575],\n",
      "        [368.1262, 126.5948,  70.1742,  39.2092],\n",
      "        [445.2047, 177.5254,  62.5345,  40.6656],\n",
      "        [271.0582, 259.0525,  56.9770,  25.8115],\n",
      "        [288.3748, 163.0042,  62.3496,  43.6668],\n",
      "        [359.5475, 210.4927,  71.1013,  35.3326]])\n",
      "xywhn: tensor([[0.3119, 0.2839, 0.3445, 0.2214],\n",
      "        [0.2967, 0.5187, 0.3106, 0.0911],\n",
      "        [0.2249, 0.5996, 0.3443, 0.0982],\n",
      "        [0.9189, 0.4555, 0.1369, 0.0745],\n",
      "        [0.7537, 0.3529, 0.1666, 0.0928],\n",
      "        [0.5938, 0.4299, 0.1380, 0.0760],\n",
      "        [0.9272, 0.2832, 0.1405, 0.1003],\n",
      "        [0.7669, 0.2637, 0.1462, 0.0817],\n",
      "        [0.9275, 0.3698, 0.1303, 0.0847],\n",
      "        [0.5647, 0.5397, 0.1187, 0.0538],\n",
      "        [0.6008, 0.3396, 0.1299, 0.0910],\n",
      "        [0.7491, 0.4385, 0.1481, 0.0736]])\n",
      "xyxy: tensor([[ 67.0288,  83.1240, 232.4117, 189.3810],\n",
      "        [ 67.8715, 227.0882, 216.9708, 270.8205],\n",
      "        [ 25.3156, 264.2044, 190.5737, 311.3641],\n",
      "        [408.2072, 200.7643, 473.9310, 236.5347],\n",
      "        [321.7723, 147.1456, 401.7643, 191.6803],\n",
      "        [251.8904, 188.0802, 318.1196, 224.5770],\n",
      "        [411.3185, 111.8710, 478.7675, 160.0285],\n",
      "        [333.0391, 106.9902, 403.2133, 146.1994],\n",
      "        [413.9374, 157.1926, 476.4720, 197.8582],\n",
      "        [242.5697, 246.1467, 299.5468, 271.9582],\n",
      "        [257.2000, 141.1708, 319.5497, 184.8376],\n",
      "        [323.9968, 192.8264, 395.0981, 228.1590]])\n",
      "xyxyn: tensor([[0.1396, 0.1732, 0.4842, 0.3945],\n",
      "        [0.1414, 0.4731, 0.4520, 0.5642],\n",
      "        [0.0527, 0.5504, 0.3970, 0.6487],\n",
      "        [0.8504, 0.4183, 0.9874, 0.4928],\n",
      "        [0.6704, 0.3066, 0.8370, 0.3993],\n",
      "        [0.5248, 0.3918, 0.6627, 0.4679],\n",
      "        [0.8569, 0.2331, 0.9974, 0.3334],\n",
      "        [0.6938, 0.2229, 0.8400, 0.3046],\n",
      "        [0.8624, 0.3275, 0.9926, 0.4122],\n",
      "        [0.5054, 0.5128, 0.6241, 0.5666],\n",
      "        [0.5358, 0.2941, 0.6657, 0.3851],\n",
      "        [0.6750, 0.4017, 0.8231, 0.4753]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104512_Pinterest_jpg.rf.6b27be1d9b728a30ae0807aba72ce260.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0.])\n",
      "conf: tensor([0.8416, 0.7886, 0.7294, 0.7139])\n",
      "data: tensor([[230.4550,  53.8016, 414.9442, 192.1866,   0.8416,   0.0000],\n",
      "        [258.0700, 221.9385, 448.5278, 373.4387,   0.7886,   0.0000],\n",
      "        [ 82.7385, 246.2718, 214.9459, 356.8403,   0.7294,   0.0000],\n",
      "        [ 14.7775,  99.6189, 212.0085, 221.3966,   0.7139,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[322.6996, 122.9941, 184.4891, 138.3850],\n",
      "        [353.2989, 297.6886, 190.4578, 151.5002],\n",
      "        [148.8422, 301.5561, 132.2074, 110.5685],\n",
      "        [113.3930, 160.5078, 197.2310, 121.7778]])\n",
      "xywhn: tensor([[0.6723, 0.2562, 0.3844, 0.2883],\n",
      "        [0.7360, 0.6202, 0.3968, 0.3156],\n",
      "        [0.3101, 0.6282, 0.2754, 0.2304],\n",
      "        [0.2362, 0.3344, 0.4109, 0.2537]])\n",
      "xyxy: tensor([[230.4550,  53.8016, 414.9442, 192.1866],\n",
      "        [258.0700, 221.9385, 448.5278, 373.4387],\n",
      "        [ 82.7385, 246.2718, 214.9459, 356.8403],\n",
      "        [ 14.7775,  99.6189, 212.0085, 221.3966]])\n",
      "xyxyn: tensor([[0.4801, 0.1121, 0.8645, 0.4004],\n",
      "        [0.5376, 0.4624, 0.9344, 0.7780],\n",
      "        [0.1724, 0.5131, 0.4478, 0.7434],\n",
      "        [0.0308, 0.2075, 0.4417, 0.4612]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/celecoxib-200-MG-4-Copy-Copy_jpg.rf.ee6e7cfae1dfeceb1167b8a95d144414.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9396, 0.9246])\n",
      "data: tensor([[254.0521,  11.9644, 479.3807, 480.0000,   0.9396,   2.0000],\n",
      "        [ 17.3687,  11.6251, 255.3778, 479.9517,   0.9246,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[366.7164, 245.9822, 225.3286, 468.0356],\n",
      "        [136.3732, 245.7884, 238.0091, 468.3266]])\n",
      "xywhn: tensor([[0.7640, 0.5125, 0.4694, 0.9751],\n",
      "        [0.2841, 0.5121, 0.4959, 0.9757]])\n",
      "xyxy: tensor([[254.0521,  11.9644, 479.3807, 480.0000],\n",
      "        [ 17.3687,  11.6251, 255.3778, 479.9517]])\n",
      "xyxyn: tensor([[0.5293, 0.0249, 0.9987, 1.0000],\n",
      "        [0.0362, 0.0242, 0.5320, 0.9999]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151711_Pinterest_jpg.rf.fab56c6cfef6b66389193f7c344d5fee.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8566])\n",
      "data: tensor([[  1.5588,  33.9917, 480.0000, 132.2201,   0.8566,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[240.7794,  83.1059, 478.4412,  98.2285]])\n",
      "xywhn: tensor([[0.5016, 0.1731, 0.9968, 0.2046]])\n",
      "xyxy: tensor([[  1.5588,  33.9917, 480.0000, 132.2201]])\n",
      "xyxyn: tensor([[0.0032, 0.0708, 1.0000, 0.2755]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Oseltamivir-45-MG-7-Copy-Copy_jpg.rf.c617886cb00bc34ff0110ce0d93e819c.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9231, 0.9130])\n",
      "data: tensor([[  1.7132,   4.1315, 231.9881, 465.8149,   0.9231,   2.0000],\n",
      "        [228.5775,   3.8167, 459.2181, 468.1446,   0.9130,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[116.8506, 234.9732, 230.2749, 461.6834],\n",
      "        [343.8978, 235.9807, 230.6406, 464.3279]])\n",
      "xywhn: tensor([[0.2434, 0.4895, 0.4797, 0.9618],\n",
      "        [0.7165, 0.4916, 0.4805, 0.9673]])\n",
      "xyxy: tensor([[  1.7132,   4.1315, 231.9881, 465.8149],\n",
      "        [228.5775,   3.8167, 459.2181, 468.1446]])\n",
      "xyxyn: tensor([[0.0036, 0.0086, 0.4833, 0.9704],\n",
      "        [0.4762, 0.0080, 0.9567, 0.9753]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094539_Pinterest_jpg.rf.52d626bf1e69cc81b4da92d7bda22379.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8143])\n",
      "data: tensor([[ 63.1475,  76.7526, 386.6544, 224.3445,   0.8143,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[224.9009, 150.5485, 323.5068, 147.5919]])\n",
      "xywhn: tensor([[0.4685, 0.3136, 0.6740, 0.3075]])\n",
      "xyxy: tensor([[ 63.1475,  76.7526, 386.6544, 224.3445]])\n",
      "xyxyn: tensor([[0.1316, 0.1599, 0.8055, 0.4674]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/sitagliptin-50-MG-12-Copy_jpg.rf.f48ebe3489a2c3a344b108e6c1766640.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.8952, 0.8821])\n",
      "data: tensor([[4.0075e+01, 2.8645e-01, 3.4539e+02, 2.2834e+02, 8.9520e-01, 2.0000e+00],\n",
      "        [4.1197e+01, 2.2662e+02, 3.4532e+02, 4.6718e+02, 8.8212e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[192.7334, 114.3124, 305.3166, 228.0519],\n",
      "        [193.2599, 346.8976, 304.1266, 240.5588]])\n",
      "xywhn: tensor([[0.4015, 0.2382, 0.6361, 0.4751],\n",
      "        [0.4026, 0.7227, 0.6336, 0.5012]])\n",
      "xyxy: tensor([[4.0075e+01, 2.8645e-01, 3.4539e+02, 2.2834e+02],\n",
      "        [4.1197e+01, 2.2662e+02, 3.4532e+02, 4.6718e+02]])\n",
      "xyxyn: tensor([[8.3490e-02, 5.9678e-04, 7.1957e-01, 4.7570e-01],\n",
      "        [8.5826e-02, 4.7212e-01, 7.1942e-01, 9.7329e-01]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100041_Pinterest_jpg.rf.fc1a2dae50c3dfdfb529ba9da28b723e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8291])\n",
      "data: tensor([[ 19.5919,  18.7990, 351.4626, 329.6069,   0.8291,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[185.5273, 174.2029, 331.8708, 310.8080]])\n",
      "xywhn: tensor([[0.3865, 0.3629, 0.6914, 0.6475]])\n",
      "xyxy: tensor([[ 19.5919,  18.7990, 351.4626, 329.6069]])\n",
      "xyxyn: tensor([[0.0408, 0.0392, 0.7322, 0.6867]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130312_jpg.rf.af07e08c4c97a6db0f4b847ecc738273.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.9027, 0.8949, 0.8021, 0.7557, 0.7096, 0.6470, 0.6265])\n",
      "data: tensor([[126.3950, 240.7571, 176.0150, 336.1088,   0.9027,   1.0000],\n",
      "        [ 84.8818, 246.6485, 136.8041, 328.1103,   0.8949,   1.0000],\n",
      "        [311.3631, 299.9629, 362.9670, 366.2845,   0.8021,   1.0000],\n",
      "        [169.6105, 253.4748, 210.8653, 340.4402,   0.7557,   1.0000],\n",
      "        [270.1604, 217.0425, 284.4300, 284.3419,   0.7096,   1.0000],\n",
      "        [208.1267, 200.0283, 220.7850, 259.3383,   0.6470,   1.0000],\n",
      "        [141.7390, 199.9713, 153.3493, 242.3768,   0.6265,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[151.2050, 288.4330,  49.6199,  95.3516],\n",
      "        [110.8429, 287.3794,  51.9223,  81.4618],\n",
      "        [337.1650, 333.1237,  51.6039,  66.3217],\n",
      "        [190.2379, 296.9575,  41.2549,  86.9654],\n",
      "        [277.2952, 250.6922,  14.2695,  67.2994],\n",
      "        [214.4559, 229.6833,  12.6583,  59.3100],\n",
      "        [147.5442, 221.1740,  11.6104,  42.4055]])\n",
      "xywhn: tensor([[0.3150, 0.6009, 0.1034, 0.1986],\n",
      "        [0.2309, 0.5987, 0.1082, 0.1697],\n",
      "        [0.7024, 0.6940, 0.1075, 0.1382],\n",
      "        [0.3963, 0.6187, 0.0859, 0.1812],\n",
      "        [0.5777, 0.5223, 0.0297, 0.1402],\n",
      "        [0.4468, 0.4785, 0.0264, 0.1236],\n",
      "        [0.3074, 0.4608, 0.0242, 0.0883]])\n",
      "xyxy: tensor([[126.3950, 240.7571, 176.0150, 336.1088],\n",
      "        [ 84.8818, 246.6485, 136.8041, 328.1103],\n",
      "        [311.3631, 299.9629, 362.9670, 366.2845],\n",
      "        [169.6105, 253.4748, 210.8653, 340.4402],\n",
      "        [270.1604, 217.0425, 284.4300, 284.3419],\n",
      "        [208.1267, 200.0283, 220.7850, 259.3383],\n",
      "        [141.7390, 199.9713, 153.3493, 242.3768]])\n",
      "xyxyn: tensor([[0.2633, 0.5016, 0.3667, 0.7002],\n",
      "        [0.1768, 0.5139, 0.2850, 0.6836],\n",
      "        [0.6487, 0.6249, 0.7562, 0.7631],\n",
      "        [0.3534, 0.5281, 0.4393, 0.7093],\n",
      "        [0.5628, 0.4522, 0.5926, 0.5924],\n",
      "        [0.4336, 0.4167, 0.4600, 0.5403],\n",
      "        [0.2953, 0.4166, 0.3195, 0.5050]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/pitavastatin-1-MG-3-Copy-Copy_jpg.rf.d1fb1d9c1b7cbdcd9f87d4f5f4b710d5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.8916, 0.8853])\n",
      "data: tensor([[147.2928, 244.4802, 441.3839, 480.0000,   0.8916,   2.0000],\n",
      "        [148.9043,  15.9282, 439.6982, 249.8357,   0.8853,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[294.3383, 362.2401, 294.0911, 235.5198],\n",
      "        [294.3013, 132.8820, 290.7939, 233.9075]])\n",
      "xywhn: tensor([[0.6132, 0.7547, 0.6127, 0.4907],\n",
      "        [0.6131, 0.2768, 0.6058, 0.4873]])\n",
      "xyxy: tensor([[147.2928, 244.4802, 441.3839, 480.0000],\n",
      "        [148.9043,  15.9282, 439.6982, 249.8357]])\n",
      "xyxyn: tensor([[0.3069, 0.5093, 0.9195, 1.0000],\n",
      "        [0.3102, 0.0332, 0.9160, 0.5205]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/pitavastatin-1-MG-20-Copy_jpg.rf.961c2f83b586cca55e47c4606837b961.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.8952, 0.8876])\n",
      "data: tensor([[248.8785,  54.4394, 478.3741, 334.0571,   0.8952,   2.0000],\n",
      "        [ 16.4792,  52.5739, 252.3620, 335.4341,   0.8876,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[363.6263, 194.2482, 229.4956, 279.6177],\n",
      "        [134.4206, 194.0040, 235.8828, 282.8603]])\n",
      "xywhn: tensor([[0.7576, 0.4047, 0.4781, 0.5825],\n",
      "        [0.2800, 0.4042, 0.4914, 0.5893]])\n",
      "xyxy: tensor([[248.8785,  54.4394, 478.3741, 334.0571],\n",
      "        [ 16.4792,  52.5739, 252.3620, 335.4341]])\n",
      "xyxyn: tensor([[0.5185, 0.1134, 0.9966, 0.6960],\n",
      "        [0.0343, 0.1095, 0.5258, 0.6988]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094210_Pinterest_jpg.rf.3c06b2c7cecdd1d29fefcff2963beb37.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8739, 0.8701])\n",
      "data: tensor([[1.5868e+02, 8.1573e+00, 4.7996e+02, 2.0929e+02, 8.7389e-01, 0.0000e+00],\n",
      "        [2.6407e-01, 1.3601e+01, 2.5812e+02, 2.1982e+02, 8.7012e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[319.3232, 108.7224, 321.2801, 201.1302],\n",
      "        [129.1915, 116.7084, 257.8549, 206.2140]])\n",
      "xywhn: tensor([[0.6653, 0.2265, 0.6693, 0.4190],\n",
      "        [0.2691, 0.2431, 0.5372, 0.4296]])\n",
      "xyxy: tensor([[1.5868e+02, 8.1573e+00, 4.7996e+02, 2.0929e+02],\n",
      "        [2.6407e-01, 1.3601e+01, 2.5812e+02, 2.1982e+02]])\n",
      "xyxyn: tensor([[3.3059e-01, 1.6994e-02, 9.9992e-01, 4.3602e-01],\n",
      "        [5.5014e-04, 2.8336e-02, 5.3775e-01, 4.5795e-01]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094425_Pinterest_jpg.rf.e79fddc0f9b03a02f993167bc80ce1b8.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7718])\n",
      "data: tensor([[ 12.3986,  27.5844, 463.2517, 412.9797,   0.7718,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[237.8251, 220.2821, 450.8531, 385.3954]])\n",
      "xywhn: tensor([[0.4955, 0.4589, 0.9393, 0.8029]])\n",
      "xyxy: tensor([[ 12.3986,  27.5844, 463.2517, 412.9797]])\n",
      "xyxyn: tensor([[0.0258, 0.0575, 0.9651, 0.8604]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240523_104248_jpg.rf.6b08e7fde9eafb84f00bc6d2f05ca068.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.6710])\n",
      "data: tensor([[ 85.7798, 148.9913, 394.2775, 383.3866,   0.6710,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[240.0287, 266.1889, 308.4977, 234.3953]])\n",
      "xywhn: tensor([[0.5001, 0.5546, 0.6427, 0.4883]])\n",
      "xyxy: tensor([[ 85.7798, 148.9913, 394.2775, 383.3866]])\n",
      "xyxyn: tensor([[0.1787, 0.3104, 0.8214, 0.7987]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100750_Pinterest_jpg.rf.1d162cf18fa96c717c71a52462c51c8d.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8955, 0.8452])\n",
      "data: tensor([[ 29.2261,  22.3105, 234.4360, 157.0077,   0.8955,   0.0000],\n",
      "        [234.3239,  20.7053, 441.6737, 155.2934,   0.8452,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[131.8310,  89.6591, 205.2099, 134.6972],\n",
      "        [337.9988,  87.9993, 207.3499, 134.5880]])\n",
      "xywhn: tensor([[0.2746, 0.1868, 0.4275, 0.2806],\n",
      "        [0.7042, 0.1833, 0.4320, 0.2804]])\n",
      "xyxy: tensor([[ 29.2261,  22.3105, 234.4360, 157.0077],\n",
      "        [234.3239,  20.7053, 441.6737, 155.2934]])\n",
      "xyxyn: tensor([[0.0609, 0.0465, 0.4884, 0.3271],\n",
      "        [0.4882, 0.0431, 0.9202, 0.3235]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240531_093054_Chrome_jpg.rf.948bcdfe2d2525ea25885f35eec2e791.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 3., 3., 3., 3., 3., 3.])\n",
      "conf: tensor([0.8623, 0.8533, 0.8360, 0.8085, 0.7736, 0.7301, 0.6085])\n",
      "data: tensor([[245.6714, 104.1813, 467.4463, 186.8181,   0.8623,   3.0000],\n",
      "        [245.5637, 242.0759, 466.5829, 323.0289,   0.8533,   3.0000],\n",
      "        [ 18.7518, 221.3939, 156.7274, 266.8056,   0.8360,   3.0000],\n",
      "        [247.8979, 375.1272, 466.8919, 451.1225,   0.8085,   3.0000],\n",
      "        [ 29.6497,  73.1349, 231.4727, 142.4014,   0.7736,   3.0000],\n",
      "        [ 15.5607, 357.2576, 161.3225, 402.0871,   0.7301,   3.0000],\n",
      "        [ 65.5155, 384.5714, 222.4234, 441.5171,   0.6085,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[356.5589, 145.4997, 221.7748,  82.6368],\n",
      "        [356.0733, 282.5524, 221.0192,  80.9530],\n",
      "        [ 87.7396, 244.0998, 137.9756,  45.4117],\n",
      "        [357.3949, 413.1248, 218.9941,  75.9952],\n",
      "        [130.5612, 107.7682, 201.8230,  69.2665],\n",
      "        [ 88.4416, 379.6724, 145.7618,  44.8295],\n",
      "        [143.9694, 413.0443, 156.9079,  56.9457]])\n",
      "xywhn: tensor([[0.7428, 0.3031, 0.4620, 0.1722],\n",
      "        [0.7418, 0.5887, 0.4605, 0.1687],\n",
      "        [0.1828, 0.5085, 0.2874, 0.0946],\n",
      "        [0.7446, 0.8607, 0.4562, 0.1583],\n",
      "        [0.2720, 0.2245, 0.4205, 0.1443],\n",
      "        [0.1843, 0.7910, 0.3037, 0.0934],\n",
      "        [0.2999, 0.8605, 0.3269, 0.1186]])\n",
      "xyxy: tensor([[245.6714, 104.1813, 467.4463, 186.8181],\n",
      "        [245.5637, 242.0759, 466.5829, 323.0289],\n",
      "        [ 18.7518, 221.3939, 156.7274, 266.8056],\n",
      "        [247.8979, 375.1272, 466.8919, 451.1225],\n",
      "        [ 29.6497,  73.1349, 231.4727, 142.4014],\n",
      "        [ 15.5607, 357.2576, 161.3225, 402.0871],\n",
      "        [ 65.5155, 384.5714, 222.4234, 441.5171]])\n",
      "xyxyn: tensor([[0.5118, 0.2170, 0.9738, 0.3892],\n",
      "        [0.5116, 0.5043, 0.9720, 0.6730],\n",
      "        [0.0391, 0.4612, 0.3265, 0.5558],\n",
      "        [0.5165, 0.7815, 0.9727, 0.9398],\n",
      "        [0.0618, 0.1524, 0.4822, 0.2967],\n",
      "        [0.0324, 0.7443, 0.3361, 0.8377],\n",
      "        [0.1365, 0.8012, 0.4634, 0.9198]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094155_Pinterest_jpg.rf.51e80075426b80a6ea3fbdf399698b21.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8870, 0.8661])\n",
      "data: tensor([[227.4096,   6.3070, 461.3239, 186.5988,   0.8870,   0.0000],\n",
      "        [ 27.8883, 158.3765, 307.2060, 337.8002,   0.8661,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[344.3668,  96.4529, 233.9143, 180.2917],\n",
      "        [167.5472, 248.0883, 279.3177, 179.4237]])\n",
      "xywhn: tensor([[0.7174, 0.2009, 0.4873, 0.3756],\n",
      "        [0.3491, 0.5169, 0.5819, 0.3738]])\n",
      "xyxy: tensor([[227.4096,   6.3070, 461.3239, 186.5988],\n",
      "        [ 27.8883, 158.3765, 307.2060, 337.8002]])\n",
      "xyxyn: tensor([[0.4738, 0.0131, 0.9611, 0.3887],\n",
      "        [0.0581, 0.3300, 0.6400, 0.7038]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130509_jpg.rf.f97975cd300274d5dcb57134a5018730.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.9098, 0.8162, 0.8081, 0.7961, 0.7302, 0.6960, 0.6196, 0.5953])\n",
      "data: tensor([[238.1968, 230.9559, 356.1115, 284.6981,   0.9098,   1.0000],\n",
      "        [173.5633, 298.9415, 314.0457, 382.5740,   0.8162,   1.0000],\n",
      "        [169.9616, 164.3392, 248.2562, 211.3374,   0.8081,   1.0000],\n",
      "        [286.0442, 127.9349, 389.3338, 210.6766,   0.7961,   1.0000],\n",
      "        [284.7061, 165.5549, 388.1880, 209.7236,   0.7302,   1.0000],\n",
      "        [101.7687, 229.2541, 179.2092, 281.2130,   0.6960,   1.0000],\n",
      "        [244.9792,  91.5103, 311.4166, 131.0854,   0.6196,   1.0000],\n",
      "        [246.2318,  65.7144, 311.5850, 131.6950,   0.5953,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([8, 6])\n",
      "xywh: tensor([[297.1541, 257.8270, 117.9147,  53.7421],\n",
      "        [243.8045, 340.7578, 140.4824,  83.6324],\n",
      "        [209.1089, 187.8383,  78.2946,  46.9982],\n",
      "        [337.6890, 169.3057, 103.2896,  82.7418],\n",
      "        [336.4471, 187.6393, 103.4819,  44.1687],\n",
      "        [140.4889, 255.2336,  77.4405,  51.9589],\n",
      "        [278.1979, 111.2979,  66.4374,  39.5751],\n",
      "        [278.9084,  98.7047,  65.3532,  65.9806]])\n",
      "xywhn: tensor([[0.6191, 0.5371, 0.2457, 0.1120],\n",
      "        [0.5079, 0.7099, 0.2927, 0.1742],\n",
      "        [0.4356, 0.3913, 0.1631, 0.0979],\n",
      "        [0.7035, 0.3527, 0.2152, 0.1724],\n",
      "        [0.7009, 0.3909, 0.2156, 0.0920],\n",
      "        [0.2927, 0.5317, 0.1613, 0.1082],\n",
      "        [0.5796, 0.2319, 0.1384, 0.0824],\n",
      "        [0.5811, 0.2056, 0.1362, 0.1375]])\n",
      "xyxy: tensor([[238.1968, 230.9559, 356.1115, 284.6981],\n",
      "        [173.5633, 298.9415, 314.0457, 382.5740],\n",
      "        [169.9616, 164.3392, 248.2562, 211.3374],\n",
      "        [286.0442, 127.9349, 389.3338, 210.6766],\n",
      "        [284.7061, 165.5549, 388.1880, 209.7236],\n",
      "        [101.7687, 229.2541, 179.2092, 281.2130],\n",
      "        [244.9792,  91.5103, 311.4166, 131.0854],\n",
      "        [246.2318,  65.7144, 311.5850, 131.6950]])\n",
      "xyxyn: tensor([[0.4962, 0.4812, 0.7419, 0.5931],\n",
      "        [0.3616, 0.6228, 0.6543, 0.7970],\n",
      "        [0.3541, 0.3424, 0.5172, 0.4403],\n",
      "        [0.5959, 0.2665, 0.8111, 0.4389],\n",
      "        [0.5931, 0.3449, 0.8087, 0.4369],\n",
      "        [0.2120, 0.4776, 0.3734, 0.5859],\n",
      "        [0.5104, 0.1906, 0.6488, 0.2731],\n",
      "        [0.5130, 0.1369, 0.6491, 0.2744]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151222_Pinterest_jpg.rf.f14e85be13c0b1ea64c4a4d7039e6074.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.9012, 0.8813, 0.8785, 0.8516, 0.8507, 0.8489, 0.8102, 0.7955, 0.7930, 0.7526])\n",
      "data: tensor([[109.3572,  96.0274, 201.3123, 154.5760,   0.9012,   0.0000],\n",
      "        [ 11.6695,  95.9183, 101.8656, 158.3691,   0.8813,   0.0000],\n",
      "        [292.9124,  95.1850, 377.0522, 157.1177,   0.8785,   0.0000],\n",
      "        [285.4742,  30.1167, 380.6922,  87.7186,   0.8516,   0.0000],\n",
      "        [215.0172,  96.6530, 283.9543, 158.3988,   0.8507,   0.0000],\n",
      "        [ 10.0025,  30.8274,  98.8056,  86.6325,   0.8489,   0.0000],\n",
      "        [105.1973,  28.9808, 197.0386,  84.9568,   0.8102,   0.0000],\n",
      "        [391.2160,  31.0479, 469.4188,  89.0947,   0.7955,   0.0000],\n",
      "        [395.7523,  95.9108, 463.5231, 152.5709,   0.7930,   0.0000],\n",
      "        [207.4710,  36.9990, 277.3978,  85.5961,   0.7526,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([10, 6])\n",
      "xywh: tensor([[155.3347, 125.3017,  91.9552,  58.5486],\n",
      "        [ 56.7676, 127.1437,  90.1961,  62.4508],\n",
      "        [334.9823, 126.1514,  84.1399,  61.9328],\n",
      "        [333.0832,  58.9176,  95.2180,  57.6019],\n",
      "        [249.4857, 127.5259,  68.9370,  61.7458],\n",
      "        [ 54.4041,  58.7299,  88.8031,  55.8051],\n",
      "        [151.1180,  56.9688,  91.8413,  55.9761],\n",
      "        [430.3174,  60.0713,  78.2028,  58.0467],\n",
      "        [429.6377, 124.2409,  67.7708,  56.6601],\n",
      "        [242.4344,  61.2976,  69.9268,  48.5971]])\n",
      "xywhn: tensor([[0.3236, 0.2610, 0.1916, 0.1220],\n",
      "        [0.1183, 0.2649, 0.1879, 0.1301],\n",
      "        [0.6979, 0.2628, 0.1753, 0.1290],\n",
      "        [0.6939, 0.1227, 0.1984, 0.1200],\n",
      "        [0.5198, 0.2657, 0.1436, 0.1286],\n",
      "        [0.1133, 0.1224, 0.1850, 0.1163],\n",
      "        [0.3148, 0.1187, 0.1913, 0.1166],\n",
      "        [0.8965, 0.1251, 0.1629, 0.1209],\n",
      "        [0.8951, 0.2588, 0.1412, 0.1180],\n",
      "        [0.5051, 0.1277, 0.1457, 0.1012]])\n",
      "xyxy: tensor([[109.3572,  96.0274, 201.3123, 154.5760],\n",
      "        [ 11.6695,  95.9183, 101.8656, 158.3691],\n",
      "        [292.9124,  95.1850, 377.0522, 157.1177],\n",
      "        [285.4742,  30.1167, 380.6922,  87.7186],\n",
      "        [215.0172,  96.6530, 283.9543, 158.3988],\n",
      "        [ 10.0025,  30.8274,  98.8056,  86.6325],\n",
      "        [105.1973,  28.9808, 197.0386,  84.9568],\n",
      "        [391.2160,  31.0479, 469.4188,  89.0947],\n",
      "        [395.7523,  95.9108, 463.5231, 152.5709],\n",
      "        [207.4710,  36.9990, 277.3978,  85.5961]])\n",
      "xyxyn: tensor([[0.2278, 0.2001, 0.4194, 0.3220],\n",
      "        [0.0243, 0.1998, 0.2122, 0.3299],\n",
      "        [0.6102, 0.1983, 0.7855, 0.3273],\n",
      "        [0.5947, 0.0627, 0.7931, 0.1827],\n",
      "        [0.4480, 0.2014, 0.5916, 0.3300],\n",
      "        [0.0208, 0.0642, 0.2058, 0.1805],\n",
      "        [0.2192, 0.0604, 0.4105, 0.1770],\n",
      "        [0.8150, 0.0647, 0.9780, 0.1856],\n",
      "        [0.8245, 0.1998, 0.9657, 0.3179],\n",
      "        [0.4322, 0.0771, 0.5779, 0.1783]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/sitagliptin-50-MG-13-Copy-Copy_jpg.rf.441a7679b25fb28d5cdbba319ed3d606.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2.])\n",
      "conf: tensor([0.9504])\n",
      "data: tensor([[  0.0000,   1.9010, 478.7375, 479.7419,   0.9504,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[239.3688, 240.8214, 478.7375, 477.8409]])\n",
      "xywhn: tensor([[0.4987, 0.5017, 0.9974, 0.9955]])\n",
      "xyxy: tensor([[  0.0000,   1.9010, 478.7375, 479.7419]])\n",
      "xyxyn: tensor([[0.0000, 0.0040, 0.9974, 0.9995]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_101834_Pinterest_jpg.rf.00558def6da53398dcd995b2706d3405.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8758])\n",
      "data: tensor([[ 36.7393,  63.6214, 439.4617, 208.2537,   0.8758,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[238.1005, 135.9376, 402.7224, 144.6322]])\n",
      "xywhn: tensor([[0.4960, 0.2832, 0.8390, 0.3013]])\n",
      "xyxy: tensor([[ 36.7393,  63.6214, 439.4617, 208.2537]])\n",
      "xyxyn: tensor([[0.0765, 0.1325, 0.9155, 0.4339]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/aprepitant-80-MG-28-Copy_jpg.rf.58fe869bdb13b1c7767c0c2d53f3c0de.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9233, 0.9135])\n",
      "data: tensor([[  3.0836,   0.8032, 230.8618, 467.3047,   0.9233,   2.0000],\n",
      "        [229.7838,   1.1339, 460.0854, 472.8521,   0.9135,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[116.9727, 234.0540, 227.7782, 466.5016],\n",
      "        [344.9346, 236.9930, 230.3016, 471.7182]])\n",
      "xywhn: tensor([[0.2437, 0.4876, 0.4745, 0.9719],\n",
      "        [0.7186, 0.4937, 0.4798, 0.9827]])\n",
      "xyxy: tensor([[  3.0836,   0.8032, 230.8618, 467.3047],\n",
      "        [229.7838,   1.1339, 460.0854, 472.8521]])\n",
      "xyxyn: tensor([[0.0064, 0.0017, 0.4810, 0.9736],\n",
      "        [0.4787, 0.0024, 0.9585, 0.9851]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100355_Pinterest_jpg.rf.26c5c6adbfc3463f2baa905d550dd32d.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8059, 0.6980, 0.6448, 0.6312, 0.5704, 0.3901])\n",
      "data: tensor([[2.6323e+02, 7.9484e+01, 4.4305e+02, 2.1466e+02, 8.0592e-01, 0.0000e+00],\n",
      "        [7.0295e+01, 1.6798e+02, 2.1146e+02, 2.6831e+02, 6.9804e-01, 0.0000e+00],\n",
      "        [5.1512e+01, 1.2973e+01, 2.0608e+02, 1.1003e+02, 6.4479e-01, 0.0000e+00],\n",
      "        [2.9025e+02, 2.5181e+02, 4.3123e+02, 3.5107e+02, 6.3121e-01, 0.0000e+00],\n",
      "        [7.2898e+01, 1.6690e+02, 1.7289e+02, 2.6983e+02, 5.7042e-01, 0.0000e+00],\n",
      "        [2.7796e+01, 3.3478e+02, 1.4946e+02, 4.2638e+02, 3.9012e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[353.1406, 147.0710, 179.8125, 135.1734],\n",
      "        [140.8790, 218.1454, 141.1671, 100.3225],\n",
      "        [128.7963,  61.5021, 154.5687,  97.0575],\n",
      "        [360.7403, 301.4418, 140.9829,  99.2650],\n",
      "        [122.8936, 218.3665,  99.9922, 102.9369],\n",
      "        [ 88.6297, 380.5805, 121.6680,  91.5995]])\n",
      "xywhn: tensor([[0.7357, 0.3064, 0.3746, 0.2816],\n",
      "        [0.2935, 0.4545, 0.2941, 0.2090],\n",
      "        [0.2683, 0.1281, 0.3220, 0.2022],\n",
      "        [0.7515, 0.6280, 0.2937, 0.2068],\n",
      "        [0.2560, 0.4549, 0.2083, 0.2145],\n",
      "        [0.1846, 0.7929, 0.2535, 0.1908]])\n",
      "xyxy: tensor([[263.2343,  79.4843, 443.0468, 214.6577],\n",
      "        [ 70.2955, 167.9841, 211.4626, 268.3066],\n",
      "        [ 51.5119,  12.9733, 206.0806, 110.0308],\n",
      "        [290.2488, 251.8093, 431.2318, 351.0743],\n",
      "        [ 72.8976, 166.8980, 172.8897, 269.8349],\n",
      "        [ 27.7957, 334.7808, 149.4637, 426.3802]])\n",
      "xyxyn: tensor([[0.5484, 0.1656, 0.9230, 0.4472],\n",
      "        [0.1464, 0.3500, 0.4405, 0.5590],\n",
      "        [0.1073, 0.0270, 0.4293, 0.2292],\n",
      "        [0.6047, 0.5246, 0.8984, 0.7314],\n",
      "        [0.1519, 0.3477, 0.3602, 0.5622],\n",
      "        [0.0579, 0.6975, 0.3114, 0.8883]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151008_Pinterest_jpg.rf.81487bb0a24823071245217e7e0ec0bf.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8818, 0.8475, 0.8209, 0.7915, 0.7661, 0.7335, 0.5935, 0.5799])\n",
      "data: tensor([[346.2100, 164.9653, 398.1874, 214.5512,   0.8818,   0.0000],\n",
      "        [ 51.0698, 105.1791, 204.9209, 250.6924,   0.8475,   0.0000],\n",
      "        [ 37.6217, 274.6834, 183.8613, 373.5846,   0.8209,   0.0000],\n",
      "        [334.2403,  37.9842, 421.0146,  74.6871,   0.7915,   0.0000],\n",
      "        [311.1716, 169.2503, 357.8813, 210.6787,   0.7661,   0.0000],\n",
      "        [328.9999, 361.6855, 383.4285, 405.5999,   0.7335,   0.0000],\n",
      "        [335.4954, 331.7963, 390.1994, 367.7353,   0.5935,   0.0000],\n",
      "        [295.5674,  17.5766, 370.3555,  65.7249,   0.5799,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([8, 6])\n",
      "xywh: tensor([[372.1987, 189.7583,  51.9774,  49.5859],\n",
      "        [127.9954, 177.9358, 153.8511, 145.5133],\n",
      "        [110.7415, 324.1340, 146.2396,  98.9012],\n",
      "        [377.6274,  56.3356,  86.7743,  36.7029],\n",
      "        [334.5265, 189.9645,  46.7097,  41.4284],\n",
      "        [356.2142, 383.6427,  54.4285,  43.9144],\n",
      "        [362.8474, 349.7658,  54.7040,  35.9390],\n",
      "        [332.9615,  41.6507,  74.7881,  48.1483]])\n",
      "xywhn: tensor([[0.7754, 0.3953, 0.1083, 0.1033],\n",
      "        [0.2667, 0.3707, 0.3205, 0.3032],\n",
      "        [0.2307, 0.6753, 0.3047, 0.2060],\n",
      "        [0.7867, 0.1174, 0.1808, 0.0765],\n",
      "        [0.6969, 0.3958, 0.0973, 0.0863],\n",
      "        [0.7421, 0.7993, 0.1134, 0.0915],\n",
      "        [0.7559, 0.7287, 0.1140, 0.0749],\n",
      "        [0.6937, 0.0868, 0.1558, 0.1003]])\n",
      "xyxy: tensor([[346.2100, 164.9653, 398.1874, 214.5512],\n",
      "        [ 51.0698, 105.1791, 204.9209, 250.6924],\n",
      "        [ 37.6217, 274.6834, 183.8613, 373.5846],\n",
      "        [334.2403,  37.9842, 421.0146,  74.6871],\n",
      "        [311.1716, 169.2503, 357.8813, 210.6787],\n",
      "        [328.9999, 361.6855, 383.4285, 405.5999],\n",
      "        [335.4954, 331.7963, 390.1994, 367.7353],\n",
      "        [295.5674,  17.5766, 370.3555,  65.7249]])\n",
      "xyxyn: tensor([[0.7213, 0.3437, 0.8296, 0.4470],\n",
      "        [0.1064, 0.2191, 0.4269, 0.5223],\n",
      "        [0.0784, 0.5723, 0.3830, 0.7783],\n",
      "        [0.6963, 0.0791, 0.8771, 0.1556],\n",
      "        [0.6483, 0.3526, 0.7456, 0.4389],\n",
      "        [0.6854, 0.7535, 0.7988, 0.8450],\n",
      "        [0.6989, 0.6912, 0.8129, 0.7661],\n",
      "        [0.6158, 0.0366, 0.7716, 0.1369]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130044_jpg.rf.d73b549ffebb81a1ed908f8780fd3663.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8656, 0.6414])\n",
      "data: tensor([[280.1771,  92.9031, 351.8190, 183.9218,   0.8656,   1.0000],\n",
      "        [371.4372, 196.1425, 404.6929, 282.2893,   0.6414,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[315.9980, 138.4125,  71.6418,  91.0187],\n",
      "        [388.0651, 239.2159,  33.2557,  86.1469]])\n",
      "xywhn: tensor([[0.6583, 0.2884, 0.1493, 0.1896],\n",
      "        [0.8085, 0.4984, 0.0693, 0.1795]])\n",
      "xyxy: tensor([[280.1771,  92.9031, 351.8190, 183.9218],\n",
      "        [371.4372, 196.1425, 404.6929, 282.2893]])\n",
      "xyxyn: tensor([[0.5837, 0.1935, 0.7330, 0.3832],\n",
      "        [0.7738, 0.4086, 0.8431, 0.5881]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240523_084456_jpg.rf.ac6643b426a5e37d9b94b9fe6ec84b13.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.7879])\n",
      "data: tensor([[172.6041, 321.9689, 331.7889, 452.2111,   0.7879,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[252.1965, 387.0900, 159.1848, 130.2422]])\n",
      "xywhn: tensor([[0.5254, 0.8064, 0.3316, 0.2713]])\n",
      "xyxy: tensor([[172.6041, 321.9689, 331.7889, 452.2111]])\n",
      "xyxyn: tensor([[0.3596, 0.6708, 0.6912, 0.9421]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/medicine-74-_jpg.rf.87cea9b0c8e17d1355262702336417ff.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2., 2.])\n",
      "conf: tensor([0.8094, 0.6887, 0.5132])\n",
      "data: tensor([[162.4870, 185.0783, 247.6386, 312.6646,   0.8094,   2.0000],\n",
      "        [238.4499, 144.9144, 318.8401, 258.6402,   0.6887,   2.0000],\n",
      "        [238.5927, 146.8117, 284.1174, 258.4526,   0.5132,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[205.0628, 248.8714,  85.1516, 127.5863],\n",
      "        [278.6450, 201.7773,  80.3903, 113.7258],\n",
      "        [261.3550, 202.6321,  45.5247, 111.6409]])\n",
      "xywhn: tensor([[0.4272, 0.5185, 0.1774, 0.2658],\n",
      "        [0.5805, 0.4204, 0.1675, 0.2369],\n",
      "        [0.5445, 0.4222, 0.0948, 0.2326]])\n",
      "xyxy: tensor([[162.4870, 185.0783, 247.6386, 312.6646],\n",
      "        [238.4499, 144.9144, 318.8401, 258.6402],\n",
      "        [238.5927, 146.8117, 284.1174, 258.4526]])\n",
      "xyxyn: tensor([[0.3385, 0.3856, 0.5159, 0.6514],\n",
      "        [0.4968, 0.3019, 0.6643, 0.5388],\n",
      "        [0.4971, 0.3059, 0.5919, 0.5384]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104700_Pinterest_jpg.rf.05fe65188f7d4ca98dfc2595ed8a6e6e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8587, 0.8433, 0.8227, 0.8160, 0.7546])\n",
      "data: tensor([[276.1464, 126.2404, 463.0775, 271.8326,   0.8587,   0.0000],\n",
      "        [246.5167, 291.3642, 435.5709, 412.4435,   0.8433,   0.0000],\n",
      "        [  0.6549, 225.8205, 222.2167, 361.8392,   0.8227,   0.0000],\n",
      "        [ 34.6490,  87.5909, 149.4679, 194.0913,   0.8160,   0.0000],\n",
      "        [136.2382,  94.3921, 250.3140, 203.7458,   0.7546,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[369.6120, 199.0365, 186.9311, 145.5922],\n",
      "        [341.0438, 351.9039, 189.0543, 121.0793],\n",
      "        [111.4358, 293.8298, 221.5618, 136.0187],\n",
      "        [ 92.0584, 140.8411, 114.8189, 106.5004],\n",
      "        [193.2761, 149.0689, 114.0758, 109.3537]])\n",
      "xywhn: tensor([[0.7700, 0.4147, 0.3894, 0.3033],\n",
      "        [0.7105, 0.7331, 0.3939, 0.2522],\n",
      "        [0.2322, 0.6121, 0.4616, 0.2834],\n",
      "        [0.1918, 0.2934, 0.2392, 0.2219],\n",
      "        [0.4027, 0.3106, 0.2377, 0.2278]])\n",
      "xyxy: tensor([[276.1464, 126.2404, 463.0775, 271.8326],\n",
      "        [246.5167, 291.3642, 435.5709, 412.4435],\n",
      "        [  0.6549, 225.8205, 222.2167, 361.8392],\n",
      "        [ 34.6490,  87.5909, 149.4679, 194.0913],\n",
      "        [136.2382,  94.3921, 250.3140, 203.7458]])\n",
      "xyxyn: tensor([[0.5753, 0.2630, 0.9647, 0.5663],\n",
      "        [0.5136, 0.6070, 0.9074, 0.8593],\n",
      "        [0.0014, 0.4705, 0.4630, 0.7538],\n",
      "        [0.0722, 0.1825, 0.3114, 0.4044],\n",
      "        [0.2838, 0.1967, 0.5215, 0.4245]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130131_jpg.rf.9ae016c3e2aa90ca0a84dd3ed5606726.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1.])\n",
      "conf: tensor([0.9571])\n",
      "data: tensor([[357.1077, 166.0281, 430.0410, 264.3026,   0.9571,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[393.5743, 215.1654,  72.9333,  98.2745]])\n",
      "xywhn: tensor([[0.8199, 0.4483, 0.1519, 0.2047]])\n",
      "xyxy: tensor([[357.1077, 166.0281, 430.0410, 264.3026]])\n",
      "xyxyn: tensor([[0.7440, 0.3459, 0.8959, 0.5506]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Oseltamivir-45-MG-8-Copy_jpg.rf.08755f5aca06c3b6d3f376ceac474384.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9422, 0.9313])\n",
      "data: tensor([[252.6636,   5.8097, 479.5558, 480.0000,   0.9422,   2.0000],\n",
      "        [ 16.5415,   5.6101, 254.3629, 479.3857,   0.9313,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[366.1097, 242.9049, 226.8922, 474.1903],\n",
      "        [135.4522, 242.4979, 237.8215, 473.7757]])\n",
      "xywhn: tensor([[0.7627, 0.5061, 0.4727, 0.9879],\n",
      "        [0.2822, 0.5052, 0.4955, 0.9870]])\n",
      "xyxy: tensor([[252.6636,   5.8097, 479.5558, 480.0000],\n",
      "        [ 16.5415,   5.6101, 254.3629, 479.3857]])\n",
      "xyxyn: tensor([[0.5264, 0.0121, 0.9991, 1.0000],\n",
      "        [0.0345, 0.0117, 0.5299, 0.9987]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_101750_Pinterest_jpg.rf.f635423a64dc671c72cff9ec3d183a98.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 3.])\n",
      "conf: tensor([0.6809, 0.6713])\n",
      "data: tensor([[ 23.9294,  93.4754, 269.3745, 160.9571,   0.6809,   3.0000],\n",
      "        [ 25.0599,  48.0008, 258.0030, 104.4211,   0.6713,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[146.6519, 127.2162, 245.4451,  67.4817],\n",
      "        [141.5314,  76.2110, 232.9431,  56.4203]])\n",
      "xywhn: tensor([[0.3055, 0.2650, 0.5113, 0.1406],\n",
      "        [0.2949, 0.1588, 0.4853, 0.1175]])\n",
      "xyxy: tensor([[ 23.9294,  93.4754, 269.3745, 160.9571],\n",
      "        [ 25.0599,  48.0008, 258.0030, 104.4211]])\n",
      "xyxyn: tensor([[0.0499, 0.1947, 0.5612, 0.3353],\n",
      "        [0.0522, 0.1000, 0.5375, 0.2175]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100005_Pinterest_jpg.rf.91527503b88adf48f7f982ceabf71085.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8722])\n",
      "data: tensor([[  7.6856,  37.5224, 448.3192, 323.4073,   0.8722,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[228.0024, 180.4648, 440.6336, 285.8849]])\n",
      "xywhn: tensor([[0.4750, 0.3760, 0.9180, 0.5956]])\n",
      "xyxy: tensor([[  7.6856,  37.5224, 448.3192, 323.4073]])\n",
      "xyxyn: tensor([[0.0160, 0.0782, 0.9340, 0.6738]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/medicine-60-_jpg.rf.9cd381121b50c490f9db1682048e1702.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2.])\n",
      "conf: tensor([0.8241])\n",
      "data: tensor([[226.7608, 200.6771, 302.9077, 340.6198,   0.8241,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[264.8343, 270.6484,  76.1469, 139.9426]])\n",
      "xywhn: tensor([[0.5517, 0.5639, 0.1586, 0.2915]])\n",
      "xyxy: tensor([[226.7608, 200.6771, 302.9077, 340.6198]])\n",
      "xyxyn: tensor([[0.4724, 0.4181, 0.6311, 0.7096]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/paracetamol_071415111756_jpg.rf.24e4e04dbb8408b28bf87f08e9841d5e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.8967, 0.8482])\n",
      "data: tensor([[227.4830, 228.0628, 297.0504, 408.9119,   0.8967,   2.0000],\n",
      "        [150.3421, 235.8579, 245.9689, 398.0872,   0.8482,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[262.2667, 318.4873,  69.5674, 180.8491],\n",
      "        [198.1555, 316.9725,  95.6268, 162.2293]])\n",
      "xywhn: tensor([[0.5464, 0.6635, 0.1449, 0.3768],\n",
      "        [0.4128, 0.6604, 0.1992, 0.3380]])\n",
      "xyxy: tensor([[227.4830, 228.0628, 297.0504, 408.9119],\n",
      "        [150.3421, 235.8579, 245.9689, 398.0872]])\n",
      "xyxyn: tensor([[0.4739, 0.4751, 0.6189, 0.8519],\n",
      "        [0.3132, 0.4914, 0.5124, 0.8293]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151622_Pinterest_jpg.rf.d59265451b13ab2f6b4df572937cd6cc.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8701])\n",
      "data: tensor([[  2.5400,  28.9192, 479.2903, 204.1190,   0.8701,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[240.9152, 116.5191, 476.7502, 175.1999]])\n",
      "xywhn: tensor([[0.5019, 0.2427, 0.9932, 0.3650]])\n",
      "xyxy: tensor([[  2.5400,  28.9192, 479.2903, 204.1190]])\n",
      "xyxyn: tensor([[0.0053, 0.0602, 0.9985, 0.4252]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094006_Pinterest_jpg.rf.d6efba86e93b7dd5a04aaac80b56abc6.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.7518, 0.7222])\n",
      "data: tensor([[291.4211, 140.7389, 431.7977, 252.7054,   0.7518,   0.0000],\n",
      "        [281.0176,  43.1187, 419.0063, 169.4596,   0.7222,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[361.6094, 196.7222, 140.3766, 111.9665],\n",
      "        [350.0120, 106.2892, 137.9886, 126.3409]])\n",
      "xywhn: tensor([[0.7534, 0.4098, 0.2925, 0.2333],\n",
      "        [0.7292, 0.2214, 0.2875, 0.2632]])\n",
      "xyxy: tensor([[291.4211, 140.7389, 431.7977, 252.7054],\n",
      "        [281.0176,  43.1187, 419.0063, 169.4596]])\n",
      "xyxyn: tensor([[0.6071, 0.2932, 0.8996, 0.5265],\n",
      "        [0.5855, 0.0898, 0.8729, 0.3530]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104736_Pinterest_jpg.rf.a4049f1fa8d41eb8836778816bd4101c.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8354, 0.8194, 0.7884, 0.7684, 0.7243, 0.7027, 0.6453])\n",
      "data: tensor([[ 27.3987, 133.5605, 191.3400, 267.7821,   0.8354,   0.0000],\n",
      "        [363.5396, 256.2891, 478.3350, 328.0212,   0.8194,   0.0000],\n",
      "        [291.2051, 315.2258, 414.8765, 387.5389,   0.7884,   0.0000],\n",
      "        [ 76.9680, 299.4156, 217.4745, 417.2985,   0.7684,   0.0000],\n",
      "        [239.6172, 151.3795, 329.3099, 224.8698,   0.7243,   0.0000],\n",
      "        [338.5928, 134.8936, 439.0809, 196.2178,   0.7027,   0.0000],\n",
      "        [246.7357,   0.0000, 380.0117,  71.6146,   0.6453,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[109.3693, 200.6713, 163.9413, 134.2216],\n",
      "        [420.9373, 292.1552, 114.7954,  71.7322],\n",
      "        [353.0408, 351.3824, 123.6714,  72.3131],\n",
      "        [147.2213, 358.3571, 140.5065, 117.8828],\n",
      "        [284.4636, 188.1247,  89.6927,  73.4902],\n",
      "        [388.8369, 165.5557, 100.4881,  61.3241],\n",
      "        [313.3737,  35.8073, 133.2759,  71.6146]])\n",
      "xywhn: tensor([[0.2279, 0.4181, 0.3415, 0.2796],\n",
      "        [0.8770, 0.6087, 0.2392, 0.1494],\n",
      "        [0.7355, 0.7320, 0.2576, 0.1507],\n",
      "        [0.3067, 0.7466, 0.2927, 0.2456],\n",
      "        [0.5926, 0.3919, 0.1869, 0.1531],\n",
      "        [0.8101, 0.3449, 0.2094, 0.1278],\n",
      "        [0.6529, 0.0746, 0.2777, 0.1492]])\n",
      "xyxy: tensor([[ 27.3987, 133.5605, 191.3400, 267.7821],\n",
      "        [363.5396, 256.2891, 478.3350, 328.0212],\n",
      "        [291.2051, 315.2258, 414.8765, 387.5389],\n",
      "        [ 76.9680, 299.4156, 217.4745, 417.2985],\n",
      "        [239.6172, 151.3795, 329.3099, 224.8698],\n",
      "        [338.5928, 134.8936, 439.0809, 196.2178],\n",
      "        [246.7357,   0.0000, 380.0117,  71.6146]])\n",
      "xyxyn: tensor([[0.0571, 0.2783, 0.3986, 0.5579],\n",
      "        [0.7574, 0.5339, 0.9965, 0.6834],\n",
      "        [0.6067, 0.6567, 0.8643, 0.8074],\n",
      "        [0.1604, 0.6238, 0.4531, 0.8694],\n",
      "        [0.4992, 0.3154, 0.6861, 0.4685],\n",
      "        [0.7054, 0.2810, 0.9148, 0.4088],\n",
      "        [0.5140, 0.0000, 0.7917, 0.1492]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/celecoxib-200-MG-1-Copy-Copy_jpg.rf.6e0d084f6b13571a845e61d481d49bf5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9451, 0.9245])\n",
      "data: tensor([[ 14.0022,   0.8658, 479.9106, 228.7415,   0.9451,   2.0000],\n",
      "        [ 11.5741, 224.0340, 478.7378, 464.3940,   0.9245,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[246.9564, 114.8036, 465.9084, 227.8757],\n",
      "        [245.1559, 344.2140, 467.1637, 240.3600]])\n",
      "xywhn: tensor([[0.5145, 0.2392, 0.9706, 0.4747],\n",
      "        [0.5107, 0.7171, 0.9733, 0.5008]])\n",
      "xyxy: tensor([[ 14.0022,   0.8658, 479.9106, 228.7415],\n",
      "        [ 11.5741, 224.0340, 478.7378, 464.3940]])\n",
      "xyxyn: tensor([[0.0292, 0.0018, 0.9998, 0.4765],\n",
      "        [0.0241, 0.4667, 0.9974, 0.9675]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240522_210242_SHEIN_jpg.rf.19a0a1a7d15a6b5d27865175847cedb3.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8602])\n",
      "data: tensor([[  0.0000, 106.5244, 480.0000, 257.1260,   0.8602,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[240.0000, 181.8252, 480.0000, 150.6017]])\n",
      "xywhn: tensor([[0.5000, 0.3788, 1.0000, 0.3138]])\n",
      "xyxy: tensor([[  0.0000, 106.5244, 480.0000, 257.1260]])\n",
      "xyxyn: tensor([[0.0000, 0.2219, 1.0000, 0.5357]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130657_jpg.rf.ca066f3d08dee76f345b3027f8c6106c.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1.])\n",
      "conf: tensor([0.8744, 0.8552])\n",
      "data: tensor([[259.4696, 266.6423, 326.5509, 350.2707,   0.8744,   1.0000],\n",
      "        [ 62.8003,  90.7000, 111.7658, 198.4646,   0.8552,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[293.0103, 308.4565,  67.0814,  83.6284],\n",
      "        [ 87.2831, 144.5823,  48.9655, 107.7646]])\n",
      "xywhn: tensor([[0.6104, 0.6426, 0.1398, 0.1742],\n",
      "        [0.1818, 0.3012, 0.1020, 0.2245]])\n",
      "xyxy: tensor([[259.4696, 266.6423, 326.5509, 350.2707],\n",
      "        [ 62.8003,  90.7000, 111.7658, 198.4646]])\n",
      "xyxyn: tensor([[0.5406, 0.5555, 0.6803, 0.7297],\n",
      "        [0.1308, 0.1890, 0.2328, 0.4135]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130245_jpg.rf.638a5954c3ab2c07c3a9c4f21e5de07c.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.6882, 0.6417, 0.4355, 0.3656, 0.2729])\n",
      "data: tensor([[2.4700e+02, 2.3033e+02, 3.2955e+02, 3.1543e+02, 6.8817e-01, 1.0000e+00],\n",
      "        [3.4242e+02, 1.9502e+02, 4.1129e+02, 2.5870e+02, 6.4175e-01, 1.0000e+00],\n",
      "        [3.4279e+02, 1.6360e+02, 3.7955e+02, 1.9481e+02, 4.3548e-01, 1.0000e+00],\n",
      "        [3.4334e+02, 1.5920e+02, 4.0526e+02, 1.9477e+02, 3.6565e-01, 1.0000e+00],\n",
      "        [3.4285e+02, 1.9872e+02, 4.0182e+02, 2.5077e+02, 2.7289e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[288.2760, 272.8811,  82.5495,  85.0966],\n",
      "        [376.8518, 226.8582,  68.8670,  63.6807],\n",
      "        [361.1730, 179.2079,  36.7579,  31.2134],\n",
      "        [374.2983, 176.9831,  61.9242,  35.5663],\n",
      "        [372.3344, 224.7441,  58.9716,  52.0513]])\n",
      "xywhn: tensor([[0.6006, 0.5685, 0.1720, 0.1773],\n",
      "        [0.7851, 0.4726, 0.1435, 0.1327],\n",
      "        [0.7524, 0.3733, 0.0766, 0.0650],\n",
      "        [0.7798, 0.3687, 0.1290, 0.0741],\n",
      "        [0.7757, 0.4682, 0.1229, 0.1084]])\n",
      "xyxy: tensor([[247.0012, 230.3328, 329.5507, 315.4294],\n",
      "        [342.4183, 195.0178, 411.2853, 258.6985],\n",
      "        [342.7941, 163.6011, 379.5520, 194.8146],\n",
      "        [343.3362, 159.2000, 405.2604, 194.7662],\n",
      "        [342.8486, 198.7184, 401.8202, 250.7697]])\n",
      "xyxyn: tensor([[0.5146, 0.4799, 0.6866, 0.6571],\n",
      "        [0.7134, 0.4063, 0.8568, 0.5390],\n",
      "        [0.7142, 0.3408, 0.7907, 0.4059],\n",
      "        [0.7153, 0.3317, 0.8443, 0.4058],\n",
      "        [0.7143, 0.4140, 0.8371, 0.5224]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/aprepitant-80-MG-12-_jpg.rf.e9223c2227d2d8a162869e955e51c09f.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9307, 0.9283])\n",
      "data: tensor([[7.1804e+00, 1.7281e-01, 4.8000e+02, 1.8884e+02, 9.3069e-01, 2.0000e+00],\n",
      "        [8.5317e+00, 1.9112e+02, 4.7974e+02, 4.1376e+02, 9.2834e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[243.5902,  94.5074, 472.8196, 188.6693],\n",
      "        [244.1374, 302.4388, 471.2113, 222.6418]])\n",
      "xywhn: tensor([[0.5075, 0.1969, 0.9850, 0.3931],\n",
      "        [0.5086, 0.6301, 0.9817, 0.4638]])\n",
      "xyxy: tensor([[7.1804e+00, 1.7281e-01, 4.8000e+02, 1.8884e+02],\n",
      "        [8.5317e+00, 1.9112e+02, 4.7974e+02, 4.1376e+02]])\n",
      "xyxyn: tensor([[1.4959e-02, 3.6001e-04, 1.0000e+00, 3.9342e-01],\n",
      "        [1.7774e-02, 3.9816e-01, 9.9946e-01, 8.6200e-01]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100634_Pinterest_jpg.rf.932e3a30f4e76d777f00ced384d4b0c8.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8021, 0.5719])\n",
      "data: tensor([[208.7383,  35.8299, 480.0000, 258.0029,   0.8021,   0.0000],\n",
      "        [ 77.9995,  40.3191, 311.3392, 233.3332,   0.5719,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[344.3691, 146.9164, 271.2617, 222.1730],\n",
      "        [194.6694, 136.8262, 233.3398, 193.0141]])\n",
      "xywhn: tensor([[0.7174, 0.3061, 0.5651, 0.4629],\n",
      "        [0.4056, 0.2851, 0.4861, 0.4021]])\n",
      "xyxy: tensor([[208.7383,  35.8299, 480.0000, 258.0029],\n",
      "        [ 77.9995,  40.3191, 311.3392, 233.3332]])\n",
      "xyxyn: tensor([[0.4349, 0.0746, 1.0000, 0.5375],\n",
      "        [0.1625, 0.0840, 0.6486, 0.4861]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104713_Pinterest_jpg.rf.c107d6a88579d7246a043ac4ba16084c.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0.])\n",
      "conf: tensor([0.9018, 0.8950, 0.8780, 0.8715])\n",
      "data: tensor([[ 19.1943,  68.2115, 231.3753, 191.8434,   0.9018,   0.0000],\n",
      "        [ 16.3918, 230.0137, 217.7769, 403.9230,   0.8950,   0.0000],\n",
      "        [290.7716,  67.3588, 447.5670, 191.6117,   0.8780,   0.0000],\n",
      "        [270.9658, 241.3583, 449.7672, 399.4293,   0.8715,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[125.2848, 130.0275, 212.1810, 123.6318],\n",
      "        [117.0843, 316.9684, 201.3852, 173.9094],\n",
      "        [369.1693, 129.4853, 156.7954, 124.2529],\n",
      "        [360.3665, 320.3938, 178.8014, 158.0709]])\n",
      "xywhn: tensor([[0.2610, 0.2709, 0.4420, 0.2576],\n",
      "        [0.2439, 0.6604, 0.4196, 0.3623],\n",
      "        [0.7691, 0.2698, 0.3267, 0.2589],\n",
      "        [0.7508, 0.6675, 0.3725, 0.3293]])\n",
      "xyxy: tensor([[ 19.1943,  68.2115, 231.3753, 191.8434],\n",
      "        [ 16.3918, 230.0137, 217.7769, 403.9230],\n",
      "        [290.7716,  67.3588, 447.5670, 191.6117],\n",
      "        [270.9658, 241.3583, 449.7672, 399.4293]])\n",
      "xyxyn: tensor([[0.0400, 0.1421, 0.4820, 0.3997],\n",
      "        [0.0341, 0.4792, 0.4537, 0.8415],\n",
      "        [0.6058, 0.1403, 0.9324, 0.3992],\n",
      "        [0.5645, 0.5028, 0.9370, 0.8321]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150523_Pinterest_jpg.rf.46269cce6643d673eecbf89643281211.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8168, 0.7693, 0.7539, 0.7524, 0.7330, 0.3327])\n",
      "data: tensor([[1.8237e+02, 5.5263e+01, 3.3160e+02, 1.2342e+02, 8.1678e-01, 0.0000e+00],\n",
      "        [1.7211e+02, 1.4613e+02, 3.1230e+02, 2.2635e+02, 7.6928e-01, 0.0000e+00],\n",
      "        [3.1621e+01, 3.8101e+01, 1.6667e+02, 1.0676e+02, 7.5391e-01, 0.0000e+00],\n",
      "        [3.4408e+02, 6.1803e+01, 4.7692e+02, 1.4436e+02, 7.5236e-01, 0.0000e+00],\n",
      "        [1.4262e+01, 1.2832e+02, 1.5731e+02, 2.1333e+02, 7.3298e-01, 0.0000e+00],\n",
      "        [3.4380e+02, 1.6192e+02, 4.6347e+02, 2.3983e+02, 3.3270e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[256.9885,  89.3397, 149.2305,  68.1538],\n",
      "        [242.2057, 186.2431, 140.1892,  80.2234],\n",
      "        [ 99.1452,  72.4286, 135.0474,  68.6555],\n",
      "        [410.5015, 103.0821, 132.8347,  82.5591],\n",
      "        [ 85.7848, 170.8264, 143.0461,  85.0154],\n",
      "        [403.6319, 200.8731, 119.6698,  77.9118]])\n",
      "xywhn: tensor([[0.5354, 0.1861, 0.3109, 0.1420],\n",
      "        [0.5046, 0.3880, 0.2921, 0.1671],\n",
      "        [0.2066, 0.1509, 0.2813, 0.1430],\n",
      "        [0.8552, 0.2148, 0.2767, 0.1720],\n",
      "        [0.1787, 0.3559, 0.2980, 0.1771],\n",
      "        [0.8409, 0.4185, 0.2493, 0.1623]])\n",
      "xyxy: tensor([[182.3733,  55.2628, 331.6038, 123.4166],\n",
      "        [172.1111, 146.1314, 312.3003, 226.3548],\n",
      "        [ 31.6215,  38.1009, 166.6688, 106.7564],\n",
      "        [344.0841,  61.8025, 476.9188, 144.3616],\n",
      "        [ 14.2617, 128.3188, 157.3078, 213.3341],\n",
      "        [343.7970, 161.9172, 463.4668, 239.8290]])\n",
      "xyxyn: tensor([[0.3799, 0.1151, 0.6908, 0.2571],\n",
      "        [0.3586, 0.3044, 0.6506, 0.4716],\n",
      "        [0.0659, 0.0794, 0.3472, 0.2224],\n",
      "        [0.7168, 0.1288, 0.9936, 0.3008],\n",
      "        [0.0297, 0.2673, 0.3277, 0.4444],\n",
      "        [0.7162, 0.3373, 0.9656, 0.4996]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151832_Pinterest_jpg.rf.ed3a795c855437cb7dc3bcf7804d5c45.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8795])\n",
      "data: tensor([[  2.2326,  62.4296, 480.0000, 232.1263,   0.8795,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[241.1163, 147.2779, 477.7674, 169.6967]])\n",
      "xywhn: tensor([[0.5023, 0.3068, 0.9953, 0.3535]])\n",
      "xyxy: tensor([[  2.2326,  62.4296, 480.0000, 232.1263]])\n",
      "xyxyn: tensor([[0.0047, 0.1301, 1.0000, 0.4836]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_105518_Pinterest_jpg.rf.2822ddeaa3b16aff497526a1fbeb9925.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 3., 3., 3., 3.])\n",
      "conf: tensor([0.8486, 0.8259, 0.7962, 0.7444, 0.6400])\n",
      "data: tensor([[250.7639, 305.6788, 466.1347, 390.1077,   0.8486,   3.0000],\n",
      "        [247.9759,  36.3896, 468.3896, 120.3772,   0.8259,   3.0000],\n",
      "        [ 10.5487, 327.0165, 232.8195, 406.5080,   0.7962,   3.0000],\n",
      "        [ 22.2955, 163.9312, 232.6149, 267.7970,   0.7444,   3.0000],\n",
      "        [244.7993, 166.2143, 462.6323, 230.6918,   0.6400,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[358.4493, 347.8932, 215.3708,  84.4289],\n",
      "        [358.1828,  78.3834, 220.4137,  83.9876],\n",
      "        [121.6841, 366.7623, 222.2708,  79.4915],\n",
      "        [127.4552, 215.8641, 210.3193, 103.8658],\n",
      "        [353.7158, 198.4531, 217.8330,  64.4776]])\n",
      "xywhn: tensor([[0.7468, 0.7248, 0.4487, 0.1759],\n",
      "        [0.7462, 0.1633, 0.4592, 0.1750],\n",
      "        [0.2535, 0.7641, 0.4631, 0.1656],\n",
      "        [0.2655, 0.4497, 0.4382, 0.2164],\n",
      "        [0.7369, 0.4134, 0.4538, 0.1343]])\n",
      "xyxy: tensor([[250.7639, 305.6788, 466.1347, 390.1077],\n",
      "        [247.9759,  36.3896, 468.3896, 120.3772],\n",
      "        [ 10.5487, 327.0165, 232.8195, 406.5080],\n",
      "        [ 22.2955, 163.9312, 232.6149, 267.7970],\n",
      "        [244.7993, 166.2143, 462.6323, 230.6918]])\n",
      "xyxyn: tensor([[0.5224, 0.6368, 0.9711, 0.8127],\n",
      "        [0.5166, 0.0758, 0.9758, 0.2508],\n",
      "        [0.0220, 0.6813, 0.4850, 0.8469],\n",
      "        [0.0464, 0.3415, 0.4846, 0.5579],\n",
      "        [0.5100, 0.3463, 0.9638, 0.4806]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/sitagliptin-50-MG-5-_jpg.rf.977549fff0f745a7c615e2405fb85d75.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.8939, 0.8904])\n",
      "data: tensor([[ 18.0265,  66.2779, 252.9141, 391.5057,   0.8939,   2.0000],\n",
      "        [248.7007,  60.9913, 479.9995, 392.5209,   0.8904,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[135.4703, 228.8918, 234.8877, 325.2278],\n",
      "        [364.3501, 226.7561, 231.2987, 331.5297]])\n",
      "xywhn: tensor([[0.2822, 0.4769, 0.4893, 0.6776],\n",
      "        [0.7591, 0.4724, 0.4819, 0.6907]])\n",
      "xyxy: tensor([[ 18.0265,  66.2779, 252.9141, 391.5057],\n",
      "        [248.7007,  60.9913, 479.9995, 392.5209]])\n",
      "xyxyn: tensor([[0.0376, 0.1381, 0.5269, 0.8156],\n",
      "        [0.5181, 0.1271, 1.0000, 0.8178]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_094351_Pinterest_jpg.rf.7a453cfb53f1cd99b563b51ea48c4d0a.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8500])\n",
      "data: tensor([[  2.9330,  42.7460, 479.9789, 306.2006,   0.8500,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[241.4560, 174.4733, 477.0458, 263.4546]])\n",
      "xywhn: tensor([[0.5030, 0.3635, 0.9938, 0.5489]])\n",
      "xyxy: tensor([[  2.9330,  42.7460, 479.9789, 306.2006]])\n",
      "xyxyn: tensor([[0.0061, 0.0891, 1.0000, 0.6379]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104744_Pinterest_jpg.rf.9cd2998d16d0745111adbd2459963b4d.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0.])\n",
      "conf: tensor([0.9099, 0.8955, 0.8875, 0.8080])\n",
      "data: tensor([[ 25.8361, 257.0583, 232.2883, 366.4343,   0.9099,   0.0000],\n",
      "        [252.3942,  12.8811, 467.9075, 153.2167,   0.8955,   0.0000],\n",
      "        [  7.6100,  90.9087, 214.9829, 217.7869,   0.8875,   0.0000],\n",
      "        [293.6717, 181.3523, 437.2601, 298.6760,   0.8080,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[129.0622, 311.7463, 206.4522, 109.3760],\n",
      "        [360.1508,  83.0489, 215.5133, 140.3355],\n",
      "        [111.2964, 154.3478, 207.3729, 126.8782],\n",
      "        [365.4659, 240.0142, 143.5884, 117.3237]])\n",
      "xywhn: tensor([[0.2689, 0.6495, 0.4301, 0.2279],\n",
      "        [0.7503, 0.1730, 0.4490, 0.2924],\n",
      "        [0.2319, 0.3216, 0.4320, 0.2643],\n",
      "        [0.7614, 0.5000, 0.2991, 0.2444]])\n",
      "xyxy: tensor([[ 25.8361, 257.0583, 232.2883, 366.4343],\n",
      "        [252.3942,  12.8811, 467.9075, 153.2167],\n",
      "        [  7.6100,  90.9087, 214.9829, 217.7869],\n",
      "        [293.6717, 181.3523, 437.2601, 298.6760]])\n",
      "xyxyn: tensor([[0.0538, 0.5355, 0.4839, 0.7634],\n",
      "        [0.5258, 0.0268, 0.9748, 0.3192],\n",
      "        [0.0159, 0.1894, 0.4479, 0.4537],\n",
      "        [0.6118, 0.3778, 0.9110, 0.6222]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104545_Pinterest_jpg.rf.edb3f5428691b42a13cdd211de4c37f5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8741, 0.8740, 0.8556, 0.8537, 0.3272])\n",
      "data: tensor([[2.7464e+01, 2.1324e+02, 2.1628e+02, 3.5898e+02, 8.7410e-01, 0.0000e+00],\n",
      "        [2.6946e+01, 9.4839e+01, 2.2010e+02, 1.8966e+02, 8.7397e-01, 0.0000e+00],\n",
      "        [2.6983e+02, 1.8328e+01, 4.4598e+02, 1.1793e+02, 8.5562e-01, 0.0000e+00],\n",
      "        [2.8408e+02, 1.6410e+02, 4.4269e+02, 2.9366e+02, 8.5368e-01, 0.0000e+00],\n",
      "        [6.0192e+01, 1.3280e+01, 1.9946e+02, 7.2723e+01, 3.2716e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[121.8724, 286.1121, 188.8164, 145.7374],\n",
      "        [123.5233, 142.2504, 193.1540,  94.8224],\n",
      "        [357.9046,  68.1277, 176.1429,  99.5990],\n",
      "        [363.3872, 228.8837, 158.6114, 129.5601],\n",
      "        [129.8281,  43.0014, 139.2730,  59.4431]])\n",
      "xywhn: tensor([[0.2539, 0.5961, 0.3934, 0.3036],\n",
      "        [0.2573, 0.2964, 0.4024, 0.1975],\n",
      "        [0.7456, 0.1419, 0.3670, 0.2075],\n",
      "        [0.7571, 0.4768, 0.3304, 0.2699],\n",
      "        [0.2705, 0.0896, 0.2902, 0.1238]])\n",
      "xyxy: tensor([[ 27.4642, 213.2433, 216.2806, 358.9808],\n",
      "        [ 26.9463,  94.8391, 220.1003, 189.6616],\n",
      "        [269.8332,  18.3282, 445.9761, 117.9272],\n",
      "        [284.0815, 164.1037, 442.6929, 293.6638],\n",
      "        [ 60.1916,  13.2799, 199.4646,  72.7230]])\n",
      "xyxyn: tensor([[0.0572, 0.4443, 0.4506, 0.7479],\n",
      "        [0.0561, 0.1976, 0.4585, 0.3951],\n",
      "        [0.5622, 0.0382, 0.9291, 0.2457],\n",
      "        [0.5918, 0.3419, 0.9223, 0.6118],\n",
      "        [0.1254, 0.0277, 0.4156, 0.1515]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Oseltamivir-45-MG-11-Copy_jpg.rf.fb0cec1a7bf5e6816bf1502ddfe29ccf.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9330, 0.9146])\n",
      "data: tensor([[  4.0451,   1.7065, 480.0000, 230.7638,   0.9330,   2.0000],\n",
      "        [  4.1243, 226.1331, 480.0000, 462.6625,   0.9146,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[242.0226, 116.2352, 475.9549, 229.0573],\n",
      "        [242.0621, 344.3978, 475.8757, 236.5294]])\n",
      "xywhn: tensor([[0.5042, 0.2422, 0.9916, 0.4772],\n",
      "        [0.5043, 0.7175, 0.9914, 0.4928]])\n",
      "xyxy: tensor([[  4.0451,   1.7065, 480.0000, 230.7638],\n",
      "        [  4.1243, 226.1331, 480.0000, 462.6625]])\n",
      "xyxyn: tensor([[0.0084, 0.0036, 1.0000, 0.4808],\n",
      "        [0.0086, 0.4711, 1.0000, 0.9639]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151456_Pinterest_jpg.rf.3c2687f58fea3ebf803df37d80ec2fd6.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8961])\n",
      "data: tensor([[  0.0000,  45.6299, 479.3244, 205.1954,   0.8961,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[239.6622, 125.4127, 479.3244, 159.5654]])\n",
      "xywhn: tensor([[0.4993, 0.2613, 0.9986, 0.3324]])\n",
      "xyxy: tensor([[  0.0000,  45.6299, 479.3244, 205.1954]])\n",
      "xyxyn: tensor([[0.0000, 0.0951, 0.9986, 0.4275]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_095224_Pinterest_jpg.rf.339767c03b35612ec381c1455ee67116.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8931, 0.8868])\n",
      "data: tensor([[223.7810,  17.5623, 479.2488, 232.5491,   0.8931,   0.0000],\n",
      "        [  0.0000,  21.4446, 251.3521, 228.6190,   0.8868,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[351.5149, 125.0557, 255.4678, 214.9868],\n",
      "        [125.6761, 125.0318, 251.3521, 207.1744]])\n",
      "xywhn: tensor([[0.7323, 0.2605, 0.5322, 0.4479],\n",
      "        [0.2618, 0.2605, 0.5237, 0.4316]])\n",
      "xyxy: tensor([[223.7810,  17.5623, 479.2488, 232.5491],\n",
      "        [  0.0000,  21.4446, 251.3521, 228.6190]])\n",
      "xyxyn: tensor([[0.4662, 0.0366, 0.9984, 0.4845],\n",
      "        [0.0000, 0.0447, 0.5237, 0.4763]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/celecoxib-200-MG-7-_jpg.rf.41866895c696f1930075b04aded809ee.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2.])\n",
      "conf: tensor([0.9516])\n",
      "data: tensor([[  1.1090,   3.8610, 472.4167, 477.1782,   0.9516,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[236.7628, 240.5196, 471.3077, 473.3171]])\n",
      "xywhn: tensor([[0.4933, 0.5011, 0.9819, 0.9861]])\n",
      "xyxy: tensor([[  1.1090,   3.8610, 472.4167, 477.1782]])\n",
      "xyxyn: tensor([[0.0023, 0.0080, 0.9842, 0.9941]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_150258_Pinterest_jpg.rf.29fb34842b49ce9eb71abe3d9c578384.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0.])\n",
      "conf: tensor([0.8774, 0.8153, 0.7968, 0.6575])\n",
      "data: tensor([[284.3505, 128.1520, 446.0373, 216.4851,   0.8774,   0.0000],\n",
      "        [ 37.6317,  90.0065, 219.5896, 243.5160,   0.8153,   0.0000],\n",
      "        [356.2802, 252.0661, 452.2240, 316.1178,   0.7968,   0.0000],\n",
      "        [266.0773, 264.4381, 363.4579, 321.1735,   0.6575,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[365.1939, 172.3186, 161.6868,  88.3331],\n",
      "        [128.6106, 166.7612, 181.9579, 153.5094],\n",
      "        [404.2521, 284.0920,  95.9438,  64.0517],\n",
      "        [314.7676, 292.8058,  97.3806,  56.7354]])\n",
      "xywhn: tensor([[0.7608, 0.3590, 0.3368, 0.1840],\n",
      "        [0.2679, 0.3474, 0.3791, 0.3198],\n",
      "        [0.8422, 0.5919, 0.1999, 0.1334],\n",
      "        [0.6558, 0.6100, 0.2029, 0.1182]])\n",
      "xyxy: tensor([[284.3505, 128.1520, 446.0373, 216.4851],\n",
      "        [ 37.6317,  90.0065, 219.5896, 243.5160],\n",
      "        [356.2802, 252.0661, 452.2240, 316.1178],\n",
      "        [266.0773, 264.4381, 363.4579, 321.1735]])\n",
      "xyxyn: tensor([[0.5924, 0.2670, 0.9292, 0.4510],\n",
      "        [0.0784, 0.1875, 0.4575, 0.5073],\n",
      "        [0.7423, 0.5251, 0.9421, 0.6586],\n",
      "        [0.5543, 0.5509, 0.7572, 0.6691]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240531_092731_Chrome_jpg.rf.10cc7c733f67ad1331f00ed881e737ee.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "conf: tensor([0.8608, 0.8339, 0.8303, 0.8298, 0.8262, 0.8253, 0.7936, 0.7704, 0.7582, 0.7520, 0.7464, 0.7406, 0.7296, 0.6315, 0.6097, 0.4945])\n",
      "data: tensor([[171.1997, 175.6366, 231.9033, 227.1161,   0.8608,   2.0000],\n",
      "        [251.8602, 177.2849, 312.3723, 225.2459,   0.8339,   2.0000],\n",
      "        [324.8678,  93.2962, 392.2623, 147.7610,   0.8303,   2.0000],\n",
      "        [390.3589, 255.2135, 460.8468, 290.8792,   0.8298,   2.0000],\n",
      "        [  9.9118, 174.0269,  74.9506, 222.1189,   0.8262,   2.0000],\n",
      "        [ 86.5140,  88.1934, 159.8624, 143.3725,   0.8253,   2.0000],\n",
      "        [332.3311, 184.1824, 382.7289, 222.0495,   0.7936,   2.0000],\n",
      "        [174.6513,  94.5329, 228.5246, 144.9637,   0.7704,   2.0000],\n",
      "        [133.3665, 249.4109, 222.3393, 286.5974,   0.7582,   2.0000],\n",
      "        [244.6157,  92.2655, 313.9451, 146.5514,   0.7520,   2.0000],\n",
      "        [272.1446, 259.1572, 331.4660, 289.2208,   0.7464,   2.0000],\n",
      "        [ 17.8932, 255.5981,  98.0953, 287.3080,   0.7406,   2.0000],\n",
      "        [416.5436, 194.8693, 454.6855, 218.9114,   0.7296,   2.0000],\n",
      "        [ 83.7238, 155.5521, 161.5458, 224.0206,   0.6315,   2.0000],\n",
      "        [403.4138,  93.1903, 468.3379, 146.9668,   0.6097,   2.0000],\n",
      "        [  1.5123,  88.7675,  83.9519, 142.5273,   0.4945,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([16, 6])\n",
      "xywh: tensor([[201.5515, 201.3764,  60.7036,  51.4795],\n",
      "        [282.1162, 201.2654,  60.5121,  47.9610],\n",
      "        [358.5651, 120.5286,  67.3945,  54.4647],\n",
      "        [425.6028, 273.0463,  70.4879,  35.6657],\n",
      "        [ 42.4312, 198.0729,  65.0388,  48.0920],\n",
      "        [123.1882, 115.7829,  73.3484,  55.1791],\n",
      "        [357.5300, 203.1160,  50.3978,  37.8671],\n",
      "        [201.5880, 119.7483,  53.8732,  50.4309],\n",
      "        [177.8529, 268.0042,  88.9728,  37.1864],\n",
      "        [279.2804, 119.4085,  69.3294,  54.2858],\n",
      "        [301.8053, 274.1890,  59.3214,  30.0636],\n",
      "        [ 57.9942, 271.4530,  80.2021,  31.7099],\n",
      "        [435.6146, 206.8904,  38.1418,  24.0421],\n",
      "        [122.6348, 189.7863,  77.8220,  68.4684],\n",
      "        [435.8758, 120.0786,  64.9241,  53.7766],\n",
      "        [ 42.7321, 115.6474,  82.4396,  53.7598]])\n",
      "xywhn: tensor([[0.4199, 0.4195, 0.1265, 0.1072],\n",
      "        [0.5877, 0.4193, 0.1261, 0.0999],\n",
      "        [0.7470, 0.2511, 0.1404, 0.1135],\n",
      "        [0.8867, 0.5688, 0.1468, 0.0743],\n",
      "        [0.0884, 0.4127, 0.1355, 0.1002],\n",
      "        [0.2566, 0.2412, 0.1528, 0.1150],\n",
      "        [0.7449, 0.4232, 0.1050, 0.0789],\n",
      "        [0.4200, 0.2495, 0.1122, 0.1051],\n",
      "        [0.3705, 0.5583, 0.1854, 0.0775],\n",
      "        [0.5818, 0.2488, 0.1444, 0.1131],\n",
      "        [0.6288, 0.5712, 0.1236, 0.0626],\n",
      "        [0.1208, 0.5655, 0.1671, 0.0661],\n",
      "        [0.9075, 0.4310, 0.0795, 0.0501],\n",
      "        [0.2555, 0.3954, 0.1621, 0.1426],\n",
      "        [0.9081, 0.2502, 0.1353, 0.1120],\n",
      "        [0.0890, 0.2409, 0.1717, 0.1120]])\n",
      "xyxy: tensor([[171.1997, 175.6366, 231.9033, 227.1161],\n",
      "        [251.8602, 177.2849, 312.3723, 225.2459],\n",
      "        [324.8678,  93.2962, 392.2623, 147.7610],\n",
      "        [390.3589, 255.2135, 460.8468, 290.8792],\n",
      "        [  9.9118, 174.0269,  74.9506, 222.1189],\n",
      "        [ 86.5140,  88.1934, 159.8624, 143.3725],\n",
      "        [332.3311, 184.1824, 382.7289, 222.0495],\n",
      "        [174.6513,  94.5329, 228.5246, 144.9637],\n",
      "        [133.3665, 249.4109, 222.3393, 286.5974],\n",
      "        [244.6157,  92.2655, 313.9451, 146.5514],\n",
      "        [272.1446, 259.1572, 331.4660, 289.2208],\n",
      "        [ 17.8932, 255.5981,  98.0953, 287.3080],\n",
      "        [416.5436, 194.8693, 454.6855, 218.9114],\n",
      "        [ 83.7238, 155.5521, 161.5458, 224.0206],\n",
      "        [403.4138,  93.1903, 468.3379, 146.9668],\n",
      "        [  1.5123,  88.7675,  83.9519, 142.5273]])\n",
      "xyxyn: tensor([[0.3567, 0.3659, 0.4831, 0.4732],\n",
      "        [0.5247, 0.3693, 0.6508, 0.4693],\n",
      "        [0.6768, 0.1944, 0.8172, 0.3078],\n",
      "        [0.8132, 0.5317, 0.9601, 0.6060],\n",
      "        [0.0206, 0.3626, 0.1561, 0.4627],\n",
      "        [0.1802, 0.1837, 0.3330, 0.2987],\n",
      "        [0.6924, 0.3837, 0.7974, 0.4626],\n",
      "        [0.3639, 0.1969, 0.4761, 0.3020],\n",
      "        [0.2778, 0.5196, 0.4632, 0.5971],\n",
      "        [0.5096, 0.1922, 0.6541, 0.3053],\n",
      "        [0.5670, 0.5399, 0.6906, 0.6025],\n",
      "        [0.0373, 0.5325, 0.2044, 0.5986],\n",
      "        [0.8678, 0.4060, 0.9473, 0.4561],\n",
      "        [0.1744, 0.3241, 0.3366, 0.4667],\n",
      "        [0.8404, 0.1941, 0.9757, 0.3062],\n",
      "        [0.0032, 0.1849, 0.1749, 0.2969]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/carvedilol-3-125-MG-24-Copy_jpg.rf.573785b008430ecf6d1e7b918d201b8e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2.])\n",
      "conf: tensor([0.9287])\n",
      "data: tensor([[  0.0000,  36.9368, 479.5530, 452.6374,   0.9287,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[239.7765, 244.7871, 479.5530, 415.7006]])\n",
      "xywhn: tensor([[0.4995, 0.5100, 0.9991, 0.8660]])\n",
      "xyxy: tensor([[  0.0000,  36.9368, 479.5530, 452.6374]])\n",
      "xyxyn: tensor([[0.0000, 0.0770, 0.9991, 0.9430]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240523_145320_jpg.rf.01b765efffec5d2bccc497afc368f7bb.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 1.])\n",
      "conf: tensor([0.8513, 0.2502])\n",
      "data: tensor([[9.1703e+01, 1.6753e+02, 4.3645e+02, 3.3795e+02, 8.5126e-01, 3.0000e+00],\n",
      "        [1.1233e+02, 1.9157e+02, 2.5679e+02, 3.3308e+02, 2.5024e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[264.0778, 252.7353, 344.7488, 170.4198],\n",
      "        [184.5592, 262.3215, 144.4672, 141.5085]])\n",
      "xywhn: tensor([[0.5502, 0.5265, 0.7182, 0.3550],\n",
      "        [0.3845, 0.5465, 0.3010, 0.2948]])\n",
      "xyxy: tensor([[ 91.7034, 167.5254, 436.4521, 337.9452],\n",
      "        [112.3256, 191.5673, 256.7928, 333.0758]])\n",
      "xyxyn: tensor([[0.1910, 0.3490, 0.9093, 0.7041],\n",
      "        [0.2340, 0.3991, 0.5350, 0.6939]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130019_jpg.rf.4e88f21266dd21a9083c3a29a76ee661.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.8883, 0.8429, 0.8303, 0.7726, 0.4033])\n",
      "data: tensor([[2.7869e+02, 1.5541e+02, 3.2134e+02, 2.4887e+02, 8.8825e-01, 1.0000e+00],\n",
      "        [3.4456e+02, 2.4927e+02, 4.2202e+02, 3.3736e+02, 8.4288e-01, 1.0000e+00],\n",
      "        [3.5339e+02, 1.3002e+02, 3.9890e+02, 2.1587e+02, 8.3035e-01, 1.0000e+00],\n",
      "        [1.5121e+02, 1.9800e+02, 2.0163e+02, 2.5093e+02, 7.7264e-01, 1.0000e+00],\n",
      "        [3.8978e+02, 1.3663e+02, 4.3917e+02, 2.1485e+02, 4.0331e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[300.0155, 202.1393,  42.6583,  93.4599],\n",
      "        [383.2919, 293.3140,  77.4613,  88.0849],\n",
      "        [376.1410, 172.9424,  45.5109,  85.8495],\n",
      "        [176.4207, 224.4665,  50.4198,  52.9298],\n",
      "        [414.4766, 175.7398,  49.3842,  78.2227]])\n",
      "xywhn: tensor([[0.6250, 0.4211, 0.0889, 0.1947],\n",
      "        [0.7985, 0.6111, 0.1614, 0.1835],\n",
      "        [0.7836, 0.3603, 0.0948, 0.1789],\n",
      "        [0.3675, 0.4676, 0.1050, 0.1103],\n",
      "        [0.8635, 0.3661, 0.1029, 0.1630]])\n",
      "xyxy: tensor([[278.6864, 155.4094, 321.3446, 248.8693],\n",
      "        [344.5612, 249.2715, 422.0225, 337.3564],\n",
      "        [353.3856, 130.0176, 398.8964, 215.8672],\n",
      "        [151.2108, 198.0016, 201.6306, 250.9314],\n",
      "        [389.7845, 136.6284, 439.1687, 214.8511]])\n",
      "xyxyn: tensor([[0.5806, 0.3238, 0.6695, 0.5185],\n",
      "        [0.7178, 0.5193, 0.8792, 0.7028],\n",
      "        [0.7362, 0.2709, 0.8310, 0.4497],\n",
      "        [0.3150, 0.4125, 0.4201, 0.5228],\n",
      "        [0.8121, 0.2846, 0.9149, 0.4476]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151126_Pinterest_jpg.rf.1eeb83dbee7f29789548aa21a7f96165.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8695, 0.8558, 0.8454, 0.8209, 0.7886, 0.6703, 0.2633])\n",
      "data: tensor([[2.5886e+02, 2.1809e+02, 4.6906e+02, 3.9346e+02, 8.6949e-01, 0.0000e+00],\n",
      "        [2.5056e+02, 5.5632e+01, 3.6328e+02, 1.5130e+02, 8.5581e-01, 0.0000e+00],\n",
      "        [3.4137e+02, 9.0056e+01, 4.6328e+02, 1.9195e+02, 8.4540e-01, 0.0000e+00],\n",
      "        [7.5403e+01, 1.3579e+02, 2.0224e+02, 1.9410e+02, 8.2092e-01, 0.0000e+00],\n",
      "        [1.3027e+02, 3.0088e+02, 2.3552e+02, 3.7430e+02, 7.8855e-01, 0.0000e+00],\n",
      "        [2.9359e+01, 2.2702e+02, 1.2424e+02, 3.0557e+02, 6.7026e-01, 0.0000e+00],\n",
      "        [1.2491e+02, 2.2018e+02, 2.3824e+02, 2.9849e+02, 2.6330e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[363.9616, 305.7733, 210.2041, 175.3678],\n",
      "        [306.9204, 103.4653, 112.7152,  95.6675],\n",
      "        [402.3275, 141.0017, 121.9147, 101.8909],\n",
      "        [138.8229, 164.9423, 126.8405,  58.3105],\n",
      "        [182.8950, 337.5916, 105.2407,  73.4137],\n",
      "        [ 76.7974, 266.2984,  94.8764,  78.5491],\n",
      "        [181.5744, 259.3399, 113.3219,  78.3102]])\n",
      "xywhn: tensor([[0.7583, 0.6370, 0.4379, 0.3653],\n",
      "        [0.6394, 0.2156, 0.2348, 0.1993],\n",
      "        [0.8382, 0.2938, 0.2540, 0.2123],\n",
      "        [0.2892, 0.3436, 0.2643, 0.1215],\n",
      "        [0.3810, 0.7033, 0.2193, 0.1529],\n",
      "        [0.1600, 0.5548, 0.1977, 0.1636],\n",
      "        [0.3783, 0.5403, 0.2361, 0.1631]])\n",
      "xyxy: tensor([[258.8596, 218.0894, 469.0637, 393.4572],\n",
      "        [250.5628,  55.6315, 363.2780, 151.2990],\n",
      "        [341.3701,  90.0562, 463.2849, 191.9471],\n",
      "        [ 75.4026, 135.7870, 202.2431, 194.0975],\n",
      "        [130.2746, 300.8847, 235.5153, 374.2984],\n",
      "        [ 29.3592, 227.0239, 124.2356, 305.5729],\n",
      "        [124.9135, 220.1848, 238.2354, 298.4950]])\n",
      "xyxyn: tensor([[0.5393, 0.4544, 0.9772, 0.8197],\n",
      "        [0.5220, 0.1159, 0.7568, 0.3152],\n",
      "        [0.7112, 0.1876, 0.9652, 0.3999],\n",
      "        [0.1571, 0.2829, 0.4213, 0.4044],\n",
      "        [0.2714, 0.6268, 0.4907, 0.7798],\n",
      "        [0.0612, 0.4730, 0.2588, 0.6366],\n",
      "        [0.2602, 0.4587, 0.4963, 0.6219]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100209_Pinterest_jpg.rf.f92117fe6f5e0ee4664fd49312a4d819.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8309, 0.8188])\n",
      "data: tensor([[ 45.6787,   3.5302, 299.3145, 241.5805,   0.8309,   0.0000],\n",
      "        [283.3275,  44.8105, 480.0000, 270.6647,   0.8188,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[172.4966, 122.5554, 253.6357, 238.0503],\n",
      "        [381.6637, 157.7376, 196.6725, 225.8541]])\n",
      "xywhn: tensor([[0.3594, 0.2553, 0.5284, 0.4959],\n",
      "        [0.7951, 0.3286, 0.4097, 0.4705]])\n",
      "xyxy: tensor([[ 45.6787,   3.5302, 299.3145, 241.5805],\n",
      "        [283.3275,  44.8105, 480.0000, 270.6647]])\n",
      "xyxyn: tensor([[0.0952, 0.0074, 0.6236, 0.5033],\n",
      "        [0.5903, 0.0934, 1.0000, 0.5639]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_101020_Pinterest_jpg.rf.30479970832f04b364ed9f7a8c02b39a.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8713])\n",
      "data: tensor([[  1.7303, 115.2248, 479.1631, 229.0813,   0.8713,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[240.4467, 172.1531, 477.4327, 113.8564]])\n",
      "xywhn: tensor([[0.5009, 0.3587, 0.9947, 0.2372]])\n",
      "xyxy: tensor([[  1.7303, 115.2248, 479.1631, 229.0813]])\n",
      "xyxyn: tensor([[0.0036, 0.2401, 0.9983, 0.4773]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_093900_Pinterest_jpg.rf.a03648da6828b3e127f7d0a1cba05da5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8286, 0.8019])\n",
      "data: tensor([[242.0206, 139.6683, 463.2244, 321.5753,   0.8286,   0.0000],\n",
      "        [ 91.0747,  28.3460, 309.1406, 207.0703,   0.8019,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[352.6225, 230.6218, 221.2039, 181.9070],\n",
      "        [200.1077, 117.7082, 218.0659, 178.7243]])\n",
      "xywhn: tensor([[0.7346, 0.4805, 0.4608, 0.3790],\n",
      "        [0.4169, 0.2452, 0.4543, 0.3723]])\n",
      "xyxy: tensor([[242.0206, 139.6683, 463.2244, 321.5753],\n",
      "        [ 91.0747,  28.3460, 309.1406, 207.0703]])\n",
      "xyxyn: tensor([[0.5042, 0.2910, 0.9651, 0.6699],\n",
      "        [0.1897, 0.0591, 0.6440, 0.4314]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151456_Pinterest_jpg.rf.198c0f50d122e7083e8cda97754ce2c5.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.8965])\n",
      "data: tensor([[  0.0000,  45.6275, 479.3239, 205.0355,   0.8965,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[239.6619, 125.3315, 479.3239, 159.4079]])\n",
      "xywhn: tensor([[0.4993, 0.2611, 0.9986, 0.3321]])\n",
      "xyxy: tensor([[  0.0000,  45.6275, 479.3239, 205.0355]])\n",
      "xyxyn: tensor([[0.0000, 0.0951, 0.9986, 0.4272]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151310_Pinterest_jpg.rf.b8a04bf82ee4459c66d86ec92e0c2170.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8394, 0.6914, 0.6543, 0.5706, 0.3317])\n",
      "data: tensor([[4.4911e+01, 6.7580e+01, 2.4863e+02, 2.1353e+02, 8.3936e-01, 0.0000e+00],\n",
      "        [3.7919e+02, 1.3842e+02, 4.7803e+02, 2.0401e+02, 6.9139e-01, 0.0000e+00],\n",
      "        [2.4818e+02, 2.1719e+02, 3.8489e+02, 2.8369e+02, 6.5428e-01, 0.0000e+00],\n",
      "        [2.4525e+02, 3.2274e+02, 4.3977e+02, 4.2912e+02, 5.7061e-01, 0.0000e+00],\n",
      "        [1.8009e+01, 2.6976e+02, 1.4010e+02, 3.4753e+02, 3.3167e-01, 0.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[146.7728, 140.5537, 203.7243, 145.9480],\n",
      "        [428.6064, 171.2177,  98.8381,  65.5877],\n",
      "        [316.5319, 250.4437, 136.7094,  66.4976],\n",
      "        [342.5096, 375.9296, 194.5214, 106.3744],\n",
      "        [ 79.0533, 308.6451, 122.0893,  77.7778]])\n",
      "xywhn: tensor([[0.3058, 0.2928, 0.4244, 0.3041],\n",
      "        [0.8929, 0.3567, 0.2059, 0.1366],\n",
      "        [0.6594, 0.5218, 0.2848, 0.1385],\n",
      "        [0.7136, 0.7832, 0.4053, 0.2216],\n",
      "        [0.1647, 0.6430, 0.2544, 0.1620]])\n",
      "xyxy: tensor([[ 44.9106,  67.5797, 248.6349, 213.5276],\n",
      "        [379.1874, 138.4238, 478.0255, 204.0115],\n",
      "        [248.1772, 217.1949, 384.8866, 283.6925],\n",
      "        [245.2489, 322.7424, 439.7703, 429.1168],\n",
      "        [ 18.0086, 269.7562, 140.0979, 347.5341]])\n",
      "xyxyn: tensor([[0.0936, 0.1408, 0.5180, 0.4448],\n",
      "        [0.7900, 0.2884, 0.9959, 0.4250],\n",
      "        [0.5170, 0.4525, 0.8018, 0.5910],\n",
      "        [0.5109, 0.6724, 0.9162, 0.8940],\n",
      "        [0.0375, 0.5620, 0.2919, 0.7240]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_104736_Pinterest_jpg.rf.dce3b7e0254b4031c655d59e2e9dc545.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8586, 0.8521, 0.8408, 0.8244, 0.7711, 0.7607, 0.7371])\n",
      "data: tensor([[ 37.9724, 104.6212, 204.6580, 241.8774,   0.8586,   0.0000],\n",
      "        [340.7387, 295.3779, 464.0618, 361.5916,   0.8521,   0.0000],\n",
      "        [245.5024, 157.4239, 336.3525, 228.0029,   0.8408,   0.0000],\n",
      "        [ 57.2001, 279.5131, 194.4840, 382.2371,   0.8244,   0.0000],\n",
      "        [324.3465, 171.8735, 449.3719, 232.4214,   0.7711,   0.0000],\n",
      "        [277.1402,   8.5788, 427.8047,  97.0219,   0.7607,   0.0000],\n",
      "        [260.2045, 335.7652, 381.0426, 403.1353,   0.7371,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[121.3152, 173.2493, 166.6856, 137.2561],\n",
      "        [402.4003, 328.4847, 123.3231,  66.2137],\n",
      "        [290.9275, 192.7134,  90.8501,  70.5791],\n",
      "        [125.8420, 330.8751, 137.2839, 102.7240],\n",
      "        [386.8592, 202.1474, 125.0253,  60.5478],\n",
      "        [352.4725,  52.8004, 150.6646,  88.4431],\n",
      "        [320.6235, 369.4503, 120.8381,  67.3701]])\n",
      "xywhn: tensor([[0.2527, 0.3609, 0.3473, 0.2860],\n",
      "        [0.8383, 0.6843, 0.2569, 0.1379],\n",
      "        [0.6061, 0.4015, 0.1893, 0.1470],\n",
      "        [0.2622, 0.6893, 0.2860, 0.2140],\n",
      "        [0.8060, 0.4211, 0.2605, 0.1261],\n",
      "        [0.7343, 0.1100, 0.3139, 0.1843],\n",
      "        [0.6680, 0.7697, 0.2517, 0.1404]])\n",
      "xyxy: tensor([[ 37.9724, 104.6212, 204.6580, 241.8774],\n",
      "        [340.7387, 295.3779, 464.0618, 361.5916],\n",
      "        [245.5024, 157.4239, 336.3525, 228.0029],\n",
      "        [ 57.2001, 279.5131, 194.4840, 382.2371],\n",
      "        [324.3465, 171.8735, 449.3719, 232.4214],\n",
      "        [277.1402,   8.5788, 427.8047,  97.0219],\n",
      "        [260.2045, 335.7652, 381.0426, 403.1353]])\n",
      "xyxyn: tensor([[0.0791, 0.2180, 0.4264, 0.5039],\n",
      "        [0.7099, 0.6154, 0.9668, 0.7533],\n",
      "        [0.5115, 0.3280, 0.7007, 0.4750],\n",
      "        [0.1192, 0.5823, 0.4052, 0.7963],\n",
      "        [0.6757, 0.3581, 0.9362, 0.4842],\n",
      "        [0.5774, 0.0179, 0.8913, 0.2021],\n",
      "        [0.5421, 0.6995, 0.7938, 0.8399]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/20240531_130241_jpg.rf.c58ff21c0649e4340851bd63156bf94a.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([1., 1., 1., 1., 1.])\n",
      "conf: tensor([0.7294, 0.6425, 0.5805, 0.5188, 0.4574])\n",
      "data: tensor([[282.2217, 229.7358, 362.6501, 300.8275,   0.7294,   1.0000],\n",
      "        [178.2480, 114.5575, 223.1009, 227.7031,   0.6425,   1.0000],\n",
      "        [178.2573, 115.4764, 221.0848, 191.0109,   0.5805,   1.0000],\n",
      "        [304.5378, 204.7603, 363.8216, 249.9791,   0.5188,   1.0000],\n",
      "        [145.7355, 114.4592, 223.1019, 227.3055,   0.4574,   1.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([5, 6])\n",
      "xywh: tensor([[322.4359, 265.2816,  80.4285,  71.0916],\n",
      "        [200.6744, 171.1303,  44.8528, 113.1456],\n",
      "        [199.6711, 153.2436,  42.8275,  75.5345],\n",
      "        [334.1797, 227.3697,  59.2839,  45.2189],\n",
      "        [184.4187, 170.8823,  77.3664, 112.8464]])\n",
      "xywhn: tensor([[0.6717, 0.5527, 0.1676, 0.1481],\n",
      "        [0.4181, 0.3565, 0.0934, 0.2357],\n",
      "        [0.4160, 0.3193, 0.0892, 0.1574],\n",
      "        [0.6962, 0.4737, 0.1235, 0.0942],\n",
      "        [0.3842, 0.3560, 0.1612, 0.2351]])\n",
      "xyxy: tensor([[282.2217, 229.7358, 362.6501, 300.8275],\n",
      "        [178.2480, 114.5575, 223.1009, 227.7031],\n",
      "        [178.2573, 115.4764, 221.0848, 191.0109],\n",
      "        [304.5378, 204.7603, 363.8216, 249.9791],\n",
      "        [145.7355, 114.4592, 223.1019, 227.3055]])\n",
      "xyxyn: tensor([[0.5880, 0.4786, 0.7555, 0.6267],\n",
      "        [0.3714, 0.2387, 0.4648, 0.4744],\n",
      "        [0.3714, 0.2406, 0.4606, 0.3979],\n",
      "        [0.6345, 0.4266, 0.7580, 0.5208],\n",
      "        [0.3036, 0.2385, 0.4648, 0.4736]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/carvedilol-3-125-MG-7-Copy-Copy_jpg.rf.f224a03f1268e2e7cba8bb1e0c61ef88.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.8906, 0.8826])\n",
      "data: tensor([[193.4285,  87.7084, 391.8661, 376.1737,   0.8906,   2.0000],\n",
      "        [  1.5009,  88.3055, 196.3569, 376.9348,   0.8826,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[292.6473, 231.9410, 198.4376, 288.4653],\n",
      "        [ 98.9289, 232.6202, 194.8560, 288.6293]])\n",
      "xywhn: tensor([[0.6097, 0.4832, 0.4134, 0.6010],\n",
      "        [0.2061, 0.4846, 0.4059, 0.6013]])\n",
      "xyxy: tensor([[193.4285,  87.7084, 391.8661, 376.1737],\n",
      "        [  1.5009,  88.3055, 196.3569, 376.9348]])\n",
      "xyxyn: tensor([[0.4030, 0.1827, 0.8164, 0.7837],\n",
      "        [0.0031, 0.1840, 0.4091, 0.7853]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100534_Pinterest_jpg.rf.adf2b6f4f6f943ebb52e3bfd31669b27.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8451])\n",
      "data: tensor([[154.2587,  32.0946, 412.4393, 198.6503,   0.8451,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[283.3490, 115.3725, 258.1805, 166.5558]])\n",
      "xywhn: tensor([[0.5903, 0.2404, 0.5379, 0.3470]])\n",
      "xyxy: tensor([[154.2587,  32.0946, 412.4393, 198.6503]])\n",
      "xyxyn: tensor([[0.3214, 0.0669, 0.8592, 0.4139]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240523_100750_Pinterest_jpg.rf.4cb27e5a068f39a818e36ce67cdcb15e.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0.])\n",
      "conf: tensor([0.8986, 0.8610])\n",
      "data: tensor([[ 24.7398,  22.2402, 229.0160, 160.0030,   0.8986,   0.0000],\n",
      "        [227.7774,  16.1453, 435.6025, 153.3520,   0.8610,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[126.8779,  91.1216, 204.2762, 137.7628],\n",
      "        [331.6900,  84.7487, 207.8251, 137.2067]])\n",
      "xywhn: tensor([[0.2643, 0.1898, 0.4256, 0.2870],\n",
      "        [0.6910, 0.1766, 0.4330, 0.2858]])\n",
      "xyxy: tensor([[ 24.7398,  22.2402, 229.0160, 160.0030],\n",
      "        [227.7774,  16.1453, 435.6025, 153.3520]])\n",
      "xyxyn: tensor([[0.0515, 0.0463, 0.4771, 0.3333],\n",
      "        [0.4745, 0.0336, 0.9075, 0.3195]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151302_Pinterest_jpg.rf.4556a63d2d343c8594614cabbe2ee6c9.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0.])\n",
      "conf: tensor([0.8327, 0.8239, 0.8205, 0.7905, 0.7123, 0.6803])\n",
      "data: tensor([[304.9036, 229.5063, 416.4592, 349.4384,   0.8327,   0.0000],\n",
      "        [270.3932,  96.5475, 415.4600, 183.1646,   0.8239,   0.0000],\n",
      "        [ 27.4021, 243.6459, 169.2615, 328.4378,   0.8205,   0.0000],\n",
      "        [ 21.0711,  44.0390, 198.2005, 116.6656,   0.7905,   0.0000],\n",
      "        [ 13.6689, 154.5749, 114.7117, 207.0736,   0.7123,   0.0000],\n",
      "        [ 89.1338, 138.1974, 229.3225, 191.4895,   0.6803,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[360.6814, 289.4724, 111.5555, 119.9321],\n",
      "        [342.9266, 139.8560, 145.0668,  86.6171],\n",
      "        [ 98.3318, 286.0418, 141.8594,  84.7919],\n",
      "        [109.6358,  80.3523, 177.1295,  72.6266],\n",
      "        [ 64.1903, 180.8243, 101.0428,  52.4987],\n",
      "        [159.2281, 164.8435, 140.1887,  53.2921]])\n",
      "xywhn: tensor([[0.7514, 0.6031, 0.2324, 0.2499],\n",
      "        [0.7144, 0.2914, 0.3022, 0.1805],\n",
      "        [0.2049, 0.5959, 0.2955, 0.1766],\n",
      "        [0.2284, 0.1674, 0.3690, 0.1513],\n",
      "        [0.1337, 0.3767, 0.2105, 0.1094],\n",
      "        [0.3317, 0.3434, 0.2921, 0.1110]])\n",
      "xyxy: tensor([[304.9036, 229.5063, 416.4592, 349.4384],\n",
      "        [270.3932,  96.5475, 415.4600, 183.1646],\n",
      "        [ 27.4021, 243.6459, 169.2615, 328.4378],\n",
      "        [ 21.0711,  44.0390, 198.2005, 116.6656],\n",
      "        [ 13.6689, 154.5749, 114.7117, 207.0736],\n",
      "        [ 89.1338, 138.1974, 229.3225, 191.4895]])\n",
      "xyxyn: tensor([[0.6352, 0.4781, 0.8676, 0.7280],\n",
      "        [0.5633, 0.2011, 0.8655, 0.3816],\n",
      "        [0.0571, 0.5076, 0.3526, 0.6842],\n",
      "        [0.0439, 0.0917, 0.4129, 0.2431],\n",
      "        [0.0285, 0.3220, 0.2390, 0.4314],\n",
      "        [0.1857, 0.2879, 0.4778, 0.3989]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240528_151657_Pinterest_jpg.rf.2566b4ab8664fe4b6a5b74df2cd500dd.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3., 3., 3., 3., 1., 3.])\n",
      "conf: tensor([0.8725, 0.8376, 0.8171, 0.5700, 0.5674, 0.4934])\n",
      "data: tensor([[ 13.8962, 267.3029, 233.7313, 351.6948,   0.8725,   3.0000],\n",
      "        [ 21.2387, 120.1396, 229.4361, 190.7319,   0.8376,   3.0000],\n",
      "        [245.9832,  20.0675, 417.7152,  92.0562,   0.8171,   3.0000],\n",
      "        [266.8391, 304.2789, 454.8965, 359.1535,   0.5700,   3.0000],\n",
      "        [138.2185, 179.1792, 231.4449, 223.9365,   0.5674,   1.0000],\n",
      "        [248.1883, 169.7011, 443.7977, 228.0045,   0.4934,   3.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[123.8137, 309.4988, 219.8351,  84.3918],\n",
      "        [125.3374, 155.4357, 208.1974,  70.5923],\n",
      "        [331.8492,  56.0618, 171.7320,  71.9887],\n",
      "        [360.8678, 331.7162, 188.0575,  54.8746],\n",
      "        [184.8317, 201.5578,  93.2263,  44.7573],\n",
      "        [345.9930, 198.8528, 195.6094,  58.3034]])\n",
      "xywhn: tensor([[0.2579, 0.6448, 0.4580, 0.1758],\n",
      "        [0.2611, 0.3238, 0.4337, 0.1471],\n",
      "        [0.6914, 0.1168, 0.3578, 0.1500],\n",
      "        [0.7518, 0.6911, 0.3918, 0.1143],\n",
      "        [0.3851, 0.4199, 0.1942, 0.0932],\n",
      "        [0.7208, 0.4143, 0.4075, 0.1215]])\n",
      "xyxy: tensor([[ 13.8962, 267.3029, 233.7313, 351.6948],\n",
      "        [ 21.2387, 120.1396, 229.4361, 190.7319],\n",
      "        [245.9832,  20.0675, 417.7152,  92.0562],\n",
      "        [266.8391, 304.2789, 454.8965, 359.1535],\n",
      "        [138.2185, 179.1792, 231.4449, 223.9365],\n",
      "        [248.1883, 169.7011, 443.7977, 228.0045]])\n",
      "xyxyn: tensor([[0.0290, 0.5569, 0.4869, 0.7327],\n",
      "        [0.0442, 0.2503, 0.4780, 0.3974],\n",
      "        [0.5125, 0.0418, 0.8702, 0.1918],\n",
      "        [0.5559, 0.6339, 0.9477, 0.7482],\n",
      "        [0.2880, 0.3733, 0.4822, 0.4665],\n",
      "        [0.5171, 0.3535, 0.9246, 0.4750]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/celecoxib-200-MG-11-Copy-Copy_jpg.rf.15a92bfbd3268f93ab421e286e008d5a.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2.])\n",
      "conf: tensor([0.9384, 0.9279])\n",
      "data: tensor([[6.8095e+00, 2.1259e-01, 4.8000e+02, 1.8992e+02, 9.3841e-01, 2.0000e+00],\n",
      "        [8.9703e+00, 1.9070e+02, 4.8000e+02, 4.1367e+02, 9.2792e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([2, 6])\n",
      "xywh: tensor([[243.4048,  95.0661, 473.1905, 189.7071],\n",
      "        [244.4851, 302.1851, 471.0297, 222.9776]])\n",
      "xywhn: tensor([[0.5071, 0.1981, 0.9858, 0.3952],\n",
      "        [0.5093, 0.6296, 0.9813, 0.4645]])\n",
      "xyxy: tensor([[6.8095e+00, 2.1259e-01, 4.8000e+02, 1.8992e+02],\n",
      "        [8.9703e+00, 1.9070e+02, 4.8000e+02, 4.1367e+02]])\n",
      "xyxyn: tensor([[1.4187e-02, 4.4289e-04, 1.0000e+00, 3.9567e-01],\n",
      "        [1.8688e-02, 3.9728e-01, 1.0000e+00, 8.6182e-01]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/Screenshot_20240522_210046_SHEIN_jpg.rf.f75e6fd9c68f57eedb85d63572d32117.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([3.])\n",
      "conf: tensor([0.3524])\n",
      "data: tensor([[2.7355e+02, 9.3428e+01, 4.3812e+02, 3.7716e+02, 3.5239e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[355.8344, 235.2955, 164.5647, 283.7341]])\n",
      "xywhn: tensor([[0.7413, 0.4902, 0.3428, 0.5911]])\n",
      "xyxy: tensor([[273.5521,  93.4285, 438.1168, 377.1625]])\n",
      "xyxyn: tensor([[0.5699, 0.1946, 0.9127, 0.7858]])\n",
      "Image: /Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images/medicine-4-_jpg.rf.20b9cb609c87be6f6dff2e78af16926d.jpg\n",
      "Predictions: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([2., 2., 2.])\n",
      "conf: tensor([0.8481, 0.8180, 0.7915])\n",
      "data: tensor([[192.3564, 216.9159, 281.3752, 315.7494,   0.8481,   2.0000],\n",
      "        [114.1243, 246.5110, 215.5164, 358.4411,   0.8180,   2.0000],\n",
      "        [219.3100,  96.3045, 327.2988, 199.8761,   0.7915,   2.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 480)\n",
      "shape: torch.Size([3, 6])\n",
      "xywh: tensor([[236.8658, 266.3326,  89.0188,  98.8335],\n",
      "        [164.8204, 302.4760, 101.3922, 111.9301],\n",
      "        [273.3044, 148.0903, 107.9888, 103.5716]])\n",
      "xywhn: tensor([[0.4935, 0.5549, 0.1855, 0.2059],\n",
      "        [0.3434, 0.6302, 0.2112, 0.2332],\n",
      "        [0.5694, 0.3085, 0.2250, 0.2158]])\n",
      "xyxy: tensor([[192.3564, 216.9159, 281.3752, 315.7494],\n",
      "        [114.1243, 246.5110, 215.5164, 358.4411],\n",
      "        [219.3100,  96.3045, 327.2988, 199.8761]])\n",
      "xyxyn: tensor([[0.4007, 0.4519, 0.5862, 0.6578],\n",
      "        [0.2378, 0.5136, 0.4490, 0.7468],\n",
      "        [0.4569, 0.2006, 0.6819, 0.4164]])\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "model = YOLO('runs/detect/train2/weights/best.pt')  # Path to your trained model\n",
    "\n",
    "# Path to the test images\n",
    "test_images_path = '/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/test/images'\n",
    "\n",
    "# Get all test images\n",
    "test_images = [os.path.join(test_images_path, img) for img in os.listdir(test_images_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Run inference on the test set\n",
    "results = model.predict(test_images, save=True, save_txt=True, save_conf=True, save_crop=False)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"Image: {result.path}\")\n",
    "    print(f\"Predictions: {result.boxes}\")\n",
    "\n",
    "# Results will be saved in the 'runs' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@17869.066] global /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_f6tvh9615u/croot/opencv-suite_1691620375715/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 288x480 1 Hands, 2 Opens, 115.9ms\n",
      "Speed: 10.4ms preprocess, 115.9ms inference, 15.8ms postprocess per image at shape (1, 3, 288, 480)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W6sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W6sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m shutdown_flag\u001b[39m.\u001b[39mset()  \u001b[39m# Signal the receive thread to shut down\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W6sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m receive_thread\u001b[39m.\u001b[39;49mjoin()  \u001b[39m# Wait for the receive thread to finish\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W6sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m client_socket\u001b[39m.\u001b[39mclose()  \u001b[39m# Close the socket connection to the Raspberry Pi\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W6sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39m# \\end{code}\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W6sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W6sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m \u001b[39m# Comment: It is always a good idea to check if a TCP socket connection was successfully established before proceeding with any communication. In your case, the client code doesn't check if the connection was successful before calling `receive_messages()`. To solve this issue, you can modify the client code as follows:\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W6sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W6sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39m# ```python\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W6sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39m# Create a socket instance\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'join'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "import socket\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "try:\n",
    "    model = YOLO('runs/detect/train2/weights/best.pt')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define the class names\n",
    "class_names = ['Hands', 'Open', 'Pill', 'PillBox']\n",
    "\n",
    "# Server address and port for Raspberry Pi\n",
    "server_address = ('172.30.248.211', 1442)  # Change this to your Raspberry Pi IP and port\n",
    "\n",
    "# Global vars for use in methods/threads\n",
    "client_socket = None\n",
    "receive_thread = None\n",
    "shutdown_flag = threading.Event()\n",
    "last_notification_time = 0\n",
    "notification_interval = 1\n",
    "\n",
    "def setup_socket_client():\n",
    "    global client_socket, receive_thread\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  # Create a socket instance\n",
    "    client_socket.connect(server_address)  # Connect to specified server\n",
    "    print(\"Connected to server\")\n",
    "\n",
    "    receive_thread = threading.Thread(target=receive_messages, args=(client_socket, shutdown_flag))\n",
    "    receive_thread.start()\n",
    "\n",
    "def receive_messages(sock, shutdown_flag):\n",
    "    sock.settimeout(1)  # Set a timeout on the socket so we can check shutdown_flag.is_set in the loop, instead of blocking\n",
    "    try:\n",
    "        while not shutdown_flag.is_set():  # As long as ctrl+c is not pressed\n",
    "            try:\n",
    "                data = sock.recv(1024)  # Try to receive 1024 bytes of data (maximum amount; can be less)\n",
    "                if not data:  # When no data is received, try again (and shutdown flag is checked again)\n",
    "                    break\n",
    "                print(\"Received from server:\", data.decode())  # Print the received data, or do something with it\n",
    "            except socket.timeout:  # When no data comes within timeout, try again\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        if not shutdown_flag.is_set():\n",
    "            print(f\"Connection error: {e}\")\n",
    "    finally:\n",
    "        sock.close()\n",
    "\n",
    "# Function to draw bounding boxes with different colors\n",
    "def draw_bounding_boxes(image, results):\n",
    "    global last_notification_time\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    confs = results[0].boxes.conf.cpu().numpy()\n",
    "    classes = results[0].boxes.cls.cpu().numpy()\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        conf = confs[i]\n",
    "        cls = int(classes[i])\n",
    "        label = class_names[cls]\n",
    "        if label == 'Hands':\n",
    "            color = (0, 255, 0)  # Green\n",
    "        elif label == 'Pills':\n",
    "            color = (255, 0, 0)  # Red\n",
    "            # Send notification to Raspberry Pi when wrong pill is detected\n",
    "            current_time = time.time()\n",
    "            if current_time - last_notification_time >= notification_interval:\n",
    "                notify_raspberry_pi(\"Wrong pill detected\")\n",
    "                last_notification_time = current_time\n",
    "        else:\n",
    "            color = (0, 0, 255)  # Blue\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.line(image, (x1, (y1 + y2) // 2), (x2, (y1 + y2) // 2), color, 1)  # Draw horizontal line in the middle of the box\n",
    "        cv2.putText(image, f'{label} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "    return image\n",
    "\n",
    "# Function to notify Raspberry Pi\n",
    "def notify_raspberry_pi(message):\n",
    "    try:\n",
    "        client_socket.sendall(message.encode())\n",
    "        print(f\"Sent to Raspberry Pi: {message}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending message to Raspberry Pi: {e}\")\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default webcam, or specify the webcam index\n",
    "\n",
    "def update_frame():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading frame from webcam\")\n",
    "        return\n",
    "\n",
    "    # Perform inference with adjusted NMS settings\n",
    "    try:\n",
    "        results = model(frame)\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing inference: {e}\")\n",
    "        return\n",
    "\n",
    "    # Annotate the frame with detection results\n",
    "    frame = draw_bounding_boxes(frame, results)\n",
    "\n",
    "    # Convert the frame to an ImageTk object\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_img = Image.fromarray(frame_rgb)\n",
    "    frame_tk = ImageTk.PhotoImage(frame_img)\n",
    "\n",
    "    # Update the ImageTk object in the Label widget\n",
    "    label.config(image=frame_tk)\n",
    "    label.image = frame_tk\n",
    "\n",
    "# Set up the Tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Pill Identifier\")\n",
    "\n",
    "# Set up the Label widget to display the frames from the webcam\n",
    "frame_width, frame_height = cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "label = tk.Label(root, text=\"Webcam output\", bg=\"white\")\n",
    "label.pack(side=\"top\", fill=\"both\", expand=True)\n",
    "\n",
    "# Start the webcam updater function in a separate thread\n",
    "cap_thread = threading.Thread(target=update_frame)\n",
    "cap_thread.start()\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "root.mainloop()\n",
    "\n",
    "# Clean up when the GUI is closed\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "shutdown_flag.set()  # Signal the receive thread to shut down\n",
    "receive_thread.join()  # Wait for the receive thread to finish\n",
    "client_socket.close()  # Close the socket connection to the Raspberry Pi\n",
    "# \\end{code}\n",
    "\n",
    "# Comment: It is always a good idea to check if a TCP socket connection was successfully established before proceeding with any communication. In your case, the client code doesn't check if the connection was successful before calling `receive_messages()`. To solve this issue, you can modify the client code as follows:\n",
    "\n",
    "# ```python\n",
    "# Create a socket instance\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Try to connect to the server\n",
    "try:\n",
    "    client_socket.connect(server_address)\n",
    "    print(\"Connected to server\")\n",
    "\n",
    "    receive_thread = threading.Thread(target=receive_messages, args=(client_socket, shutdown_flag))\n",
    "    receive_thread.start()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to server: {e}\")\n",
    "    shutdown_flag.set()  # Signal the receive thread to shut down\n",
    "    client_socket.close()  # Close the socket connection to the Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@329.346] global /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_f6tvh9615u/croot/opencv-suite_1691620375715/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 288x480 (no detections), 76.0ms\n",
      "Speed: 3.1ms preprocess, 76.0ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 46.3ms\n",
      "Speed: 1.1ms preprocess, 46.3ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 47.6ms\n",
      "Speed: 1.1ms preprocess, 47.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.0ms\n",
      "Speed: 1.6ms preprocess, 45.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 2.0ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 0.9ms preprocess, 38.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.0ms\n",
      "Speed: 1.3ms preprocess, 44.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.5ms\n",
      "Speed: 1.2ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.8ms\n",
      "Speed: 1.3ms preprocess, 39.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.0ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.9ms\n",
      "Speed: 2.1ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.9ms\n",
      "Speed: 1.0ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 40.2ms\n",
      "Speed: 1.3ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 62.5ms\n",
      "Speed: 1.2ms preprocess, 62.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.6ms\n",
      "Speed: 1.3ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.3ms\n",
      "Speed: 1.3ms preprocess, 39.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.4ms\n",
      "Speed: 1.1ms preprocess, 39.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 36.8ms\n",
      "Speed: 1.0ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.7ms\n",
      "Speed: 1.4ms preprocess, 38.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.6ms\n",
      "Speed: 1.1ms preprocess, 37.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 36.7ms\n",
      "Speed: 1.1ms preprocess, 36.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.6ms\n",
      "Speed: 1.3ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.2ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.2ms\n",
      "Speed: 1.3ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.4ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.6ms\n",
      "Speed: 1.1ms preprocess, 42.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.3ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 56.9ms\n",
      "Speed: 1.0ms preprocess, 56.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 101.4ms\n",
      "Speed: 2.7ms preprocess, 101.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.5ms\n",
      "Speed: 1.0ms preprocess, 41.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.0ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.1ms preprocess, 39.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.9ms\n",
      "Speed: 1.2ms preprocess, 37.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 36.9ms\n",
      "Speed: 1.0ms preprocess, 36.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 41.3ms\n",
      "Speed: 1.1ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.6ms\n",
      "Speed: 1.3ms preprocess, 38.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 43.2ms\n",
      "Speed: 1.3ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.2ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 37.5ms\n",
      "Speed: 1.1ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.6ms\n",
      "Speed: 1.1ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 36.8ms\n",
      "Speed: 1.1ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 1.2ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 61.5ms\n",
      "Speed: 1.0ms preprocess, 61.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 54.6ms\n",
      "Speed: 1.1ms preprocess, 54.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 59.9ms\n",
      "Speed: 1.7ms preprocess, 59.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 PillBox, 47.7ms\n",
      "Speed: 1.0ms preprocess, 47.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.9ms\n",
      "Speed: 10.7ms preprocess, 45.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.0ms\n",
      "Speed: 1.2ms preprocess, 39.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.9ms\n",
      "Speed: 1.2ms preprocess, 36.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.8ms\n",
      "Speed: 1.2ms preprocess, 36.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 75.0ms\n",
      "Speed: 1.1ms preprocess, 75.0ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 43.4ms\n",
      "Speed: 1.4ms preprocess, 43.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 59.2ms\n",
      "Speed: 1.2ms preprocess, 59.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.6ms\n",
      "Speed: 1.3ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.7ms\n",
      "Speed: 1.1ms preprocess, 40.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.1ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 45.6ms\n",
      "Speed: 1.1ms preprocess, 45.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.6ms\n",
      "Speed: 1.2ms preprocess, 41.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.2ms preprocess, 39.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.1ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.8ms\n",
      "Speed: 1.3ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.6ms\n",
      "Speed: 1.1ms preprocess, 40.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 0.9ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.1ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.7ms\n",
      "Speed: 1.4ms preprocess, 43.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 40.8ms\n",
      "Speed: 1.1ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.4ms\n",
      "Speed: 1.2ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.3ms\n",
      "Speed: 1.3ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.1ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.2ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 0.9ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.6ms\n",
      "Speed: 1.2ms preprocess, 43.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 60.5ms\n",
      "Speed: 1.0ms preprocess, 60.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.3ms\n",
      "Speed: 1.1ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.8ms\n",
      "Speed: 1.1ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 45.8ms\n",
      "Speed: 1.3ms preprocess, 45.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 42.1ms\n",
      "Speed: 1.2ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.6ms\n",
      "Speed: 1.2ms preprocess, 39.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.3ms\n",
      "Speed: 1.1ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 41.1ms\n",
      "Speed: 1.2ms preprocess, 41.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 40.4ms\n",
      "Speed: 1.2ms preprocess, 40.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 36.6ms\n",
      "Speed: 1.2ms preprocess, 36.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.2ms\n",
      "Speed: 1.3ms preprocess, 38.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.0ms\n",
      "Speed: 1.3ms preprocess, 41.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.8ms\n",
      "Speed: 1.2ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.2ms\n",
      "Speed: 1.0ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.3ms\n",
      "Speed: 1.4ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 45.9ms\n",
      "Speed: 1.0ms preprocess, 45.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.3ms\n",
      "Speed: 1.2ms preprocess, 40.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.4ms\n",
      "Speed: 1.2ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.4ms\n",
      "Speed: 1.2ms preprocess, 40.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.3ms\n",
      "Speed: 1.0ms preprocess, 37.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.2ms\n",
      "Speed: 1.1ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.9ms\n",
      "Speed: 0.9ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.1ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.0ms\n",
      "Speed: 1.3ms preprocess, 43.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 36.9ms\n",
      "Speed: 1.1ms preprocess, 36.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.0ms\n",
      "Speed: 1.0ms preprocess, 39.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.0ms\n",
      "Speed: 1.4ms preprocess, 41.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.1ms\n",
      "Speed: 1.2ms preprocess, 41.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.2ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.0ms\n",
      "Speed: 1.3ms preprocess, 40.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.2ms\n",
      "Speed: 1.1ms preprocess, 37.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.0ms\n",
      "Speed: 1.1ms preprocess, 40.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.3ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.2ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.2ms\n",
      "Speed: 1.4ms preprocess, 43.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.2ms\n",
      "Speed: 1.2ms preprocess, 44.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.3ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.2ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.5ms\n",
      "Speed: 1.2ms preprocess, 36.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.3ms\n",
      "Speed: 1.2ms preprocess, 40.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.9ms\n",
      "Speed: 1.3ms preprocess, 36.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 47.6ms\n",
      "Speed: 1.5ms preprocess, 47.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.2ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.6ms\n",
      "Speed: 1.1ms preprocess, 36.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.1ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.3ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.0ms\n",
      "Speed: 1.3ms preprocess, 39.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.1ms\n",
      "Speed: 1.2ms preprocess, 41.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.3ms\n",
      "Speed: 1.1ms preprocess, 40.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 44.3ms\n",
      "Speed: 1.1ms preprocess, 44.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 37.1ms\n",
      "Speed: 1.2ms preprocess, 37.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.5ms\n",
      "Speed: 1.3ms preprocess, 43.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.8ms\n",
      "Speed: 1.1ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.5ms\n",
      "Speed: 1.1ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 45.4ms\n",
      "Speed: 1.4ms preprocess, 45.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.8ms\n",
      "Speed: 1.0ms preprocess, 39.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 37.4ms\n",
      "Speed: 1.0ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 36.8ms\n",
      "Speed: 1.1ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 47.2ms\n",
      "Speed: 1.2ms preprocess, 47.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.5ms\n",
      "Speed: 1.2ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 36.9ms\n",
      "Speed: 1.2ms preprocess, 36.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.9ms\n",
      "Speed: 1.1ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.3ms\n",
      "Speed: 1.3ms preprocess, 41.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.8ms\n",
      "Speed: 1.3ms preprocess, 44.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.3ms\n",
      "Speed: 1.2ms preprocess, 40.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.5ms\n",
      "Speed: 1.0ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 58.3ms\n",
      "Speed: 1.1ms preprocess, 58.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 36.9ms\n",
      "Speed: 1.0ms preprocess, 36.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.2ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 0.9ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.8ms\n",
      "Speed: 1.4ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.0ms preprocess, 39.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.0ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.0ms\n",
      "Speed: 1.2ms preprocess, 40.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.3ms\n",
      "Speed: 1.0ms preprocess, 38.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.2ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.0ms\n",
      "Speed: 1.3ms preprocess, 45.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.8ms\n",
      "Speed: 1.3ms preprocess, 39.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.9ms\n",
      "Speed: 1.0ms preprocess, 37.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.6ms\n",
      "Speed: 1.0ms preprocess, 43.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 59.1ms\n",
      "Speed: 1.3ms preprocess, 59.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.6ms\n",
      "Speed: 1.2ms preprocess, 41.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.4ms\n",
      "Speed: 1.0ms preprocess, 44.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.0ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 1.1ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.2ms preprocess, 38.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.0ms\n",
      "Speed: 1.3ms preprocess, 42.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.3ms preprocess, 40.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.3ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.2ms\n",
      "Speed: 1.2ms preprocess, 42.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.1ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.7ms\n",
      "Speed: 1.2ms preprocess, 36.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.9ms\n",
      "Speed: 1.0ms preprocess, 36.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.3ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.1ms\n",
      "Speed: 1.3ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.1ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.2ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.6ms\n",
      "Speed: 1.0ms preprocess, 40.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.9ms\n",
      "Speed: 1.0ms preprocess, 36.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.5ms\n",
      "Speed: 1.4ms preprocess, 41.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.2ms\n",
      "Speed: 1.1ms preprocess, 44.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.1ms\n",
      "Speed: 1.2ms preprocess, 42.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.5ms\n",
      "Speed: 1.4ms preprocess, 42.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.2ms\n",
      "Speed: 1.1ms preprocess, 42.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.2ms\n",
      "Speed: 1.0ms preprocess, 39.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.4ms\n",
      "Speed: 1.3ms preprocess, 44.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.1ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.1ms\n",
      "Speed: 1.0ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 36.8ms\n",
      "Speed: 1.2ms preprocess, 36.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.3ms\n",
      "Speed: 1.3ms preprocess, 45.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.6ms\n",
      "Speed: 1.1ms preprocess, 37.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.4ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.6ms\n",
      "Speed: 1.3ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.3ms\n",
      "Speed: 1.1ms preprocess, 41.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.3ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.9ms\n",
      "Speed: 1.1ms preprocess, 40.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.2ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.3ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.2ms preprocess, 39.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.4ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.2ms\n",
      "Speed: 1.3ms preprocess, 40.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.9ms\n",
      "Speed: 1.1ms preprocess, 38.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.1ms preprocess, 39.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 1.3ms preprocess, 38.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.1ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 69.7ms\n",
      "Speed: 1.3ms preprocess, 69.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 84.7ms\n",
      "Speed: 1.4ms preprocess, 84.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 113.4ms\n",
      "Speed: 1.6ms preprocess, 113.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 139.0ms\n",
      "Speed: 1.7ms preprocess, 139.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 178.8ms\n",
      "Speed: 1.3ms preprocess, 178.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 2 Handss, 39.0ms\n",
      "Speed: 1.1ms preprocess, 39.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.8ms\n",
      "Speed: 1.0ms preprocess, 40.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.8ms\n",
      "Speed: 1.0ms preprocess, 38.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.9ms\n",
      "Speed: 1.4ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 88.4ms\n",
      "Speed: 1.3ms preprocess, 88.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 76.6ms\n",
      "Speed: 1.5ms preprocess, 76.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.3ms\n",
      "Speed: 1.3ms preprocess, 42.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.2ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.5ms preprocess, 40.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.9ms\n",
      "Speed: 1.0ms preprocess, 37.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 61.0ms\n",
      "Speed: 1.0ms preprocess, 61.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 47.7ms\n",
      "Speed: 1.3ms preprocess, 47.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.5ms\n",
      "Speed: 1.3ms preprocess, 42.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.1ms\n",
      "Speed: 1.5ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.2ms preprocess, 40.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.6ms\n",
      "Speed: 1.1ms preprocess, 44.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 48.0ms\n",
      "Speed: 1.0ms preprocess, 48.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.2ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.6ms\n",
      "Speed: 1.4ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.3ms\n",
      "Speed: 1.0ms preprocess, 38.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 53.1ms\n",
      "Speed: 10.0ms preprocess, 53.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 1.1ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 42.1ms\n",
      "Speed: 1.3ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.2ms preprocess, 39.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.6ms\n",
      "Speed: 1.1ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.1ms\n",
      "Speed: 1.4ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.2ms\n",
      "Speed: 1.3ms preprocess, 43.2ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.6ms\n",
      "Speed: 1.0ms preprocess, 40.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 36.9ms\n",
      "Speed: 1.3ms preprocess, 36.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.7ms\n",
      "Speed: 1.3ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 59.5ms\n",
      "Speed: 1.3ms preprocess, 59.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.2ms\n",
      "Speed: 1.1ms preprocess, 37.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 44.5ms\n",
      "Speed: 1.4ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.5ms\n",
      "Speed: 1.1ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.3ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.4ms\n",
      "Speed: 1.3ms preprocess, 43.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.5ms\n",
      "Speed: 1.0ms preprocess, 39.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 47.9ms\n",
      "Speed: 1.3ms preprocess, 47.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.0ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.9ms\n",
      "Speed: 1.0ms preprocess, 37.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.4ms\n",
      "Speed: 1.3ms preprocess, 45.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 40.9ms\n",
      "Speed: 1.1ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.0ms\n",
      "Speed: 1.1ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.8ms\n",
      "Speed: 1.3ms preprocess, 41.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.0ms\n",
      "Speed: 1.3ms preprocess, 41.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.0ms\n",
      "Speed: 1.2ms preprocess, 41.0ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 36.8ms\n",
      "Speed: 1.1ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.9ms\n",
      "Speed: 1.1ms preprocess, 38.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.7ms\n",
      "Speed: 1.0ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.2ms\n",
      "Speed: 1.1ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 42.1ms\n",
      "Speed: 1.3ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.7ms\n",
      "Speed: 2.2ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.3ms\n",
      "Speed: 1.2ms preprocess, 39.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 36.8ms\n",
      "Speed: 1.1ms preprocess, 36.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.1ms\n",
      "Speed: 1.1ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.7ms\n",
      "Speed: 1.1ms preprocess, 39.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.4ms\n",
      "Speed: 1.0ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.8ms\n",
      "Speed: 1.0ms preprocess, 39.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.2ms\n",
      "Speed: 1.0ms preprocess, 37.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.0ms\n",
      "Speed: 1.0ms preprocess, 40.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.7ms\n",
      "Speed: 1.3ms preprocess, 41.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.3ms\n",
      "Speed: 1.2ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 57.4ms\n",
      "Speed: 1.1ms preprocess, 57.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.3ms\n",
      "Speed: 1.2ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.0ms\n",
      "Speed: 1.2ms preprocess, 40.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 42.8ms\n",
      "Speed: 1.3ms preprocess, 42.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.0ms\n",
      "Speed: 1.2ms preprocess, 39.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.3ms\n",
      "Speed: 1.3ms preprocess, 37.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.8ms\n",
      "Speed: 1.3ms preprocess, 40.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 62.5ms\n",
      "Speed: 1.4ms preprocess, 62.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 44.6ms\n",
      "Speed: 1.3ms preprocess, 44.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.9ms\n",
      "Speed: 1.1ms preprocess, 37.9ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.8ms\n",
      "Speed: 1.1ms preprocess, 37.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.7ms\n",
      "Speed: 1.3ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 46.1ms\n",
      "Speed: 1.2ms preprocess, 46.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.9ms\n",
      "Speed: 1.3ms preprocess, 40.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 46.5ms\n",
      "Speed: 1.2ms preprocess, 46.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 42.1ms\n",
      "Speed: 1.1ms preprocess, 42.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 45.0ms\n",
      "Speed: 1.3ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.6ms\n",
      "Speed: 1.3ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.8ms\n",
      "Speed: 1.2ms preprocess, 39.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.9ms\n",
      "Speed: 1.1ms preprocess, 38.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.3ms\n",
      "Speed: 1.3ms preprocess, 43.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.2ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 46.4ms\n",
      "Speed: 1.3ms preprocess, 46.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.2ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.4ms\n",
      "Speed: 1.3ms preprocess, 45.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.9ms\n",
      "Speed: 1.2ms preprocess, 40.9ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.5ms\n",
      "Speed: 1.0ms preprocess, 43.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 52.5ms\n",
      "Speed: 1.3ms preprocess, 52.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.7ms\n",
      "Speed: 1.3ms preprocess, 43.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.9ms preprocess, 38.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.2ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.7ms\n",
      "Speed: 1.2ms preprocess, 39.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 47.4ms\n",
      "Speed: 1.1ms preprocess, 47.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.1ms\n",
      "Speed: 1.2ms preprocess, 38.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 43.3ms\n",
      "Speed: 1.1ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.0ms\n",
      "Speed: 1.1ms preprocess, 40.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.3ms\n",
      "Speed: 1.5ms preprocess, 41.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.1ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 44.5ms\n",
      "Speed: 1.2ms preprocess, 44.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.4ms\n",
      "Speed: 1.4ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.2ms\n",
      "Speed: 1.0ms preprocess, 41.2ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 49.5ms\n",
      "Speed: 1.1ms preprocess, 49.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.1ms\n",
      "Speed: 1.1ms preprocess, 39.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 45.8ms\n",
      "Speed: 1.5ms preprocess, 45.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 38.5ms\n",
      "Speed: 1.2ms preprocess, 38.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 45.1ms\n",
      "Speed: 1.3ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 39.6ms\n",
      "Speed: 1.1ms preprocess, 39.6ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 39.4ms\n",
      "Speed: 1.1ms preprocess, 39.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.1ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 37.1ms\n",
      "Speed: 1.1ms preprocess, 37.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 1 Pill, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 44.0ms\n",
      "Speed: 1.3ms preprocess, 44.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.5ms\n",
      "Speed: 1.2ms preprocess, 38.5ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 44.4ms\n",
      "Speed: 1.3ms preprocess, 44.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.5ms\n",
      "Speed: 1.1ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.4ms\n",
      "Speed: 1.3ms preprocess, 38.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.1ms\n",
      "Speed: 1.1ms preprocess, 37.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.5ms\n",
      "Speed: 1.2ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 74.5ms\n",
      "Speed: 2.4ms preprocess, 74.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.3ms\n",
      "Speed: 1.3ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 45.5ms\n",
      "Speed: 1.0ms preprocess, 45.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.1ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.4ms\n",
      "Speed: 1.3ms preprocess, 39.4ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 42.2ms\n",
      "Speed: 1.3ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.1ms\n",
      "Speed: 1.3ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.0ms preprocess, 38.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 45.1ms\n",
      "Speed: 1.4ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.3ms\n",
      "Speed: 1.0ms preprocess, 40.3ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.6ms\n",
      "Speed: 1.4ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 44.1ms\n",
      "Speed: 1.3ms preprocess, 44.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 57.4ms\n",
      "Speed: 1.1ms preprocess, 57.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.0ms\n",
      "Speed: 1.1ms preprocess, 39.0ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 43.7ms\n",
      "Speed: 1.2ms preprocess, 43.7ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 40.2ms\n",
      "Speed: 1.3ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 38.3ms\n",
      "Speed: 1.1ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 41.4ms\n",
      "Speed: 1.7ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.5ms\n",
      "Speed: 1.0ms preprocess, 43.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Pill, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.6ms\n",
      "Speed: 1.3ms preprocess, 40.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.2ms\n",
      "Speed: 1.3ms preprocess, 40.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.2ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.1ms\n",
      "Speed: 1.1ms preprocess, 40.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.4ms\n",
      "Speed: 1.3ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.3ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.0ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.3ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.3ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.2ms\n",
      "Speed: 1.3ms preprocess, 41.2ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 39.8ms\n",
      "Speed: 1.4ms preprocess, 39.8ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 37.3ms\n",
      "Speed: 1.3ms preprocess, 37.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.3ms preprocess, 39.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.0ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.2ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.3ms\n",
      "Speed: 0.9ms preprocess, 38.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.2ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 60.1ms\n",
      "Speed: 1.0ms preprocess, 60.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.1ms\n",
      "Speed: 1.1ms preprocess, 45.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.0ms\n",
      "Speed: 1.2ms preprocess, 38.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.5ms\n",
      "Speed: 1.3ms preprocess, 43.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.3ms\n",
      "Speed: 1.2ms preprocess, 43.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.1ms\n",
      "Speed: 1.1ms preprocess, 42.1ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.2ms\n",
      "Speed: 1.0ms preprocess, 42.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.4ms\n",
      "Speed: 1.1ms preprocess, 40.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.7ms\n",
      "Speed: 1.0ms preprocess, 39.7ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.1ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.2ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.2ms\n",
      "Speed: 1.3ms preprocess, 43.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 49.2ms\n",
      "Speed: 1.1ms preprocess, 49.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.9ms\n",
      "Speed: 1.1ms preprocess, 40.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.0ms\n",
      "Speed: 1.4ms preprocess, 42.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.4ms\n",
      "Speed: 1.1ms preprocess, 43.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.6ms\n",
      "Speed: 1.2ms preprocess, 37.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 1.1ms preprocess, 38.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.2ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.5ms\n",
      "Speed: 1.1ms preprocess, 38.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.2ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.6ms\n",
      "Speed: 1.3ms preprocess, 40.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.7ms\n",
      "Speed: 1.0ms preprocess, 42.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.9ms\n",
      "Speed: 1.3ms preprocess, 38.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.2ms\n",
      "Speed: 1.1ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.2ms\n",
      "Speed: 1.1ms preprocess, 39.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.1ms\n",
      "Speed: 1.2ms preprocess, 37.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 59.8ms\n",
      "Speed: 1.1ms preprocess, 59.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.0ms\n",
      "Speed: 1.1ms preprocess, 37.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.6ms\n",
      "Speed: 1.1ms preprocess, 38.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.0ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 1.5ms preprocess, 38.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.2ms preprocess, 37.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.4ms\n",
      "Speed: 1.3ms preprocess, 39.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.2ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.1ms preprocess, 38.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.0ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.2ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 59.8ms\n",
      "Speed: 1.1ms preprocess, 59.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.2ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.6ms\n",
      "Speed: 1.2ms preprocess, 39.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 49.0ms\n",
      "Speed: 1.3ms preprocess, 49.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.2ms preprocess, 39.1ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.0ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.8ms\n",
      "Speed: 1.1ms preprocess, 38.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.1ms preprocess, 40.5ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.7ms\n",
      "Speed: 1.2ms preprocess, 44.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.6ms\n",
      "Speed: 1.3ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.1ms\n",
      "Speed: 1.1ms preprocess, 40.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.1ms\n",
      "Speed: 1.2ms preprocess, 41.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.6ms\n",
      "Speed: 1.6ms preprocess, 44.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 38.6ms\n",
      "Speed: 1.2ms preprocess, 38.6ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.9ms\n",
      "Speed: 1.2ms preprocess, 42.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.5ms\n",
      "Speed: 1.2ms preprocess, 40.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.1ms preprocess, 39.3ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.7ms\n",
      "Speed: 1.6ms preprocess, 45.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.3ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.8ms\n",
      "Speed: 1.1ms preprocess, 41.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.7ms\n",
      "Speed: 1.4ms preprocess, 43.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.3ms\n",
      "Speed: 1.1ms preprocess, 39.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.4ms\n",
      "Speed: 1.1ms preprocess, 41.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.9ms\n",
      "Speed: 1.0ms preprocess, 39.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 44.3ms\n",
      "Speed: 1.1ms preprocess, 44.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.2ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.4ms\n",
      "Speed: 1.1ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.1ms\n",
      "Speed: 1.1ms preprocess, 45.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.0ms\n",
      "Speed: 1.0ms preprocess, 40.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.2ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.3ms\n",
      "Speed: 1.3ms preprocess, 41.3ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.5ms\n",
      "Speed: 1.1ms preprocess, 43.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.4ms\n",
      "Speed: 1.1ms preprocess, 42.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 43.6ms\n",
      "Speed: 1.3ms preprocess, 43.6ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.3ms\n",
      "Speed: 1.0ms preprocess, 41.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.1ms\n",
      "Speed: 1.3ms preprocess, 38.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.1ms\n",
      "Speed: 1.0ms preprocess, 41.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.4ms\n",
      "Speed: 1.1ms preprocess, 37.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 64.3ms\n",
      "Speed: 1.3ms preprocess, 64.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.1ms\n",
      "Speed: 1.1ms preprocess, 40.1ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.1ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 65.1ms\n",
      "Speed: 1.4ms preprocess, 65.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.7ms\n",
      "Speed: 1.1ms preprocess, 37.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 38.7ms\n",
      "Speed: 1.0ms preprocess, 38.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 37.3ms\n",
      "Speed: 1.2ms preprocess, 37.3ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 1 Hands, 41.5ms\n",
      "Speed: 1.1ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.9ms\n",
      "Speed: 1.3ms preprocess, 41.9ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.2ms\n",
      "Speed: 1.1ms preprocess, 40.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.2ms\n",
      "Speed: 1.2ms preprocess, 39.2ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.8ms\n",
      "Speed: 1.3ms preprocess, 41.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 40.8ms\n",
      "Speed: 1.2ms preprocess, 40.8ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 45.4ms\n",
      "Speed: 1.2ms preprocess, 45.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 39.1ms\n",
      "Speed: 1.1ms preprocess, 39.1ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.7ms\n",
      "Speed: 1.3ms preprocess, 41.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.0ms\n",
      "Speed: 1.2ms preprocess, 41.0ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.5ms\n",
      "Speed: 1.2ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 63.7ms\n",
      "Speed: 1.1ms preprocess, 63.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 41.7ms\n",
      "Speed: 1.2ms preprocess, 41.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n",
      "\n",
      "0: 288x480 (no detections), 42.4ms\n",
      "Speed: 1.4ms preprocess, 42.4ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 480)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "try:\n",
    "    model = YOLO('runs/detect/train2/weights/best.pt')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define the class names\n",
    "class_names = ['Hands', 'Open', 'Pill', 'PillBox']\n",
    "\n",
    "# Function to draw bounding boxes with different colors\n",
    "def draw_bounding_boxes(image, results):\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    confs = results[0].boxes.conf.cpu().numpy()\n",
    "    classes = results[0].boxes.cls.cpu().numpy()\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        conf = confs[i]\n",
    "        cls = int(classes[i])\n",
    "        label = class_names[cls]\n",
    "        if label == 'Hands':\n",
    "            color = (0, 255, 0)  # Green\n",
    "        elif label == 'Pills':\n",
    "            color = (255, 0, 0)  # Red\n",
    "            # Send notification to Raspberry Pi when wrong pill is detected\n",
    "            current_time = time.time()\n",
    "            if current_time - last_notification_time >= notification_interval:\n",
    "                notify_raspberry_pi(\"Wrong pill detected\")\n",
    "                last_notification_time = current_time\n",
    "        else:\n",
    "            color = (0, 0, 255)  # Blue\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(image, f'{label} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "    return image\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default webcam, or specify the webcam index\n",
    "\n",
    "def update_frame():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading frame from webcam\")\n",
    "        return\n",
    "\n",
    "    # Perform inference with adjusted NMS settings\n",
    "    try:\n",
    "        results = model(frame)\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing inference: {e}\")\n",
    "        return\n",
    "\n",
    "    # Annotate the frame with detection results\n",
    "    frame = draw_bounding_boxes(frame, results)\n",
    "\n",
    "    # Convert the frame to an ImageTk object\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(frame_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "    # Update the label with the new image\n",
    "    lbl.imgtk = imgtk  # Keep a reference to the image to prevent garbage collection\n",
    "    lbl.configure(image=imgtk)\n",
    "\n",
    "    # Schedule the next update\n",
    "    lbl.after(10, update_frame)\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Webcam Detection\")\n",
    "\n",
    "# Create a label to display the video feed\n",
    "lbl = tk.Label(root)\n",
    "lbl.pack()\n",
    "\n",
    "# Start the Tkinter event loop and the video feed\n",
    "root.after(0, update_frame)\n",
    "root.mainloop()\n",
    "\n",
    "# Release the webcam\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mClient stopped gracefully\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     main()\n",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mglobal\u001b[39;00m client_socket, receive_thread\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     setup_socket_client()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mif\u001b[39;00m client_socket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNot connected, is server running on \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(server_address[\u001b[39m0\u001b[39m], server_address[\u001b[39m1\u001b[39m]))\n",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mglobal\u001b[39;00m client_socket, receive_thread\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m client_socket \u001b[39m=\u001b[39m socket\u001b[39m.\u001b[39msocket(socket\u001b[39m.\u001b[39mAF_INET, socket\u001b[39m.\u001b[39mSOCK_STREAM) \u001b[39m# create a socket instance\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m client_socket\u001b[39m.\u001b[39;49mconnect(server_address) \u001b[39m# connect to specified server\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mConnected to server\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m receive_thread \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mThread(target\u001b[39m=\u001b[39mreceive_messages, args\u001b[39m=\u001b[39m(client_socket, shutdown_flag))\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import threading\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "server_address = ('192.168.168.167', 1442)  # Connect to RPi (or other server) on ip ... and port ... (the port is set in server.py)\n",
    "# the ip address can also be the WiFi ip of your RPi, but this can change. You can print your WiFi IP on your LCD? (if needed)\n",
    "\n",
    "\n",
    "# Global vars for use in methods/threads\n",
    "client_socket = None\n",
    "receive_thread = None\n",
    "shutdown_flag = threading.Event() # see: https://docs.python.org/3/library/threading.html#event-objects\n",
    "\n",
    "\n",
    "def setup_socket_client():\n",
    "    global client_socket, receive_thread\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # create a socket instance\n",
    "    client_socket.connect(server_address) # connect to specified server\n",
    "    print(\"Connected to server\")\n",
    "\n",
    "    receive_thread = threading.Thread(target=receive_messages, args=(client_socket, shutdown_flag))\n",
    "    receive_thread.start()\n",
    "\n",
    "def receive_messages(sock, shutdown_flag):\n",
    "    sock.settimeout(1)  # Set a timeout on the socket so when can check shutdown_flag.is_set in the loop, instead of blocking\n",
    "    counter = 0 # count the incoming messages, part of demo\n",
    "    try:\n",
    "        while not shutdown_flag.is_set(): # as long as ctrl+c is not pressed\n",
    "            try:\n",
    "                data = sock.recv(1024) # try to receive 1024 bytes of data (maximum amount; can be less)\n",
    "                if not data: # when no data is received, try again (and shutdown flag is checked again)\n",
    "                    break\n",
    "                print(\"Received from server:\", data.decode()) # print the received data, or do something with it\n",
    "                counter += 1 # up the count by 1\n",
    "                response = \"{} message(s) received\".format(counter) # create a response string\n",
    "                sock.sendall(response.encode()) # encode and send the data\n",
    "            except socket.timeout: # when no data comes within timeout, try again\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        if not shutdown_flag.is_set():\n",
    "            print(f\"Connection error: {e}\")\n",
    "    finally:\n",
    "        sock.close()\n",
    "\n",
    "def main():\n",
    "    global client_socket, receive_thread\n",
    "\n",
    "    setup_socket_client()\n",
    "\n",
    "    if client_socket is None:\n",
    "        print(\"Not connected, is server running on {}:{}?\".format(server_address[0], server_address[1]))\n",
    "        sys.exit()\n",
    "    \n",
    "    # send \"hello I'm connected\" message\n",
    "    client_socket.sendall(\"Hello from AI / notebook\".encode()) # send a \"connected\" message from client > server\n",
    "        \n",
    "\n",
    "    try:\n",
    "        while True: # random loop for other things\n",
    "            time.sleep(6)\n",
    "            print(\"doing other things...\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Client disconnecting...\")\n",
    "        shutdown_flag.set()\n",
    "    finally:\n",
    "        client_socket.close()\n",
    "        receive_thread.join()\n",
    "        print(\"Client stopped gracefully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@18323.307] global /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_f6tvh9615u/croot/opencv-suite_1691620375715/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 64] Host is down",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W5sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m lbl\u001b[39m.\u001b[39mpack()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W5sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m \u001b[39m# Setup the socket client and start the update loop\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W5sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m setup_socket_client()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W5sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m root\u001b[39m.\u001b[39mafter(\u001b[39m0\u001b[39m, update_frame)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W5sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m root\u001b[39m.\u001b[39mmainloop()\n",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mglobal\u001b[39;00m client_socket, receive_thread\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m client_socket \u001b[39m=\u001b[39m socket\u001b[39m.\u001b[39msocket(socket\u001b[39m.\u001b[39mAF_INET, socket\u001b[39m.\u001b[39mSOCK_STREAM)  \u001b[39m# Create a socket instance\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m client_socket\u001b[39m.\u001b[39;49mconnect(server_address)  \u001b[39m# Connect to specified server\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mConnected to server\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/2023-2024-projectone-ctai-AlessCol7/PillPoint/AI/PillPoint.v3i.yolov8/PillPoint.ipynb#W5sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m receive_thread \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mThread(target\u001b[39m=\u001b[39mreceive_messages, args\u001b[39m=\u001b[39m(client_socket, shutdown_flag))\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 64] Host is down"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "import socket\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "try:\n",
    "    model = YOLO('runs/detect/train2/weights/best.pt')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define the class names\n",
    "class_names = ['Hands', 'Open', 'Pill', 'PillBox']\n",
    "\n",
    "# Server address and port for Raspberry Pi\n",
    "server_address = ('192.168.168.167', 1443)  # Change this to your Raspberry Pi IP and port\n",
    "\n",
    "# Global vars for use in methods/threads\n",
    "client_socket = None\n",
    "receive_thread = None\n",
    "shutdown_flag = threading.Event()\n",
    "last_notification_time = 0\n",
    "notification_interval = 1\n",
    "\n",
    "def setup_socket_client():\n",
    "    global client_socket, receive_thread\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  # Create a socket instance\n",
    "    client_socket.connect(server_address)  # Connect to specified server\n",
    "    print(\"Connected to server\")\n",
    "\n",
    "    receive_thread = threading.Thread(target=receive_messages, args=(client_socket, shutdown_flag))\n",
    "    receive_thread.start()\n",
    "\n",
    "def receive_messages(sock, shutdown_flag):\n",
    "    sock.settimeout(1)  # Set a timeout on the socket so we can check shutdown_flag.is_set in the loop, instead of blocking\n",
    "    try:\n",
    "        while not shutdown_flag.is_set():  # As long as ctrl+c is not pressed\n",
    "            try:\n",
    "                data = sock.recv(1024)  # Try to receive 1024 bytes of data (maximum amount; can be less)\n",
    "                if not data:  # When no data is received, try again (and shutdown flag is checked again)\n",
    "                    break\n",
    "                print(\"Received from server:\", data.decode())  # Print the received data, or do something with it\n",
    "            except socket.timeout:  # When no data comes within timeout, try again\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        if not shutdown_flag.is_set():\n",
    "            print(f\"Connection error: {e}\")\n",
    "    finally:\n",
    "        sock.close()\n",
    "\n",
    "# Function to draw bounding boxes with different colors\n",
    "def draw_bounding_boxes(image, results):\n",
    "    global last_notification_time\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    confs = results[0].boxes.conf.cpu().numpy()\n",
    "    classes = results[0].boxes.cls.cpu().numpy()\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        conf = confs[i]\n",
    "        cls = int(classes[i])\n",
    "        label = class_names[cls]\n",
    "        if label == 'Hands':\n",
    "            color = (0, 255, 0)  # Green\n",
    "        elif label == 'Pills':\n",
    "            color = (255, 0, 0)  # Red\n",
    "            # Send notification to Raspberry Pi when wrong pill is detected\n",
    "            current_time = time.time()\n",
    "            if current_time - last_notification_time >= notification_interval:\n",
    "                notify_raspberry_pi(\"Wrong pill detected\")\n",
    "                last_notification_time = current_time\n",
    "        else:\n",
    "            color = (0, 0, 255)  # Blue\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(image, f'{label} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "    return image\n",
    "\n",
    "# Function to notify Raspberry Pi\n",
    "def notify_raspberry_pi(message):\n",
    "    try:\n",
    "        client_socket.sendall(message.encode())\n",
    "        print(f\"Sent to Raspberry Pi: {message}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending message to Raspberry Pi: {e}\")\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default webcam, or specify the webcam index\n",
    "\n",
    "def update_frame():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading frame from webcam\")\n",
    "        return\n",
    "\n",
    "    # Perform inference with adjusted NMS settings\n",
    "    try:\n",
    "        results = model(frame)\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing inference: {e}\")\n",
    "        return\n",
    "\n",
    "    # Annotate the frame with detection results\n",
    "    frame = draw_bounding_boxes(frame, results)\n",
    "\n",
    "    # Convert the frame to an ImageTk object\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(frame_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "    # Update the label with the new image\n",
    "    lbl.imgtk = imgtk  # Keep a reference to the image to prevent garbage collection\n",
    "    lbl.configure(image=imgtk)\n",
    "\n",
    "    # Schedule the next update\n",
    "    lbl.after(10, update_frame)\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Webcam Detection\")\n",
    "\n",
    "# Create a label to display the video feed\n",
    "lbl = tk.Label(root)\n",
    "lbl.pack()\n",
    "\n",
    "# Setup the socket client and start the update loop\n",
    "setup_socket_client()\n",
    "root.after(0, update_frame)\n",
    "root.mainloop()\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "shutdown_flag.set()\n",
    "if client_socket:\n",
    "    client_socket.close()\n",
    "if receive_thread:\n",
    "    receive_thread.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
